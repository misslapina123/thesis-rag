{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "dotenv.load_dotenv()\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "import getpass\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\")\n",
    "OPENAI_DEPLOYMENT = os.environ.get(\"OPENAI_DEPLOYMENT\")\n",
    "EMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "EMBEDDING_DEPLOYMENT = os.environ.get(\"EMBEDDING_DEPLOYMENT\")\n",
    "OPENAI_MODEL_GPT4 = os.environ.get(\"OPENAI_MODEL_GPT4\")\n",
    "OPENAI_DEPLOYMENT_GPT4 = os.environ.get(\"OPENAI_DEPLOYMENT_GPT4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_client = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDING_DEPLOYMENT,\n",
    "    openai_api_version=OPENAI_API_VERSION)\n",
    "llm = AzureChatOpenAI(model_name=OPENAI_MODEL, azure_deployment=OPENAI_DEPLOYMENT,temperature=0)\n",
    "llm_gpt4 = AzureChatOpenAI(model_name=OPENAI_MODEL_GPT4, azure_deployment=OPENAI_DEPLOYMENT_GPT4,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_llm(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  \n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "        raise_exceptions=False\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_rag(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  # most likely\n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "        raise_exceptions=False,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Analyze the following quote from Dr. Volumnia Gaul from 'Ballad of Songbirds and Snakes': 'There is a point to everything or nothing at all, depending on your worldview.'\",\n",
    "    \"Analyze the following quote from 'Ballad of Songbirds and Snakes': 'He'd loved the unfamiliar sense of safety that their defeat had brought. The security that could only come with power. The ability to control things. Yes, that was what he'd loved best of all.'\",\n",
    "    \"Analyze this quote from 'Ballad of Songbirds and Snakes': 'Snow lands on top.'\",\n",
    "    \"Analyze this quote from 'Ballad of Songbirds and Snakes': '...at eighteen, the heir to the once-great house of Snow had nothing to live on but his wits.'\",\n",
    "    \"Analyze this quote from 'Ballad of Songbirds and Snakes': 'Bombs and blood. That was how the rebels had killed his mother. He wondered if they had killed Lucy Gray's, too. 'Just like her pearly white bones.' She seemed to have no love for District 12, always separating herself from it, saying she was, what was it . . . Covey?'\",\n",
    "    \"Analyze this quote from 'Ballad of Songbirds and Snakes': 'All right, so he'd dropped the handkerchief with Lucy Gray's scent - the one from the outside pocket of his book bag - into the snake tank. He'd done it so they would not bite her as they had Clemensia. So they would not kill her. Because he cared about her. Because he cared about her? Or because he wanted to win the Hunger Games so that he could secure the Plinth Prize. If it was the latter, he had cheated to win, and that was that.'\",\n",
    "    \"Analyze this quote from 'Ballad of Songbirds and Snakes': 'The Hunger Games are an unnatural, vicious punishment. How could a good person like you be expected to go along with them?'\",\n",
    "    \"Analyze this quote from 'Ballad of Songbirds and Snakes': 'If I'm helping to kill people in the districts, how is it any better than helping to kill them in the Hunger Games?'\",\n",
    "    \"Analyze this quote from 'Ballad of Songbirds and Snakes': 'You don't think I've invested all this time in you to hand you off to those imbeciles in the districts, do you?'\"\n",
    "]\n",
    "\n",
    "\n",
    "ground_truths = [\n",
    "    [\"Throughout The Ballad, Dr. Gaul presents Coryo and the other students with items of political philosophy, often in order to explain her own views of power and control. Here, her contrast between a 'point' and 'nothing' plays into a few of her larger ideas. Authoritarian though she is, Dr. Gaul sees the power of the Capitol as serving a useful point in rescuing Panem from chaos. On another level, the situation that provokes this comment - Coryo and Clemensia observing Dr. Gaul's multicolored snakes and wondering if there is a 'point' to the coloration - reveals that the idea of purpose extends well beyond vast political constructs in the novel. Dr. Gaul's rainbow snakes serve a direct point in making the Hunger Games a stark spectacle, and they serve a narrative point in connecting Lucy with one of her symbolic animals in the arena. It is clear what worldview Dr. Gaul would endorse.\"],\n",
    "    [\"Here, Coryo is completing an assignment for Dr. Gaul on the rather twisted topic of what he liked about the war, and he is proving just how compatible Dr. Gaul's authoritarian worldview is with his own. Of course, Coryo's reasons for disliking the Districts are not purely sociopathic. The warfare caused his family real suffering and gave him the impression that, despite his apparent intelligence and his potential for decency, he was, for a time, fundamentally helpless. Still, is it reassuring 'security' or maniacal 'power' that really appeals to Coriolanus? On the basis of the later events of his life, his desire for control and power becomes twisted beyond understandable self-preservation and into open villainy.\"],\n",
    "    [\"This is something of a Snow family motto for Tigris and Coryo: the 'saying that had gotten them through the war' (Ballad, 9), as the two cousins recollect early in the novel. Though loaded with reassurance and a pleasing sense of inevitability, this mantra varies in tone, context, and implication as The Ballad of Songbirds and Snakes progresses. 'Snow lands on top' appears early on, when Tigris is contemplating her family's poverty and Snow is eager to excel as a mentor. These are inspiring words for difficult circumstances, but by the final time the motto appears—in the Epilogue, with Coryo victorious and increasingly vicious—a Snow has in fact landed on top and has succeeded in crushing plenty of other people.\"],\n",
    "    [\"Here, we are told that Snow is only eighteen, and we are led to contemplate the differences between the wealthy dictator of the later Hunger Games trilogy and the earnest young Coryo we see here. His kindly mother and his domineering father are dead, and he doesn't have their fortune—or any fortune—to benefit him, since his family's resources were lost in the rebellion with the presumed destruction of District 13. Now, Snow must use his wits and cunning to escape his situation. Thus, the book sets up Coryo's need to restore his family's name as one of the character's central motivations. Readers of the final trilogy may know the end result of Snow's life, but the task of connecting this impoverished young man to the well-heeled, sociopathic President Snow offers plenty of intrigue.\"],\n",
    "    [\"Is the bond between Snow and Lucy mainly based on self-interest or on genuine affection? While a romance between an aristocrat from the Capitol and a wandering musician may seem to have a shaky foundation, Snow's early reflections indicate that he feels some sympathy for Lucy. Their shared experience of the war is one of loneliness and displacement. Still, as Snow's need to excel takes precedence, his connection to Lucy becomes more complicated. The idea that Lucy has 'no love for District 12,' for instance, will later be deployed by Snow in discussions and interviews meant to appeal to Capitol sponsors. Of course, construing Lucy as non-District may help her popularity and her odds of winning, but it is impossible to separate Snow's bond with Lucy from his own determination to improve his life.\"],\n",
    "    [\"The narration of The Ballad is by no means indirect about the moral dilemmas that Coryo encounters. Here, the narrative explicitly raises the question of whether he desires Lucy's safety or a personal win more, underscoring this theme with sentences that are identical but for a single unit of punctuation: 'Because he cared about her. Because he cared about her?' At this point in the novel, it is possible to read a single action as doubled-sided and Coryo's confusion as genuine. By the end, however, he will decisively choose his future over Lucy, reaching a level of self-interest that he doesn't definitively attain in this scene.\"],\n",
    "    [\"There are a few Capitol citizens who object to the Hunger Games in a principled manner, though not all of them speak out as sharply as Sejanus Plinth does. Among the mentors, Lysistrata Vickers seems to harbor real compassion for her tribute, and Coriolanus himself is troubled - at least for a time - by the fact that he kills a tribute while escaping the arena. Tigris's own objections are, from one perspective, surprising. Though related to a Capitol loyalist (her grandmother) and a future authoritarian (her cousin), she finds the games miserable and believes that Coryo does the same. Her judgment of Coriolanus may be flawed and her idea may not quite fit her family's sympathies.\"],\n",
    "    [\"In part, Sejanus's decision to become a Peacekeeper was meant to remove him from a Capitol milieu that he never found appealing. Nonetheless, this quotation reveals that Sejanus is in an impossible position. If becoming a Peacekeeper - ideally, a Peacekeeper doctor or medic - was meant as a form of escape and a route to atonement, then Sejanus's new approach has led him down the wrong path. He is forced to help an institution that executes rebels without trial and that, for the most part, is willing to let the Districts remain in disrepair - beyond providing the Capitol with resources, that is.\"],\n",
    "    [\"Here, Dr. Gaul reveals that, in her own supremely self-interested way, she holds Coriolanus in high regard. He is a fantastic investment, and his time in District 12 has proven that he can thrive even in conditions of deprivation and uncertainty despite his Capitol pedigree. Dr. Gaul's own Capitol elitism is on display in her reference to the 'imbeciles' from the Districts. How much, however, did Coryo really learn from his time in the Districts? His stint as a Peacemaker is construed as a necessary toughening in The Ballad, but the events of the later Hunger Games trilogy suggest that Coriolanus comes to underestimate those District 'imbeciles' at his own peril.\"]\n",
    "   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_average = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highest(average_score):\n",
    "    global max_average\n",
    "    if average_score > max_average:\n",
    "        max_average = average_score\n",
    "        print(\"This is the new best value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary(result):\n",
    "    dict_result = dict(result)\n",
    "    average_score = sum(dict_result.values()) / len(dict_result)\n",
    "    print(f\"The average score is: {average_score}\")\n",
    "    find_highest(average_score)\n",
    "    return average_score, dict_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General answer by LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm}\n",
    ")\n",
    "llm_chain_gpt4 =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm_gpt4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_llm = []\n",
    "contexts_llm = [[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in questions:\n",
    "    response = llm_chain.invoke({\"question\": query})\n",
    "    answers_llm.append(response[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  69%|██████▉   | 25/36 [00:03<00:00, 12.19it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 36/36 [00:07<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8114, 'answer_relevancy': 0.8701, 'answer_similarity': 0.8660, 'answer_correctness': 0.6800}\n",
      "The average score is: 0.8068829769941132\n",
      "This is the new best value!\n"
     ]
    }
   ],
   "source": [
    "llm_results = evaluation_llm(questions, answers_llm, contexts_llm, ground_truths)\n",
    "print(llm_results)\n",
    "avg_llm_results ,dict_llm_results = dictionary(llm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 36/36 [00:07<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8631, 'answer_relevancy': 0.8825, 'answer_similarity': 0.8583, 'answer_correctness': 0.6600}\n",
      "The average score is: 0.8159797900896288\n",
      "This is the new best value!\n"
     ]
    }
   ],
   "source": [
    "answers_llm_gpt4 = []\n",
    "for query in questions:\n",
    "    response = llm_chain_gpt4.invoke({\"question\": query})\n",
    "    answers_llm_gpt4.append(response[\"response\"].content)\n",
    "llm_results_gpt4 = evaluation_llm(questions, answers_llm_gpt4, contexts_llm, ground_truths)\n",
    "print(llm_results_gpt4)\n",
    "avg_llm_results_gpt4 ,dict_llm_results_gpt4 = dictionary(llm_results_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_chain(prompt, retriever, llm):\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    return retrieval_augmented_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"..\\ballad\\the_ballad_of_songbirds_and_snakes.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "text_splitter = CharacterTextSplitter()\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "db_naive = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"../ballad/vectordb/naive\")\n",
    "db_naive.persist()\n",
    "retriever_naive = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_naive = Chroma(persist_directory = \"../ballad/vectordb/naive\", embedding_function=embeddings_client)\n",
    "retriever_naive = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question}. \n",
    "Provided context {context}.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_naive = []\n",
    "contexts_naive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_naive, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_naive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_naive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_naive.append(\"No answer\")\n",
    "        context_full = retriever_naive.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_naive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:10<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.7556, 'answer_relevancy': 0.8829, 'context_precision': 0.6019, 'context_recall': 0.6111, 'answer_similarity': 0.8727, 'answer_correctness': 0.6182}\n",
      "The average score is: 0.7236995269750812\n"
     ]
    }
   ],
   "source": [
    "result_naive_rag = evaluation_rag(questions, answers_naive, contexts_naive, ground_truths)\n",
    "print(result_naive_rag)\n",
    "avg_result_naive_rag, dict_result_naive_rag = dictionary(result_naive_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recursive splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "text_splitter = text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder()\n",
    "chunks_r = text_splitter.split_documents(documents)\n",
    "db_basic = Chroma.from_documents(chunks_r, embeddings_client, persist_directory = \"../ballad/vectordb/recursive_basic\")\n",
    "db_basic.persist()\n",
    "retriever_basic = db_basic.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_basic = Chroma(persist_directory = \"../ballad/vectordb/recursive_basic\", embedding_function=embeddings_client)\n",
    "retriever_basic = db_basic.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Analyze this quote from 'Ballad of Songbirds and Snakes': 'All right, so he'd dropped the handkerchief with Lucy Gray's scent - the one from the outside pocket of his book bag - into the snake tank. He'd done it so they would not bite her as they had Clemensia. So they would not kill her. Because he cared about her. Because he cared about her? Or because he wanted to win the Hunger Games so that he could secure the Plinth Prize. If it was the latter, he had cheated to win, and that was that.'\n"
     ]
    }
   ],
   "source": [
    "answers_recursive = []\n",
    "contexts_recursive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_basic, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_recursive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_recursive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_recursive.append(\"No answer\")\n",
    "        context_full = retriever_basic.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_recursive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  20%|██        | 11/54 [00:03<00:11,  3.74it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:09<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.7311, 'answer_relevancy': 0.7567, 'context_precision': 0.4875, 'context_recall': 0.6765, 'answer_similarity': 0.7017, 'answer_correctness': 0.7877}\n",
      "The average score is: 0.6902103845750811\n"
     ]
    }
   ],
   "source": [
    "result_recursive = evaluation_rag(questions, answers_recursive, contexts_recursive, ground_truths)\n",
    "print(result_recursive)\n",
    "avg_result_recursive, dict_result_recursive = dictionary(result_recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_evaluate(retriever, prompt, llm):\n",
    "    answers_recursive = []\n",
    "    contexts_recursive = []\n",
    "\n",
    "    for query in questions:\n",
    "        try:  \n",
    "            response = retrieval_chain(prompt, retriever, llm).invoke({\"question\": query})\n",
    "            # Access the response content\n",
    "            answers_recursive.append(response[\"response\"].content)\n",
    "            # Access the context content\n",
    "            context_content = [context.page_content for context in response[\"context\"]]\n",
    "            contexts_recursive.append(context_content)  \n",
    "        except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers_recursive.append(\"No answer\")\n",
    "            context_full = retriever.get_relevant_documents(query)\n",
    "            context_content = [context.page_content for context in context_full]\n",
    "            contexts_recursive.append(context_content)\n",
    "\n",
    "\n",
    "    result = evaluation_rag(questions, answers_recursive, contexts_recursive, ground_truths)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 100)\n",
    "chunks_1000 = text_splitter.split_documents(documents)\n",
    "print(len(chunks_1000))\n",
    "db_1000 = Chroma.from_documents(chunks_1000, embeddings_client, persist_directory = \"../ballad/vectordb/recursive_1000\")\n",
    "db_1000.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_1000 = Chroma(persist_directory = \"../ballad/vectordb/recursive_1000\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Analyze this quote from 'Ballad of Songbirds and Snakes': 'All right, so he'd dropped the handkerchief with Lucy Gray's scent - the one from the outside pocket of his book bag - into the snake tank. He'd done it so they would not bite her as they had Clemensia. So they would not kill her. Because he cared about her. Because he cared about her? Or because he wanted to win the Hunger Games so that he could secure the Plinth Prize. If it was the latter, he had cheated to win, and that was that.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  26%|██▌       | 14/54 [00:03<00:08,  4.52it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000\n",
      "{'faithfulness': 0.8073, 'answer_relevancy': 0.7830, 'context_precision': 0.4722, 'context_recall': 0.7878, 'answer_similarity': 0.6856, 'answer_correctness': 0.8029}\n",
      "The average score is: 0.7231345250780993\n"
     ]
    }
   ],
   "source": [
    "retriever_1000 = db_1000.as_retriever()\n",
    "result_1000 = run_and_evaluate(retriever_1000, prompt, llm)\n",
    "print(\"CHUNK SIZE 1000\")\n",
    "print(result_1000)\n",
    "avg_result_1000, dict_result_1000 = dictionary(result_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "# THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500, chunk_overlap = 50)\n",
    "chunks_500 = text_splitter.split_documents(documents)\n",
    "print(len(chunks_500))\n",
    "db_500 = Chroma.from_documents(chunks_500, embeddings_client, persist_directory = \"../ballad/vectordb/recursive_500\")\n",
    "db_500.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_500 = Chroma(persist_directory = \"../ballad/vectordb/recursive_500\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-202' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-203' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-204' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-205' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-206' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-207' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 500\n",
      "{'faithfulness': 0.8163, 'answer_relevancy': 0.9034, 'context_precision': 0.4012, 'context_recall': 0.6481, 'answer_similarity': 0.8669, 'answer_correctness': 0.6844}\n",
      "The average score is: 0.720067425291858\n"
     ]
    }
   ],
   "source": [
    "retriever_500 = db_500.as_retriever()\n",
    "result_500 = run_and_evaluate(retriever_500, prompt, llm)\n",
    "print(\"CHUNK SIZE 500\")\n",
    "print(result_500)\n",
    "avg_result_500, dict_result_500 = dictionary(result_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 2000, chunk_overlap = 200)\n",
    "chunks_2000 = text_splitter.split_documents(documents)\n",
    "print(len(chunks_2000))\n",
    "db_2000 = Chroma.from_documents(chunks_2000, embeddings_client, persist_directory = \"../ballad/vectordb/recursive_2000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_2000 = Chroma(persist_directory = \"../ballad/vectordb/recursive_2000\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Analyze this quote from 'Ballad of Songbirds and Snakes': 'All right, so he'd dropped the handkerchief with Lucy Gray's scent - the one from the outside pocket of his book bag - into the snake tank. He'd done it so they would not bite her as they had Clemensia. So they would not kill her. Because he cared about her. Because he cared about her? Or because he wanted to win the Hunger Games so that he could secure the Plinth Prize. If it was the latter, he had cheated to win, and that was that.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  20%|██        | 11/54 [00:03<00:10,  3.93it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:09<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 2000\n",
      "{'faithfulness': 0.8064, 'answer_relevancy': 0.7431, 'context_precision': 0.4389, 'context_recall': 0.6734, 'answer_similarity': 0.6855, 'answer_correctness': 0.7563}\n",
      "The average score is: 0.6839296969751175\n"
     ]
    }
   ],
   "source": [
    "retriever_2000 = db_2000.as_retriever()\n",
    "result_2000 = run_and_evaluate(retriever_2000, prompt, llm)\n",
    "print(\"CHUNK SIZE 2000\")\n",
    "print(result_2000)\n",
    "avg_result_2000, dict_result_2000 = dictionary(result_2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 3000, chunk_overlap = 300)\n",
    "chunks_3000 = text_splitter.split_documents(documents)\n",
    "print(len(chunks_3000))\n",
    "db_3000 = Chroma.from_documents(chunks_3000, embeddings_client, persist_directory = \"../ballad/vectordb/recursive_3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_3000 = Chroma(persist_directory = \"../ballad/vectordb/recursive_3000\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Analyze this quote from 'Ballad of Songbirds and Snakes': 'All right, so he'd dropped the handkerchief with Lucy Gray's scent - the one from the outside pocket of his book bag - into the snake tank. He'd done it so they would not bite her as they had Clemensia. So they would not kill her. Because he cared about her. Because he cared about her? Or because he wanted to win the Hunger Games so that he could secure the Plinth Prize. If it was the latter, he had cheated to win, and that was that.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  26%|██▌       | 14/54 [00:03<00:07,  5.49it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 3000\n",
      "{'faithfulness': 0.8087, 'answer_relevancy': 0.7442, 'context_precision': 0.5190, 'context_recall': 0.6554, 'answer_similarity': 0.7109, 'answer_correctness': 0.8236}\n",
      "The average score is: 0.7103154417058323\n"
     ]
    }
   ],
   "source": [
    "retriever_3000 = db_3000.as_retriever()\n",
    "result_3000 = run_and_evaluate(retriever_3000, prompt, llm)\n",
    "print(\"CHUNK SIZE 3000\")\n",
    "print(result_3000)\n",
    "avg_result_3000 ,dict_result_3000 = dictionary(result_3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now time to look for different top-k\n",
    "\n",
    "Note: We continue with the size chunk of 1000 as it had the highest average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:09<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000, K=3\n",
      "{'faithfulness': 0.8200, 'answer_relevancy': 0.9060, 'context_precision': 0.5370, 'context_recall': 0.7963, 'answer_similarity': 0.8722, 'answer_correctness': 0.6900}\n",
      "The average score is: 0.7702549797617143\n"
     ]
    }
   ],
   "source": [
    "retriever_3 = db_1000.as_retriever(search_kwargs={\"k\": 3})\n",
    "result_3 = run_and_evaluate(retriever_3, prompt, llm)\n",
    "print(\"CHUNK SIZE 1000, K=3\")\n",
    "print(result_3)\n",
    "avg_result_3, dict_result_3 = dictionary(result_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:10<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000, K=5\n",
      "{'faithfulness': 0.8667, 'answer_relevancy': 0.8925, 'context_precision': 0.4636, 'context_recall': 0.6204, 'answer_similarity': 0.8748, 'answer_correctness': 0.6645}\n",
      "The average score is: 0.7303917310800102\n"
     ]
    }
   ],
   "source": [
    "retriever_5 = db_1000.as_retriever(search_kwargs={\"k\": 5})\n",
    "result_5 = run_and_evaluate(retriever_5, prompt, llm)\n",
    "print(\"CHUNK SIZE 1000, K=5\")\n",
    "print(result_5)\n",
    "avg_result_5, dict_result_5 = dictionary(result_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:10<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000, K=5\n",
      "{'faithfulness': 0.8667, 'answer_relevancy': 0.9038, 'context_precision': 0.4434, 'context_recall': 0.6481, 'answer_similarity': 0.8671, 'answer_correctness': 0.6306}\n",
      "The average score is: 0.7266041952543404\n"
     ]
    }
   ],
   "source": [
    "retriever_6= db_1000.as_retriever(search_kwargs={\"k\": 6})\n",
    "result_6 = run_and_evaluate(retriever_6, prompt, llm)\n",
    "print(\"CHUNK SIZE 1000, K=5\")\n",
    "print(result_6)\n",
    "avg_result_6, dict_result_6 = dictionary(result_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Analyze this quote from 'Ballad of Songbirds and Snakes': 'Bombs and blood. That was how the rebels had killed his mother. He wondered if they had killed Lucy Gray's, too. 'Just like her pearly white bones.' She seemed to have no love for District 12, always separating herself from it, saying she was, what was it . . . Covey?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  20%|██        | 11/54 [00:03<00:12,  3.38it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000, K=7\n",
      "{'faithfulness': 0.9253, 'answer_relevancy': 0.7778, 'context_precision': 0.4403, 'context_recall': 0.8073, 'answer_similarity': 0.7465, 'answer_correctness': 0.5476}\n",
      "The average score is: 0.7074589251052892\n"
     ]
    }
   ],
   "source": [
    "retriever_7= db_1000.as_retriever(search_kwargs={\"k\": 7})\n",
    "result_7 = run_and_evaluate(retriever_7, prompt, llm)\n",
    "print(\"CHUNK SIZE 1000, K=7\")\n",
    "print(result_7)\n",
    "avg_result_7, dict_result_7 = dictionary(result_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:11<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000, K=2\n",
      "{'faithfulness': 0.6167, 'answer_relevancy': 0.8891, 'context_precision': 0.5000, 'context_recall': 0.7778, 'answer_similarity': 0.8708, 'answer_correctness': 0.6751}\n",
      "The average score is: 0.7215756009304392\n"
     ]
    }
   ],
   "source": [
    "retriever_2 = db_1000.as_retriever(search_kwargs={\"k\": 2})\n",
    "result_2 = run_and_evaluate(retriever_2, prompt, llm)\n",
    "print(\"CHUNK SIZE 1000, K=2\")\n",
    "print(result_2)\n",
    "avg_result_2, dict_result_2 = dictionary(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look for different retrievers\n",
    "\n",
    "3 chunks was the best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parent document retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.6056, 'answer_relevancy': 0.8858, 'context_precision': 0.5556, 'context_recall': 0.4167, 'answer_similarity': 0.8703, 'answer_correctness': 0.6878}\n",
      "The average score is: 0.6702675283107856\n"
     ]
    }
   ],
   "source": [
    "parent_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=2000)\n",
    "child_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap = 0)\n",
    "\n",
    "# vectorstore = Chroma(collection_name=\"split_parents\",persist_directory = \"../ballad/vectordb/parent\", embedding_function=embeddings_client)\n",
    "# vectorstore.persist()\n",
    "vectorstore = Chroma(persist_directory = \"../ballad/vectordb/parent\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "parent_document_retriever.add_documents(documents)\n",
    "result_parent = run_and_evaluate(parent_document_retriever, prompt, llm)\n",
    "print(result_parent)\n",
    "avg_result_parent, dict_result_parent = dictionary(result_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:10<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.8619, 'answer_relevancy': 0.8815, 'context_precision': 0.3333, 'context_recall': 0.5556, 'answer_similarity': 0.8801, 'answer_correctness': 0.7004}\n",
      "The average score is: 0.7021394525630296\n"
     ]
    }
   ],
   "source": [
    "parent_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap = 50)\n",
    "child_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap = 0)\n",
    "\n",
    "# vectorstore = Chroma(collection_name=\"split_parents_small\",persist_directory = \"../ballad/vectordb/parent_small\", embedding_function=embeddings_client)\n",
    "# vectorstore.persist()\n",
    "vectorstore = Chroma(persist_directory = \"../ballad/vectordb/parent_small\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_small = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_small,\n",
    "    parent_splitter=parent_splitter_small,\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "parent_document_retriever_small.add_documents(documents)\n",
    "result_parent_small = run_and_evaluate(parent_document_retriever_small, prompt, llm)\n",
    "print(result_parent_small)\n",
    "avg_result_parent_small, dict_result_parent_small = dictionary(result_parent_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Analyze the following quote from Dr. Volumnia Gaul from 'Ballad of Songbirds and Snakes': 'There is a point to everything or nothing at all, depending on your worldview.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  20%|██        | 11/54 [00:03<00:10,  4.01it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9333, 'answer_relevancy': 0.9006, 'context_precision': 0.5185, 'context_recall': 0.7500, 'answer_similarity': 0.8543, 'answer_correctness': 0.6110}\n",
      "The average score is: 0.76130090703488\n"
     ]
    }
   ],
   "source": [
    "parent_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=3000)\n",
    "child_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap = 0)\n",
    "\n",
    "# vectorstore = Chroma(collection_name=\"split_parents_large\",persist_directory = \"../ballad/vectordb/parent_large\", embedding_function=embeddings_client)\n",
    "# vectorstore.persist()\n",
    "vectorstore = Chroma(persist_directory = \"../ballad/vectordb/parent_large\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_large = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_large,\n",
    "    parent_splitter=parent_splitter_large,\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "parent_document_retriever_large.add_documents(documents)\n",
    "result_parent_large = run_and_evaluate(parent_document_retriever_large, prompt, llm)\n",
    "print(result_parent_large)\n",
    "avg_result_parent_large, dict_result_parent_large = dictionary(result_parent_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum marginal relevance retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:11<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal relevance\n",
      "{'faithfulness': 0.6400, 'answer_relevancy': 0.9034, 'context_precision': 0.5000, 'context_recall': 0.6111, 'answer_similarity': 0.8681, 'answer_correctness': 0.6699}\n",
      "The average score is: 0.6987434556587875\n"
     ]
    }
   ],
   "source": [
    "retriever_mmr = db_1000.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 3})\n",
    "result_mmr = run_and_evaluate(retriever_mmr, prompt, llm)\n",
    "print(\"Marginal relevance\")\n",
    "print(result_mmr)\n",
    "average_score_mmr, dict_result_mmr = dictionary(result_mmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Analyze this quote from 'Ballad of Songbirds and Snakes': 'All right, so he'd dropped the handkerchief with Lucy Gray's scent - the one from the outside pocket of his book bag - into the snake tank. He'd done it so they would not bite her as they had Clemensia. So they would not kill her. Because he cared about her. Because he cared about her? Or because he wanted to win the Hunger Games so that he could secure the Plinth Prize. If it was the latter, he had cheated to win, and that was that.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  20%|██        | 11/54 [00:03<00:10,  4.21it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:09<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal relevance\n",
      "{'faithfulness': 0.7406, 'answer_relevancy': 0.7681, 'context_precision': 0.6815, 'context_recall': 0.6941, 'answer_similarity': 0.7529, 'answer_correctness': 0.7410}\n",
      "The average score is: 0.7296927753868454\n"
     ]
    }
   ],
   "source": [
    "retriever_mmr_4 = db_1000.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 4})\n",
    "result_mmr_4 = run_and_evaluate(retriever_mmr_4, prompt, llm)\n",
    "print(\"Marginal relevance\")\n",
    "print(result_mmr_4)\n",
    "average_score_mmr_4, dict_result_mmr_4 = dictionary(result_mmr_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Analyze this quote from 'Ballad of Songbirds and Snakes': '...at eighteen, the heir to the once-great house of Snow had nothing to live on but his wits.'\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Analyze this quote from 'Ballad of Songbirds and Snakes': 'All right, so he'd dropped the handkerchief with Lucy Gray's scent - the one from the outside pocket of his book bag - into the snake tank. He'd done it so they would not bite her as they had Clemensia. So they would not kill her. Because he cared about her. Because he cared about her? Or because he wanted to win the Hunger Games so that he could secure the Plinth Prize. If it was the latter, he had cheated to win, and that was that.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   2%|▏         | 1/54 [00:00<00:21,  2.48it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  26%|██▌       | 14/54 [00:03<00:08,  4.93it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:09<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal relevance\n",
      "{'faithfulness': 0.7226, 'answer_relevancy': 0.6523, 'context_precision': 0.5508, 'context_recall': 0.5813, 'answer_similarity': 0.9035, 'answer_correctness': 0.6234}\n",
      "The average score is: 0.6723002779387696\n"
     ]
    }
   ],
   "source": [
    "retriever_mmr_5 = db_1000.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 5})\n",
    "result_mmr_5 = run_and_evaluate(retriever_mmr_5, prompt, llm)\n",
    "print(\"Marginal relevance\")\n",
    "print(result_mmr_5)\n",
    "average_score_mmr_5, dict_result_mmr_5 = dictionary(result_mmr_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 100)\n",
    "chunks_1000 = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25\n",
      "{'faithfulness': 0.7074, 'answer_relevancy': 0.8639, 'context_precision': 0.6667, 'context_recall': 0.6667, 'answer_similarity': 0.8815, 'answer_correctness': 0.6775}\n",
      "The average score is: 0.7439547122210094\n"
     ]
    }
   ],
   "source": [
    "retriever_bm25 = BM25Retriever.from_documents(chunks_1000, search_kwargs = {\"k\": 3})\n",
    "result_bm25 = run_and_evaluate(retriever_bm25, prompt, llm)\n",
    "print(\"BM25\")\n",
    "print(result_bm25)\n",
    "average_score_bm25, dict_result_bm25 = dictionary(result_bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensambler - Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:07<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensambler 75/25\n",
      "{'faithfulness': 0.8963, 'answer_relevancy': 0.9131, 'context_precision': 0.4892, 'context_recall': 0.7500, 'answer_similarity': 0.8615, 'answer_correctness': 0.6664}\n",
      "The average score is: 0.7627580910110087\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_1 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_3], weights=[0.75, 0.25])\n",
    "result_ensemble1 = run_and_evaluate(ensemble_retriever_1, prompt, llm)\n",
    "print(\"Ensambler 75/25\")\n",
    "print(result_ensemble1)\n",
    "average_score_ensambler1, dict_result_ensemble1 = dictionary(result_ensemble1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-1137' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1138' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1139' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1140' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1141' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1142' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1143' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1144' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1145' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1146' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1147' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1148' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating: 100%|██████████| 54/54 [00:09<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensambler 50/50\n",
      "{'faithfulness': 0.8714, 'answer_relevancy': 0.8963, 'context_precision': 0.4432, 'context_recall': 0.6963, 'answer_similarity': 0.8763, 'answer_correctness': 0.6891}\n",
      "The average score is: 0.7454412490331638\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_2 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_3], weights=[0.5, 0.5])\n",
    "result_ensemble2 = run_and_evaluate(ensemble_retriever_2, prompt, llm)\n",
    "print(\"Ensambler 50/50\")\n",
    "print(result_ensemble2)\n",
    "avg_result_ensemble2, dict_result_ensemble2 = dictionary(result_ensemble2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensambler 25/75\n",
      "{'faithfulness': 0.9613, 'answer_relevancy': 0.9009, 'context_precision': 0.4486, 'context_recall': 0.6963, 'answer_similarity': 0.8728, 'answer_correctness': 0.6790}\n",
      "The average score is: 0.7598177392513458\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_3 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_3], weights=[0.25,0.75])\n",
    "result_ensemble3 = run_and_evaluate(ensemble_retriever_3, prompt, llm)\n",
    "print(\"Ensambler 25/75\")\n",
    "print(result_ensemble3)\n",
    "avg_result_ensemble3, dict_result_ensemble3 = dictionary(result_ensemble3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:09<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensambler 10/90\n",
      "{'faithfulness': 0.9613, 'answer_relevancy': 0.8993, 'context_precision': 0.4486, 'context_recall': 0.7241, 'answer_similarity': 0.8728, 'answer_correctness': 0.6915}\n",
      "The average score is: 0.766267291905808\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_4 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_3], weights=[0.1,0.9])\n",
    "result_ensemble4 = run_and_evaluate(ensemble_retriever_4, prompt, llm)\n",
    "print(\"Ensambler 10/90\")\n",
    "print(result_ensemble4)\n",
    "avg_result_ensemble4, dict_result_ensemble4 = dictionary(result_ensemble4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-stage - reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker\n",
      "{'faithfulness': 0.8286, 'answer_relevancy': 0.8882, 'context_precision': 0.5185, 'context_recall': 0.7407, 'answer_similarity': 0.8663, 'answer_correctness': 0.6374}\n",
      "The average score is: 0.7466056282196497\n"
     ]
    }
   ],
   "source": [
    "retriever_context = retriever_3\n",
    "compressor = CohereRerank(top_n = 3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever_context\n",
    ")\n",
    "\n",
    "result_compression = run_and_evaluate(compression_retriever, prompt, llm)\n",
    "print(\"Reranker\")\n",
    "print(result_compression)\n",
    "avg_result_compression, dict_result_compression = dictionary(result_compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/54 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker\n",
      "{'faithfulness': 0.6468, 'answer_relevancy': 0.8313, 'context_precision': 0.6104, 'context_recall': 0.5648, 'answer_similarity': 0.8625, 'answer_correctness': 0.7789}\n",
      "The average score is: 0.7157937819391315\n"
     ]
    }
   ],
   "source": [
    "retriever_context_more = retriever_1000\n",
    "compressor = CohereRerank(top_n = 3)\n",
    "compression_retriever_more = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever_context_more\n",
    ")\n",
    "\n",
    "result_compression_more = run_and_evaluate(compression_retriever_more, prompt, llm)\n",
    "print(\"Reranker\")\n",
    "print(result_compression_more)\n",
    "avg_result_compression_more, dict_result_compression_more = dictionary(result_compression_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating context by remaking the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_context = \"Generate a search query to fetch the relevant documents using the user's {question}. Craft a query that specifically targets the keywords in the question. In the answer provide only the query.\"\n",
    "prompt_context = ChatPromptTemplate.from_template(template_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  96%|█████████▋| 52/54 [00:08<00:00,  4.30it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 54/54 [00:09<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.6627, 'answer_relevancy': 0.6392, 'context_precision': 0.6711, 'context_recall': 0.4074, 'answer_similarity': 0.5373, 'answer_correctness': 0.8619}\n"
     ]
    }
   ],
   "source": [
    "answers_final = []\n",
    "contexts_final = []\n",
    "llm_for_context =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt_context | llm}\n",
    ")\n",
    "for query in questions:\n",
    "    response_check = llm_for_context.invoke({\"question\": query})\n",
    "    search_query = response_check[\"response\"].content\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"context\"), \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "    docs = retriever_3.get_relevant_documents(search_query)\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        resulting_doc = doc.page_content\n",
    "        formatted_docs.append(resulting_doc)\n",
    "    try:  \n",
    "            response = retrieval_augmented_qa_chain.invoke({\"context\": formatted_docs, \"question\": query})\n",
    "            # Access the response content\n",
    "            answers_final.append(response[\"response\"].content)\n",
    "            contexts_final.append(formatted_docs)  \n",
    "    except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers_final.append(\"No answer\")\n",
    "            contexts_final.append(formatted_docs)\n",
    "\n",
    "\n",
    "result_search_query = evaluation_rag(questions, answers_final, contexts_final, ground_truths)\n",
    "print(result_search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.6299341753432749\n"
     ]
    }
   ],
   "source": [
    "avg_result_search_query, dict_result_search_query = dictionary(result_search_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change model to GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:08<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3 + gpt-4\n",
      "{'faithfulness': 0.8267, 'answer_relevancy': 0.8664, 'context_precision': 0.4815, 'context_recall': 0.7685, 'answer_similarity': 0.8805, 'answer_correctness': 0.6948}\n",
      "The average score is: 0.753078100078859\n"
     ]
    }
   ],
   "source": [
    "result_3_gpt4 = run_and_evaluate(retriever_3, prompt, llm_gpt4)\n",
    "print(\"k=3 + gpt-4\")\n",
    "print(result_3_gpt4)\n",
    "avg_result_3_gpt4, dict_result_3_gpt4 = dictionary(result_3_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" Answer the following usery query {question}. \n",
    "Answer should use the provided context and general knowledge. \n",
    "Context {context}.\"\"\"\n",
    "prompt_better = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:10<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=3 + gpt-4\n",
      "{'faithfulness': 0.6593, 'answer_relevancy': 0.7789, 'context_precision': 0.5926, 'context_recall': 0.8056, 'answer_similarity': 0.8779, 'answer_correctness': 0.6865}\n",
      "The average score is: 0.7334556911163995\n"
     ]
    }
   ],
   "source": [
    "result_3_gpt4_prompt = run_and_evaluate(retriever_3, prompt_better, llm_gpt4)\n",
    "print(\"k=3 + gpt-4\")\n",
    "print(result_3_gpt4_prompt)\n",
    "avg_result_3_gpt4_prompt, dict_result_3_gpt4_prompt = dictionary(result_3_gpt4_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
