{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "dotenv.load_dotenv()\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\")\n",
    "OPENAI_DEPLOYMENT = os.environ.get(\"OPENAI_DEPLOYMENT\")\n",
    "EMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "EMBEDDING_DEPLOYMENT = os.environ.get(\"EMBEDDING_DEPLOYMENT\")\n",
    "OPENAI_MODEL_GPT4 = os.environ.get(\"OPENAI_MODEL_GPT4\")\n",
    "OPENAI_DEPLOYMENT_GPT4 = os.environ.get(\"OPENAI_DEPLOYMENT_GPT4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_client = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDING_DEPLOYMENT,\n",
    "    openai_api_version=OPENAI_API_VERSION)\n",
    "llm = AzureChatOpenAI(model_name=OPENAI_MODEL, azure_deployment=OPENAI_DEPLOYMENT,temperature=0)\n",
    "llm_gpt4 = AzureChatOpenAI(model_name=OPENAI_MODEL_GPT4, azure_deployment=OPENAI_DEPLOYMENT_GPT4,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_llm(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  \n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_rag(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  # most likely\n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "        raise_exceptions=False,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"System\", \"Faithfulness\", \"Answer Relevancy\", \"Context Precision\", \"Context Recall\", \"Answer Similarity\", \"Answer Correctness\"]\n",
    "results_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_average = 0\n",
    "def find_highest(average_score):\n",
    "    global max_average\n",
    "    if average_score > max_average:\n",
    "        max_average = average_score\n",
    "        print(\"This is the new best value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary(result):\n",
    "    dict_result = dict(result)\n",
    "    average_score = sum(dict_result.values()) / len(dict_result)\n",
    "    print(f\"The average score is: {average_score}\")\n",
    "    find_highest(average_score)\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(system_name, questions, answers, contexts, ground_truths):\n",
    "    result = evaluation_rag(questions, answers, contexts, ground_truths)\n",
    "    average = dictionary(result)\n",
    "    # Create a dictionary to store the results\n",
    "    system_results = {\n",
    "        \"System\": system_name,\n",
    "        \"Faithfulness\": result[\"faithfulness\"],\n",
    "        \"Answer Relevancy\": result[\"answer_relevancy\"],\n",
    "        \"Context Precision\": result[\"context_precision\"],\n",
    "        \"Context Recall\": result[\"context_recall\"],\n",
    "        \"Answer Similarity\": result[\"answer_similarity\"],\n",
    "        \"Answer Correctness\": result[\"answer_correctness\"],\n",
    "        \"Average\": average\n",
    "    }\n",
    "    df_system_results = pd.DataFrame([system_results])\n",
    "    return df_system_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LLM(system_name, questions, answers, contexts, ground_truths):\n",
    "    result = evaluation_rag(questions, answers, contexts, ground_truths)\n",
    "    average = dictionary(result)\n",
    "    # Create a dictionary to store the results\n",
    "    system_results = {\n",
    "        \"System\": system_name,\n",
    "        \"Faithfulness\": result[\"faithfulness\"],\n",
    "        \"Answer Relevancy\": result[\"answer_relevancy\"],\n",
    "        \"Context Precision\": np.nan,\n",
    "        \"Context Recall\": np.nan,\n",
    "        \"Answer Similarity\": result[\"answer_similarity\"],\n",
    "        \"Answer Correctness\": result[\"answer_correctness\"],\n",
    "        \"Average\": average\n",
    "    }\n",
    "    df_llm_results = pd.DataFrame([system_results])\n",
    "    return df_llm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the paper by Chen et al. about?\",\n",
    "    \"What is the main focus and contribution of the paper by Cheonsu Jeong regarding generative AI technology, specifically in addressing information scarcity, and what is the significance of the proposed Retrieval-Augmented Generation (RAG) model in improving content generation within the Large Language Models (LLM) application architecture?\",\n",
    "    \"What are the conclusions of CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models?\",\n",
    "    \"How does the proposed approach in the 'Efficient Retrieval Augmented Generation from Unstructured Knowledge for Task-Oriented Dialog' address the computational complexity issue in knowledge selection and what methods are employed in response generation to consider multiple knowledge snippets?\",\n",
    "    \"What challenges and potential applications are discussed for large language models like GPT in the context of automated form completion and knowledge-intensive tasks, and what innovative methods are proposed to address these challenges in each scenario?\",\n",
    "    \"Explain Metacognitive RAG\",\n",
    "    \"What is the HybridRAG framework and its main components, and how does it aim to enhance the performance of a client-based language model?\",\n",
    "    \"What are the reasons for using GPT-4 over GPT-3.5?\",\n",
    "    \"What are the key challenges in the implementation of RAG systems, and how do the authors of 'THE CHRONICLES OF RAG: THE RETRIEVER, THE CHUNK AND THE GENERATOR' propose to address them, particularly focusing on retriever quality, input size optimization, and overall performance evaluation?\",\n",
    "    \"Why is RAG preferred over general LLMs?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    [\"The research paper 'Benchmarking Large Language Models in Retrieval-Augmented Generation' evaluates the abilities of Large Language Models (LLMs) in utilizing retrieved information for generation tasks. It introduces the Retrieval-Augmented Generation Benchmark (RGB) to assess LLMs' capabilities in noisy environments, negative rejection scenarios, information integration, and counterfactual robustness. The paper aims to comprehensively evaluate LLMs' performance in effectively utilizing external information and overcoming challenges in information retrieval.\"],\n",
    "    [\"The research paper focuses on implementing generative AI services using the Large Language Models (LLM) application architecture. The main contributions of the paper include addressing information scarcity within LLM by proposing specific remedies using fine-tuning techniques and direct document integration. The significant contribution is the development of the Retrieval-Augmented Generation (RAG) model, which enhances information storage and retrieval processes, leading to improved content generation within enterprises utilizing LLMs. The RAG model combines information retrieval with text generation, improving the accuracy and reliability of generated content, specifically addressing challenges related to data insufficiency within LLMs.\"],\n",
    "    [\"The paper introduces a comprehensive framework, CRUD-RAG, for evaluating retrieval-augmented generation (RAG) systems in text generation tasks. The framework categorizes tasks into CRUD types (Create, Read, Update, Delete), providing a structured assessment of RAG capabilities. Large-scale datasets for each CRUD category are created to evaluate RAG performance under various conditions. Experimental comparisons demonstrate that RAG systems significantly improve content quality by incorporating information from external sources. The study emphasizes the importance of fine-tuning RAG systems, addressing factors such as retrieval model optimization, context length, knowledge base construction, and large language model deployment. The insights aim to guide researchers and practitioners, offering a roadmap for the development and refinement of RAG systems. The paper envisions stimulating further exploration and innovation in RAG technologies to advance text generation applications and enhance the integration of retrieval mechanisms and language models for more intelligent and context-aware generative systems.\"],\n",
    "    [\"The proposed approach addresses the computational complexity issue in knowledge selection by introducing the Hierarchical Selection method. This method facilitates the reduction of computational cost by splitting the task of classifying a relevant document into subsequent stages. By first identifying the relevant domain and entity before selecting the relevant snippet among the documents of that entity, the approach is able to significantly reduce the computational cost. In response generation, the approach employs Retrieval Augmented Generation (RAG) to consider multiple knowledge snippets. By using RAG, the model is able to generate responses based on multiple selected snippets, allowing for a more comprehensive and informed generation process. This method incorporates the knowledge from multiple sources to enhance the response generation process. Additionally, the knowledge selection process is optimized through the use of metadata, and two variants are presented: one involving three separate models for domain, entity, and document retrieval, and the other involving joint retrieval of domain and entity. This reduces the computational complexity from O(|K|) to O(|D| + |E|) and O(|E| + |K|) for the first and second variants, respectively. The response generation task is formulated as a marginalization over selected knowledge snippets, enabling the consideration of multiple snippets in generating responses and addressing potential errors introduced by a hard decision in the selection step. The method involves a probability distribution over knowledge snippets given a dialog context and includes both selection and generation probabilities.\"],\n",
    "    [\"The challenges and potential applications discussed for large language models like GPT include their remarkable natural language processing capabilities in tasks such as automated form completion and knowledge-intensive tasks like open-domain question answering (QA), fact checking, and dialogue systems. In the context of automated form completion, challenges include understanding form layout, guidelines, user intent, and reasoning over data. The proposed method involves creating a knowledge base, augmenting the language model with the knowledge base using retrieval-augmented generation, and using prompt engineering techniques. In knowledge-intensive tasks, the text introduces a generate-then-read (GENREAD) method, replacing document retrievers with large language model generators. Additionally, the Hybrid Retrieval-Augmented Generation (HybridRAG) framework is proposed to efficiently combine a cloud-based LLM with a smaller client-side language model. Another aspect is the introduction of MetaRAG, which combines retrieval-augmented generation with metacognition to enhance introspective reasoning abilities and improve response strategies through a three-step metacognitive regulation pipeline.\"],\n",
    "    [\"Metacognitive Retrieval-Augmented Generation (MetaRAG) is a framework for enhancing retrieval-augmented generation processes in large language models through the integration of metacognition. MetaRAG combines the capabilities of retrieval systems with metacognitive abilities inspired by human cognitive processes. It allows the model to monitor, evaluate, and plan its response strategies, enabling introspective reasoning and self-awareness in the cognitive process. This approach aims to identify and rectify errors or shortcomings in the model's reasoning, ultimately leading to more accurate and precise answer derivation.\"],\n",
    "    [\"The HybridRAG framework comprises an augmentation coordinator (client), a memory-augmented client model (client), a retriever model (cloud), and a language model (LLM)-based memory generator (cloud). The augmentation coordinator monitors writing context and requests augmented memory from the cloud based on context changes. The retriever model searches for relevant data in a retrieval corpus using Dense Passage Retrieval. The LLM-based memory generator generates concise key takeaways from retrieved documents, optimizing memory size. The memory is integrated into the client model to enhance its overall performance. The client model is fine-tuned using instruction-based prompts and LLM-generated references to effectively leverage cloud-generated memory.\"],\n",
    "    [\"Enhanced Performance: GPT-4 surpasses GPT-3.5 in various tasks, particularly excelling in question-answering scenarios that demand reasoning across multiple documents. Advanced Knowledge Integration: GPT-4 exhibits remarkable proficiency in fusing knowledge, especially when a comprehensive understanding of information from diverse documents is imperative. This underscores its superior capability in handling intricate tasks. Efficient Text Refinement: GPT-4 effectively refines and corrects text in scenarios requiring modification, maintaining accuracy and content relevance while addressing errors or hallucinations in the text. Concise Summary Production: GPT-4 generates concise summaries by eliminating redundant information, showcasing its efficiency in creating informative and succinct content. Robust Performance: While GPT-4 stands out as the preferred choice due to its overall outstanding performance, models like Qwen-7B and Qwen-14B also demonstrate competitive results in tasks such as text continuation, summary generation, and question answering.\"],\n",
    "    [\"The challenges in RAG system implementation include effective integration of retrieval models, representation learning efficiency, diverse data handling, computational efficiency optimization, and quality evaluation. The article proposes best practices for implementing RAG on a Brazilian Portuguese dataset, emphasizing a simplified pipeline. Key findings include a 35.4% improvement in MRR@10 with retriever quality enhancement, a 2.4% performance boost through input size optimization, and a final accuracy of 98.61%, marking a 40.73% improvement compared to the baseline. The study underscores the importance of data quality in input, retriever, and evaluation, providing insights into formulating queries, understanding information retrieval behavior, and defining evaluation metrics. The future work involves exploring additional datasets for segmentation and chunk construction techniques.\"],\n",
    "    [\"RAG (Retrieval Augmented Generation) is preferred over regular Large Language Models (LLMs) due to its ability to access external data, address training limitations, and mitigate hallucinations by integrating retrieved information. It leverages an effective retriever pipeline for coherent responses, enhancing text generation quality with relevant external information. RAG employs external knowledge sources, frameworks like LlamaIndex and LangChain, and metacognitive abilities to improve response credibility, accuracy, and reasoning over multiple evidence pieces. It offers benefits such as enhanced utility, low latency through HybridRAG, reduced client-to-cloud communication, and efficient memory integration. In form filling, RAG accesses additional information for accurate responses, correcting errors, and conditioning response generation on selected knowledge snippets. It mitigates issues in LLMs, such as factual errors and outdated knowledge, providing a promising solution by integrating external knowledge for more reliable and coherent responses.\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General answer by LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm}\n",
    ")\n",
    "llm_chain_gpt4 =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm_gpt4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_llm = []\n",
    "contexts_llm = [[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in questions:\n",
    "    response = llm_chain.invoke({\"question\": query})\n",
    "    answers_llm.append(response[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.7011075175058377\n",
      "This is the new best value!\n",
      "    System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0  GPT-3.5           1.0          0.743175                NaN             NaN   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.915695            0.487775  0.701108  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sigitalapina\\AppData\\Local\\Temp\\ipykernel_26452\\800954320.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, llm_results], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "llm_results = evaluate_LLM(\"GPT-3.5\", questions, answers_llm, contexts_llm, ground_truths)\n",
    "results_df = pd.concat([results_df, llm_results], ignore_index=True)\n",
    "print(llm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.6902620942552353\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0  GPT-4      0.971429          0.723279                NaN             NaN   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.902019            0.484846  0.690262  \n"
     ]
    }
   ],
   "source": [
    "answers_llm_gpt4 = []\n",
    "for query in questions:\n",
    "    response = llm_chain_gpt4.invoke({\"question\": query})\n",
    "    answers_llm_gpt4.append(response[\"response\"].content)\n",
    "llm_results_gpt4 = evaluate_LLM(\"GPT-4\",questions, answers_llm_gpt4, contexts_llm, ground_truths)\n",
    "results_df = pd.concat([results_df, llm_results_gpt4], ignore_index=True)\n",
    "print(llm_results_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"User input {question}. \n",
    "Context {context}.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_chain(prompt, retriever, llm):\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    return retrieval_augmented_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../papers', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "# text_splitter = CharacterTextSplitter()\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "# db_naive = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"../papers/vectordb-edit/naive\")\n",
    "# db_naive.persist()\n",
    "# retriever_naive = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_naive = Chroma(persist_directory = \"../papers/vectordb-edit/naive\", embedding_function=embeddings_client)\n",
    "retriever_naive = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_naive = []\n",
    "contexts_naive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_naive, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_naive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_naive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_naive.append(\"No answer\")\n",
    "        context_full = retriever_naive.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_naive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  30%|███       | 18/60 [00:05<00:09,  4.44it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8784985152727972\n",
      "This is the new best value!\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0  Naive      0.686914          0.964286           0.874123        0.905556   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.930638            0.909474  0.878499  \n"
     ]
    }
   ],
   "source": [
    "result_naive_rag = evaluate_system(\"Naive\", questions, answers_naive, contexts_naive, ground_truths)\n",
    "results_df = pd.concat([results_df, result_naive_rag], ignore_index=True)\n",
    "print(result_naive_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recursive splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "# text_splitter = text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder()\n",
    "# chunks_r = text_splitter.split_documents(documents)\n",
    "# db_basic = Chroma.from_documents(chunks_r, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_basic\")\n",
    "# db_basic.persist()\n",
    "# retriever_basic = db_basic.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_basic = Chroma(persist_directory = \"../papers/vectordb-edit/recursive_basic\", embedding_function=embeddings_client)\n",
    "retriever_basic = db_basic.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_recursive = []\n",
    "contexts_recursive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_basic, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_recursive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_recursive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_recursive.append(\"No answer\")\n",
    "        context_full = retriever_basic.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_recursive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  18%|█▊        | 11/60 [00:05<00:18,  2.61it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-253' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-254' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating:  20%|██        | 12/60 [00:05<00:17,  2.79it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:10<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8775262827504023\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.669849</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.885355</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>0.877526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "0  Recursive      0.669849          0.964286           0.885355   \n",
       "\n",
       "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "0        0.905556           0.930638            0.909474  0.877526  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_recursive = evaluate_system(\"Recursive\", questions, answers_naive, contexts_naive, ground_truths)\n",
    "results_df = pd.concat([results_df, result_recursive], ignore_index=True)\n",
    "result_recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_evaluate(name, retriever, prompt, llm, results_df):\n",
    "    answers = []\n",
    "    contexts_extra = []\n",
    "\n",
    "    for query in questions:\n",
    "        try:  \n",
    "            response = retrieval_chain(prompt, retriever, llm).invoke({\"question\": query})\n",
    "            # Access the response content\n",
    "            answers.append(response[\"response\"].content)\n",
    "            # Access the context content\n",
    "            context_content = [context.page_content for context in response[\"context\"]]\n",
    "            contexts_extra.append(context_content)  \n",
    "        except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers.append(\"No answer\")\n",
    "            context_full = retriever.get_relevant_documents(query)\n",
    "            context_content = [context.page_content for context in context_full]\n",
    "            contexts_extra.append(context_content)\n",
    "\n",
    "    result = evaluate_system(name, questions, answers, contexts_extra, ground_truths)\n",
    "    results_df = pd.concat([results_df, result], ignore_index=True)\n",
    "    return result, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks for chunk size 500, overlap 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  23%|██▎       | 14/60 [00:04<00:16,  2.87it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8478294359303452\n",
      "CHUNK SIZE 500, 0% overlap:\n",
      "                  System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 0%      0.953704          0.806519           0.805556   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.9            0.93744            0.683758  0.847829  \n",
      "Number of chunks for chunk size 500, overlap 5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  72%|███████▏  | 43/60 [00:05<00:01, 15.55it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8932811010205719\n",
      "This is the new best value!\n",
      "CHUNK SIZE 500, 5% overlap:\n",
      "                  System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 5%      0.944444          0.933492           0.891667   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.9           0.947441            0.742642  0.893281  \n",
      "Number of chunks for chunk size 500, overlap 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  23%|██▎       | 14/60 [00:04<00:16,  2.75it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8524234424988394\n",
      "CHUNK SIZE 500, 10% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 10%      0.705793          0.974965            0.80458   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.872222           0.875755            0.881225  0.852423  \n",
      "Number of chunks for chunk size 500, overlap 15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-490' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-491' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-492' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-493' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-494' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-495' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-496' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-497' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating:  20%|██        | 12/60 [00:04<00:14,  3.23it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8761099363291128\n",
      "CHUNK SIZE 500, 15% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 15%      0.752637          0.907425           0.939409   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness  Average  \n",
      "0            0.85            0.98129            0.825899  0.87611  \n",
      "Number of chunks for chunk size 500, overlap 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  23%|██▎       | 14/60 [00:04<00:16,  2.74it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating:  27%|██▋       | 16/60 [00:05<00:13,  3.16it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8752834041915757\n",
      "CHUNK SIZE 500, 20% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 20%      0.984127          0.931073                0.8   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0           0.875           0.938601            0.722899  0.875283  \n",
      "Number of chunks for chunk size 1000, overlap 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.855402338615498\n",
      "CHUNK SIZE 1000, 0% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 0%           1.0          0.811036           0.858333   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0          0.8675           0.927681            0.667863  0.855402  \n",
      "Number of chunks for chunk size 1000, overlap 5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.854468635603447\n",
      "CHUNK SIZE 1000, 5% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 5%        0.8775           0.89488           0.855556   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.848889           0.948182            0.701805  0.854469  \n",
      "Number of chunks for chunk size 1000, overlap 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8163619543706294\n",
      "CHUNK SIZE 1000, 10% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 10%         0.875          0.724113           0.836111   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.863333           0.927738            0.671876  0.816362  \n",
      "Number of chunks for chunk size 1000, overlap 15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  23%|██▎       | 14/60 [00:05<00:17,  2.68it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8387833135494179\n",
      "CHUNK SIZE 1000, 15% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 15%      0.687556           0.85145           0.853393   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0           0.825            0.94969            0.865611  0.838783  \n",
      "Number of chunks for chunk size 1000, overlap 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8426677663390895\n",
      "CHUNK SIZE 1000, 20% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 20%      0.920635          0.797818           0.822222   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.9           0.936076            0.679255  0.842668  \n",
      "Number of chunks for chunk size 2000, overlap 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  33%|███▎      | 20/60 [00:05<00:06,  5.74it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8699671378021915\n",
      "CHUNK SIZE 2000, 0% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 0%      0.679827          0.942857            0.88572   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.922222           0.870638            0.918538  0.869967  \n",
      "Number of chunks for chunk size 2000, overlap 5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8794751429774427\n",
      "CHUNK SIZE 2000, 5% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 5%           1.0          0.880269           0.897222   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0           0.835           0.937845            0.726514  0.879475  \n",
      "Number of chunks for chunk size 2000, overlap 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  35%|███▌      | 21/60 [00:05<00:07,  5.16it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8970455466967152\n",
      "This is the new best value!\n",
      "CHUNK SIZE 2000, 10% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 10%      0.716439               1.0           0.877715   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.938889           0.930638            0.918592  0.897046  \n",
      "Number of chunks for chunk size 2000, overlap 15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  28%|██▊       | 17/60 [00:05<00:09,  4.32it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8802579176463924\n",
      "CHUNK SIZE 2000, 15% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 15%      0.701334               1.0           0.891013   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.880556           0.890638            0.918007  0.880258  \n",
      "Number of chunks for chunk size 2000, overlap 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  18%|█▊        | 11/60 [00:04<00:18,  2.59it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:11,  3.97it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8739585054178122\n",
      "CHUNK SIZE 2000, 20% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 20%      0.815049          0.826622           0.906254   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.928893            0.85286            0.914072  0.873959  \n",
      "Number of chunks for chunk size 3000, overlap 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  20%|██        | 12/60 [00:05<00:15,  3.12it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:11<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8644635704772573\n",
      "CHUNK SIZE 3000, 0% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 0%       0.69633               1.0           0.796392   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.922222           0.855638            0.916199  0.864464  \n",
      "Number of chunks for chunk size 3000, overlap 5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  18%|█▊        | 11/60 [00:05<00:20,  2.40it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:10<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8405938442566869\n",
      "CHUNK SIZE 3000, 5% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 5%      0.697668          0.982143           0.798822   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.822222           0.830638             0.91207  0.840594  \n",
      "Number of chunks for chunk size 3000, overlap 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  33%|███▎      | 20/60 [00:05<00:07,  5.24it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8963464067484345\n",
      "CHUNK SIZE 3000, 10% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 10%      0.712127               1.0            0.86971   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.947222           0.930638            0.918381  0.896346  \n",
      "Number of chunks for chunk size 3000, overlap 15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-1421' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1422' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1423' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1424' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1425' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1426' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1427' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1428' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating:  23%|██▎       | 14/60 [00:05<00:18,  2.43it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8572789644008911\n",
      "CHUNK SIZE 3000, 15% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 15%      0.690314               1.0           0.804194   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.905556           0.830638            0.912973  0.857279  \n",
      "Number of chunks for chunk size 3000, overlap 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  33%|███▎      | 20/60 [00:05<00:08,  4.95it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:10<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8377334742206918\n",
      "CHUNK SIZE 3000, 20% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 20%      0.671085               1.0            0.79439   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.797222           0.850638            0.913066  0.837733  \n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [500, 1000, 2000, 3000]\n",
    "overlap_percentages = [0, 5, 10, 15, 20]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    for overlap_percentage in overlap_percentages:\n",
    "        # Calculate overlap based on percentage\n",
    "        chunk_overlap = int(chunk_size * overlap_percentage / 100)\n",
    "        \n",
    "        # Create text splitter\n",
    "        # text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        \n",
    "        # # Split documents\n",
    "        # chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Print number of chunks\n",
    "        print(f\"Number of chunks for chunk size {chunk_size}, overlap {overlap_percentage}%\")\n",
    "        \n",
    "        # Create Chroma database\n",
    "        # db = Chroma.from_documents(chunks, embeddings_client, persist_directory=f\"../papers/vectordb-edit/chunking_{chunk_size}_{overlap_percentage}\")\n",
    "        db = Chroma(persist_directory = f\"../papers/vectordb-edit/chunking_{chunk_size}_{overlap_percentage}\", embedding_function=embeddings_client)\n",
    "        db.persist()\n",
    "        \n",
    "        # Create retriever\n",
    "        retriever = db.as_retriever()\n",
    "        \n",
    "        # Run and evaluate\n",
    "        result,results_df = run_and_evaluate(f\"Chunk {chunk_size}, overlap {overlap_percentage}%\", retriever, prompt, llm, results_df)\n",
    "        print(f\"CHUNK SIZE {chunk_size}, {overlap_percentage}% overlap:\")\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(f\"../papers/results/results_summarize.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 100)\n",
    "# chunks_1000 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_1000))\n",
    "# db_1000 = Chroma.from_documents(chunks_1000, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_1000\")\n",
    "# db_1000.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_1000 = Chroma(persist_directory = \"../papers/vectordb-edit/recursive_1000\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_1000 = db_1000.as_retriever()\n",
    "# result_1000 = run_and_evaluate(retriever_1000, prompt, llm)\n",
    "# print(\"CHUNK SIZE 1000\")\n",
    "# print(result_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_result_1000 = dictionary(result_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500, chunk_overlap = 50)\n",
    "# chunks_500 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_500))\n",
    "# db_500 = Chroma.from_documents(chunks_500, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_500\")\n",
    "# db_500.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_500 = Chroma(persist_directory = \"../papers/vectordb-edit/recursive_500\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_500 = db_500.as_retriever()\n",
    "# result_500 = run_and_evaluate(retriever_500, prompt, llm)\n",
    "# print(\"CHUNK SIZE 500\")\n",
    "# print(result_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_result_500 = dictionary(result_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 2000, chunk_overlap = 200)\n",
    "# chunks_2000 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_2000))\n",
    "# db_2000 = Chroma.from_documents(chunks_2000, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_2000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_2000 = Chroma(persist_directory = \"../papers/vectordb-edit/recursive_2000\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_2000 = db_2000.as_retriever()\n",
    "# result_2000 = run_and_evaluate(retriever_2000, prompt, llm)\n",
    "# print(\"CHUNK SIZE 2000\")\n",
    "# print(result_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_result_2000 = dictionary(result_2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chunk = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 3000, chunk_overlap = 300)\n",
    "# chunks_3000 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_3000))\n",
    "# db_3000 = Chroma.from_documents(chunks_3000, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_3000 = Chroma(persist_directory = \"../papers/vectordb-edit/recursive_3000\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_3000 = db_3000.as_retriever()\n",
    "# result_3000 = run_and_evaluate(retriever_3000, prompt, llm)\n",
    "# print(\"CHUNK SIZE 3000\")\n",
    "# print(result_3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_result_3000 = dictionary(result_3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.716439</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877715</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.918592</td>\n",
       "      <td>0.897046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     System  Faithfulness  Answer Relevancy  \\\n",
       "16  Chunk 2000, overlap 10%      0.716439               1.0   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "16           0.877715        0.938889           0.930638            0.918592   \n",
       "\n",
       "     Average  \n",
       "16  0.897046  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest = results_df.nlargest(1, \"Average\")\n",
    "highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"../papers/results/results_summarize.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now time to look for different top-k\n",
    "\n",
    "Note: We continue with the size chunk of 1000 as it had the highest average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_k = Chroma(persist_directory = \"../papers/vectordb-edit/chunking_2000_10\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   2%|▏         | 1/60 [00:02<02:17,  2.33s/it]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.882500889118932\n",
      "Results for K=2:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 2000, overlap 10%, K=2      0.937241          0.823195   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.973148            0.92           0.935414            0.706006   \n",
      "\n",
      "    Average  \n",
      "0  0.882501  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.9186200201759759\n",
      "This is the new best value!\n",
      "Results for K=3:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 2000, overlap 10%, K=3      0.946429           0.91532   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.966667             1.0           0.943323            0.739982   \n",
      "\n",
      "   Average  \n",
      "0  0.91862  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:11<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.9114472045250462\n",
      "Results for K=5:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 2000, overlap 10%, K=5           1.0          0.849876   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.902917             1.0            0.94531            0.770581   \n",
      "\n",
      "    Average  \n",
      "0  0.911447  \n",
      "Warning: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8239 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}on the following question: How does the proposed approach in the 'Efficient Retrieval Augmented Generation from Unstructured Knowledge for Task-Oriented Dialog' address the computational complexity issue in knowledge selection and what methods are employed in response generation to consider multiple knowledge snippets?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  17%|█▋        | 10/60 [00:02<00:09,  5.42it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8268 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:13,  3.32it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:12<00:00,  2.45it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8795 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating:  98%|█████████▊| 59/60 [01:43<00:22, 22.61s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8530 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating: 100%|██████████| 60/60 [02:40<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8800784633872859\n",
      "Results for K=6:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 2000, overlap 10%, K=6      0.857163          0.901688   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.923695        0.910616           0.878918             0.80839   \n",
      "\n",
      "    Average  \n",
      "0  0.880078  \n",
      "Warning: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8520 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}on the following question: What is the paper by Chen et al. about?\n",
      "Warning: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8822 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}on the following question: How does the proposed approach in the 'Efficient Retrieval Augmented Generation from Unstructured Knowledge for Task-Oriented Dialog' address the computational complexity issue in knowledge selection and what methods are employed in response generation to consider multiple knowledge snippets?\n",
      "Warning: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8721 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}on the following question: What challenges and potential applications are discussed for large language models like GPT in the context of automated form completion and knowledge-intensive tasks, and what innovative methods are proposed to address these challenges in each scenario?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8416 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating:   2%|▏         | 1/60 [00:00<00:13,  4.29it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8656 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8516 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating:   5%|▌         | 3/60 [00:00<00:06,  9.10it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8557 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8797 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating:   8%|▊         | 5/60 [00:02<00:27,  1.99it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8657 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:12<00:01,  2.79it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 8862 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating:  97%|█████████▋| 58/60 [01:30<00:32, 16.39s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 9324 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating:  98%|█████████▊| 59/60 [02:15<00:22, 22.93s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 9100 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Evaluating: 100%|██████████| 60/60 [02:16<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8673777310114494\n",
      "Results for K=7:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 2000, overlap 10%, K=7      0.981916          0.833274   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.852652            0.95           0.905625            0.680799   \n",
      "\n",
      "    Average  \n",
      "0  0.867378  \n"
     ]
    }
   ],
   "source": [
    "k_values = [2, 3, 5, 6, 7]\n",
    "\n",
    "# Iterate over different k values\n",
    "for k in k_values:\n",
    "    # Create retriever with k value\n",
    "    retriever = db_k.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    # Run and evaluate\n",
    "    result,results_df = run_and_evaluate(f\"Chunk size 2000, overlap 10%, K={k}\", retriever, prompt, llm, results_df)\n",
    "    print(f\"Results for K={k}:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_3 = db_1000.as_retriever(search_kwargs={\"k\": 3})\n",
    "# result_3 = run_and_evaluate(retriever_3, prompt, llm)\n",
    "# print(\"CHUNK SIZE 1000, K=3\")\n",
    "# print(result_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_result_3 = dictionary(result_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_5 = db_1000.as_retriever(search_kwargs={\"k\": 5})\n",
    "# result_5 = run_and_evaluate(retriever_5, prompt, llm)\n",
    "# print(\"CHUNK SIZE 1000, K=5\")\n",
    "# print(result_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_result_5 = dictionary(result_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_6= db_1000.as_retriever(search_kwargs={\"k\": 6})\n",
    "# result_6 = run_and_evaluate(retriever_6, prompt, llm)\n",
    "# print(\"CHUNK SIZE 1000, K=5\")\n",
    "# print(result_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_result_6 = dictionary(result_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_7= db_1000.as_retriever(search_kwargs={\"k\": 7})\n",
    "# result_7 = run_and_evaluate(retriever_7, prompt, llm)\n",
    "# print(\"CHUNK SIZE 1000, K=7\")\n",
    "# print(result_7)\n",
    "# dict_result_7 = dictionary(result_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look for different retrievers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parent document retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  27%|██▋       | 16/60 [00:04<00:10,  4.05it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8408850037495385\n",
      "                      System  Faithfulness  Answer Relevancy  \\\n",
      "0  Parent Retriever 1000-200      0.693712           0.90409   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.764877             0.9           0.955329            0.827303   \n",
      "\n",
      "    Average  \n",
      "0  0.840885  \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap = 200)\n",
    "child_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap = 0)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents\",persist_directory = \"../papers/vectordb-edit/summary-parent\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "parent_document_retriever.add_documents(documents)\n",
    "result_parent, results_df = run_and_evaluate(f\"Parent Retriever 1000-200\", parent_document_retriever, prompt, llm, results_df)\n",
    "print(result_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:31<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8675080541266112\n",
      "                     System  Faithfulness  Answer Relevancy  \\\n",
      "0  Parent Retriever 500-100      0.962963          0.802671   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0                0.9             0.9           0.934762            0.704652   \n",
      "\n",
      "    Average  \n",
      "0  0.867508  \n"
     ]
    }
   ],
   "source": [
    "parent_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap = 50)\n",
    "child_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap = 0)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents_small\",persist_directory = \"../papers/vectordb-edit/summary-parent_small\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_small = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_small,\n",
    "    parent_splitter=parent_splitter_small,\n",
    ")\n",
    "parent_document_retriever_small.add_documents(documents)\n",
    "result_parent_small, results_df = run_and_evaluate(f\"Parent Retriever 500-100\", parent_document_retriever_small, prompt, llm, results_df)\n",
    "print(result_parent_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1500, chunk_overlap = 150)\n",
    "child_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap = 0)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents_large\",persist_directory = \"../papers/vectordb-edit/summary-parent_large\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_large = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_large,\n",
    "    parent_splitter=parent_splitter_large,\n",
    ")\n",
    "parent_document_retriever_large.add_documents(documents)\n",
    "result_parent_large , results_df = run_and_evaluate(f\"Parent Retriever 1500-200\", parent_document_retriever_small, prompt, llm, results_df)\n",
    "print(result_parent_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.743175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915695</td>\n",
       "      <td>0.487775</td>\n",
       "      <td>0.701108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.723279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.902019</td>\n",
       "      <td>0.484846</td>\n",
       "      <td>0.690262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.686914</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.874123</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>0.878499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.669849</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.885355</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>0.877526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.806519</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.937440</td>\n",
       "      <td>0.683758</td>\n",
       "      <td>0.847829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.933492</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.947441</td>\n",
       "      <td>0.742642</td>\n",
       "      <td>0.893281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>0.705793</td>\n",
       "      <td>0.974965</td>\n",
       "      <td>0.804580</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.875755</td>\n",
       "      <td>0.881225</td>\n",
       "      <td>0.852423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.752637</td>\n",
       "      <td>0.907425</td>\n",
       "      <td>0.939409</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.981290</td>\n",
       "      <td>0.825899</td>\n",
       "      <td>0.876110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.931073</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.938601</td>\n",
       "      <td>0.722899</td>\n",
       "      <td>0.875283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811036</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.927681</td>\n",
       "      <td>0.667863</td>\n",
       "      <td>0.855402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>0.894880</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.848889</td>\n",
       "      <td>0.948182</td>\n",
       "      <td>0.701805</td>\n",
       "      <td>0.854469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.724113</td>\n",
       "      <td>0.836111</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.927738</td>\n",
       "      <td>0.671876</td>\n",
       "      <td>0.816362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>0.687556</td>\n",
       "      <td>0.851450</td>\n",
       "      <td>0.853393</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.949690</td>\n",
       "      <td>0.865611</td>\n",
       "      <td>0.838783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.797818</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.936076</td>\n",
       "      <td>0.679255</td>\n",
       "      <td>0.842668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.679827</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.885720</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.870638</td>\n",
       "      <td>0.918538</td>\n",
       "      <td>0.869967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880269</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.937845</td>\n",
       "      <td>0.726514</td>\n",
       "      <td>0.879475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.716439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877715</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.918592</td>\n",
       "      <td>0.897046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.701334</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891013</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.890638</td>\n",
       "      <td>0.918007</td>\n",
       "      <td>0.880258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.815049</td>\n",
       "      <td>0.826622</td>\n",
       "      <td>0.906254</td>\n",
       "      <td>0.928893</td>\n",
       "      <td>0.852860</td>\n",
       "      <td>0.914072</td>\n",
       "      <td>0.873959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.696330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796392</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.855638</td>\n",
       "      <td>0.916199</td>\n",
       "      <td>0.864464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.697668</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.798822</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.830638</td>\n",
       "      <td>0.912070</td>\n",
       "      <td>0.840594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.712127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869710</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.918381</td>\n",
       "      <td>0.896346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.690314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.804194</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.830638</td>\n",
       "      <td>0.912973</td>\n",
       "      <td>0.857279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.671085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794390</td>\n",
       "      <td>0.797222</td>\n",
       "      <td>0.850638</td>\n",
       "      <td>0.913066</td>\n",
       "      <td>0.837733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=2</td>\n",
       "      <td>0.937241</td>\n",
       "      <td>0.823195</td>\n",
       "      <td>0.973148</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.935414</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0.882501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=3</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.915320</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943323</td>\n",
       "      <td>0.739982</td>\n",
       "      <td>0.918620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.849876</td>\n",
       "      <td>0.902917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945310</td>\n",
       "      <td>0.770581</td>\n",
       "      <td>0.911447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=6</td>\n",
       "      <td>0.857163</td>\n",
       "      <td>0.901688</td>\n",
       "      <td>0.923695</td>\n",
       "      <td>0.910616</td>\n",
       "      <td>0.878918</td>\n",
       "      <td>0.808390</td>\n",
       "      <td>0.880078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=7</td>\n",
       "      <td>0.981916</td>\n",
       "      <td>0.833274</td>\n",
       "      <td>0.852652</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.905625</td>\n",
       "      <td>0.680799</td>\n",
       "      <td>0.867378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.723332</td>\n",
       "      <td>0.630200</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.727664</td>\n",
       "      <td>0.592638</td>\n",
       "      <td>0.524651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MMR</td>\n",
       "      <td>0.715055</td>\n",
       "      <td>0.901328</td>\n",
       "      <td>0.892569</td>\n",
       "      <td>0.867301</td>\n",
       "      <td>0.936088</td>\n",
       "      <td>0.781449</td>\n",
       "      <td>0.848965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.634115</td>\n",
       "      <td>0.487036</td>\n",
       "      <td>0.329077</td>\n",
       "      <td>0.194862</td>\n",
       "      <td>0.468355</td>\n",
       "      <td>0.725303</td>\n",
       "      <td>0.473125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ensambler 1</td>\n",
       "      <td>0.875663</td>\n",
       "      <td>0.869943</td>\n",
       "      <td>0.383121</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.945115</td>\n",
       "      <td>0.735196</td>\n",
       "      <td>0.775951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ensambler 2</td>\n",
       "      <td>0.790304</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.700311</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.861840</td>\n",
       "      <td>0.916791</td>\n",
       "      <td>0.805152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ensambler 3</td>\n",
       "      <td>0.701580</td>\n",
       "      <td>0.968679</td>\n",
       "      <td>0.778720</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890661</td>\n",
       "      <td>0.892838</td>\n",
       "      <td>0.866524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Search query</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.897058</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.948863</td>\n",
       "      <td>0.718350</td>\n",
       "      <td>0.867379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=3, GPT-4</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.898766</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946864</td>\n",
       "      <td>0.736261</td>\n",
       "      <td>0.918278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=5, GPT-4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823842</td>\n",
       "      <td>0.886250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949949</td>\n",
       "      <td>0.721710</td>\n",
       "      <td>0.896959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Chunk 2000, overlap 10%, GPT-4</td>\n",
       "      <td>0.695043</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.872279</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.956041</td>\n",
       "      <td>0.936567</td>\n",
       "      <td>0.892087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.725443</td>\n",
       "      <td>0.479055</td>\n",
       "      <td>0.723903</td>\n",
       "      <td>0.896721</td>\n",
       "      <td>0.579556</td>\n",
       "      <td>0.750275</td>\n",
       "      <td>0.692492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>0.621269</td>\n",
       "      <td>0.609448</td>\n",
       "      <td>0.776955</td>\n",
       "      <td>0.655679</td>\n",
       "      <td>0.806471</td>\n",
       "      <td>0.754321</td>\n",
       "      <td>0.704024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      System  Faithfulness  Answer Relevancy  \\\n",
       "0                                    GPT-3.5      1.000000          0.743175   \n",
       "1                                      GPT-4      0.971429          0.723279   \n",
       "2                                      Naive      0.686914          0.964286   \n",
       "3                                  Recursive      0.669849          0.964286   \n",
       "4                      Chunk 500, overlap 0%      0.953704          0.806519   \n",
       "5                      Chunk 500, overlap 5%      0.944444          0.933492   \n",
       "6                     Chunk 500, overlap 10%      0.705793          0.974965   \n",
       "7                     Chunk 500, overlap 15%      0.752637          0.907425   \n",
       "8                     Chunk 500, overlap 20%      0.984127          0.931073   \n",
       "9                     Chunk 1000, overlap 0%      1.000000          0.811036   \n",
       "10                    Chunk 1000, overlap 5%      0.877500          0.894880   \n",
       "11                   Chunk 1000, overlap 10%      0.875000          0.724113   \n",
       "12                   Chunk 1000, overlap 15%      0.687556          0.851450   \n",
       "13                   Chunk 1000, overlap 20%      0.920635          0.797818   \n",
       "14                    Chunk 2000, overlap 0%      0.679827          0.942857   \n",
       "15                    Chunk 2000, overlap 5%      1.000000          0.880269   \n",
       "16                   Chunk 2000, overlap 10%      0.716439          1.000000   \n",
       "17                   Chunk 2000, overlap 15%      0.701334          1.000000   \n",
       "18                   Chunk 2000, overlap 20%      0.815049          0.826622   \n",
       "19                    Chunk 3000, overlap 0%      0.696330          1.000000   \n",
       "20                    Chunk 3000, overlap 5%      0.697668          0.982143   \n",
       "21                   Chunk 3000, overlap 10%      0.712127          1.000000   \n",
       "22                   Chunk 3000, overlap 15%      0.690314          1.000000   \n",
       "23                   Chunk 3000, overlap 20%      0.671085          1.000000   \n",
       "24         Chunk size 2000, overlap 10%, K=2      0.937241          0.823195   \n",
       "25         Chunk size 2000, overlap 10%, K=3      0.946429          0.915320   \n",
       "26         Chunk size 2000, overlap 10%, K=5      1.000000          0.849876   \n",
       "27         Chunk size 2000, overlap 10%, K=6      0.857163          0.901688   \n",
       "28         Chunk size 2000, overlap 10%, K=7      0.981916          0.833274   \n",
       "29                 Parent Retriever 1000-200      0.723332          0.630200   \n",
       "32                                       MMR      0.715055          0.901328   \n",
       "33                                      BM25      0.634115          0.487036   \n",
       "34                               Ensambler 1      0.875663          0.869943   \n",
       "35                               Ensambler 2      0.790304          0.875000   \n",
       "36                               Ensambler 3      0.701580          0.968679   \n",
       "37                              Search query      0.873333          0.897058   \n",
       "38  Chunk size 2000, overlap 10%, K=3, GPT-4      0.944444          0.898766   \n",
       "39  Chunk size 2000, overlap 10%, K=5, GPT-4      1.000000          0.823842   \n",
       "40            Chunk 2000, overlap 10%, GPT-4      0.695043          0.953704   \n",
       "41                 Parent Retriever 1000-200      0.725443          0.479055   \n",
       "42                  Parent Retriever 500-100      0.621269          0.609448   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.915695            0.487775   \n",
       "1                 NaN             NaN           0.902019            0.484846   \n",
       "2            0.874123        0.905556           0.930638            0.909474   \n",
       "3            0.885355        0.905556           0.930638            0.909474   \n",
       "4            0.805556        0.900000           0.937440            0.683758   \n",
       "5            0.891667        0.900000           0.947441            0.742642   \n",
       "6            0.804580        0.872222           0.875755            0.881225   \n",
       "7            0.939409        0.850000           0.981290            0.825899   \n",
       "8            0.800000        0.875000           0.938601            0.722899   \n",
       "9            0.858333        0.867500           0.927681            0.667863   \n",
       "10           0.855556        0.848889           0.948182            0.701805   \n",
       "11           0.836111        0.863333           0.927738            0.671876   \n",
       "12           0.853393        0.825000           0.949690            0.865611   \n",
       "13           0.822222        0.900000           0.936076            0.679255   \n",
       "14           0.885720        0.922222           0.870638            0.918538   \n",
       "15           0.897222        0.835000           0.937845            0.726514   \n",
       "16           0.877715        0.938889           0.930638            0.918592   \n",
       "17           0.891013        0.880556           0.890638            0.918007   \n",
       "18           0.906254        0.928893           0.852860            0.914072   \n",
       "19           0.796392        0.922222           0.855638            0.916199   \n",
       "20           0.798822        0.822222           0.830638            0.912070   \n",
       "21           0.869710        0.947222           0.930638            0.918381   \n",
       "22           0.804194        0.905556           0.830638            0.912973   \n",
       "23           0.794390        0.797222           0.850638            0.913066   \n",
       "24           0.973148        0.920000           0.935414            0.706006   \n",
       "25           0.966667        1.000000           0.943323            0.739982   \n",
       "26           0.902917        1.000000           0.945310            0.770581   \n",
       "27           0.923695        0.910616           0.878918            0.808390   \n",
       "28           0.852652        0.950000           0.905625            0.680799   \n",
       "29           0.074074        0.400000           0.727664            0.592638   \n",
       "32           0.892569        0.867301           0.936088            0.781449   \n",
       "33           0.329077        0.194862           0.468355            0.725303   \n",
       "34           0.383121        0.846667           0.945115            0.735196   \n",
       "35           0.700311        0.686667           0.861840            0.916791   \n",
       "36           0.778720        0.966667           0.890661            0.892838   \n",
       "37           0.866667        0.900000           0.948863            0.718350   \n",
       "38           0.983333        1.000000           0.946864            0.736261   \n",
       "39           0.886250        1.000000           0.949949            0.721710   \n",
       "40           0.872279        0.938889           0.956041            0.936567   \n",
       "41           0.723903        0.896721           0.579556            0.750275   \n",
       "42           0.776955        0.655679           0.806471            0.754321   \n",
       "\n",
       "     Average  \n",
       "0   0.701108  \n",
       "1   0.690262  \n",
       "2   0.878499  \n",
       "3   0.877526  \n",
       "4   0.847829  \n",
       "5   0.893281  \n",
       "6   0.852423  \n",
       "7   0.876110  \n",
       "8   0.875283  \n",
       "9   0.855402  \n",
       "10  0.854469  \n",
       "11  0.816362  \n",
       "12  0.838783  \n",
       "13  0.842668  \n",
       "14  0.869967  \n",
       "15  0.879475  \n",
       "16  0.897046  \n",
       "17  0.880258  \n",
       "18  0.873959  \n",
       "19  0.864464  \n",
       "20  0.840594  \n",
       "21  0.896346  \n",
       "22  0.857279  \n",
       "23  0.837733  \n",
       "24  0.882501  \n",
       "25  0.918620  \n",
       "26  0.911447  \n",
       "27  0.880078  \n",
       "28  0.867378  \n",
       "29  0.524651  \n",
       "32  0.848965  \n",
       "33  0.473125  \n",
       "34  0.775951  \n",
       "35  0.805152  \n",
       "36  0.866524  \n",
       "37  0.867379  \n",
       "38  0.918278  \n",
       "39  0.896959  \n",
       "40  0.892087  \n",
       "41  0.692492  \n",
       "42  0.704024  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_df.drop(30, inplace = True)\n",
    "# results_df.drop(31, inplace = True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=3</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.91532</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943323</td>\n",
       "      <td>0.739982</td>\n",
       "      <td>0.91862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               System  Faithfulness  Answer Relevancy  \\\n",
       "25  Chunk size 2000, overlap 10%, K=3      0.946429           0.91532   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "25           0.966667             1.0           0.943323            0.739982   \n",
       "\n",
       "    Average  \n",
       "25  0.91862  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest = results_df.nlargest(1, \"Average\")\n",
    "highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum marginal relevance retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8378031577041688\n",
      "This is the new best value!\n",
      "Marginal relevance\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0    MMR           1.0          0.723678           0.916667             0.8   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0            0.92837            0.658104  0.837803  \n"
     ]
    }
   ],
   "source": [
    "retriever_mmr = db_k.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 3})\n",
    "result_mmr, results_df = run_and_evaluate(f\"MMR\", retriever_mmr, prompt, llm, results_df)\n",
    "print(\"Marginal relevance\")\n",
    "print(result_mmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.743175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915695</td>\n",
       "      <td>0.487775</td>\n",
       "      <td>0.701108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.723279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.902019</td>\n",
       "      <td>0.484846</td>\n",
       "      <td>0.690262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.686914</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.874123</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>0.878499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.669849</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.885355</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>0.877526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.806519</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.937440</td>\n",
       "      <td>0.683758</td>\n",
       "      <td>0.847829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.933492</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.947441</td>\n",
       "      <td>0.742642</td>\n",
       "      <td>0.893281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>0.705793</td>\n",
       "      <td>0.974965</td>\n",
       "      <td>0.804580</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.875755</td>\n",
       "      <td>0.881225</td>\n",
       "      <td>0.852423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.752637</td>\n",
       "      <td>0.907425</td>\n",
       "      <td>0.939409</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.981290</td>\n",
       "      <td>0.825899</td>\n",
       "      <td>0.876110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.931073</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.938601</td>\n",
       "      <td>0.722899</td>\n",
       "      <td>0.875283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811036</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.927681</td>\n",
       "      <td>0.667863</td>\n",
       "      <td>0.855402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>0.894880</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.848889</td>\n",
       "      <td>0.948182</td>\n",
       "      <td>0.701805</td>\n",
       "      <td>0.854469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.724113</td>\n",
       "      <td>0.836111</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.927738</td>\n",
       "      <td>0.671876</td>\n",
       "      <td>0.816362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>0.687556</td>\n",
       "      <td>0.851450</td>\n",
       "      <td>0.853393</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.949690</td>\n",
       "      <td>0.865611</td>\n",
       "      <td>0.838783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.797818</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.936076</td>\n",
       "      <td>0.679255</td>\n",
       "      <td>0.842668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.679827</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.885720</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.870638</td>\n",
       "      <td>0.918538</td>\n",
       "      <td>0.869967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880269</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.937845</td>\n",
       "      <td>0.726514</td>\n",
       "      <td>0.879475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.716439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877715</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.918592</td>\n",
       "      <td>0.897046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.701334</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891013</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.890638</td>\n",
       "      <td>0.918007</td>\n",
       "      <td>0.880258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.815049</td>\n",
       "      <td>0.826622</td>\n",
       "      <td>0.906254</td>\n",
       "      <td>0.928893</td>\n",
       "      <td>0.852860</td>\n",
       "      <td>0.914072</td>\n",
       "      <td>0.873959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.696330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796392</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.855638</td>\n",
       "      <td>0.916199</td>\n",
       "      <td>0.864464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.697668</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.798822</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.830638</td>\n",
       "      <td>0.912070</td>\n",
       "      <td>0.840594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.712127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869710</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.918381</td>\n",
       "      <td>0.896346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.690314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.804194</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.830638</td>\n",
       "      <td>0.912973</td>\n",
       "      <td>0.857279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.671085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794390</td>\n",
       "      <td>0.797222</td>\n",
       "      <td>0.850638</td>\n",
       "      <td>0.913066</td>\n",
       "      <td>0.837733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=2</td>\n",
       "      <td>0.937241</td>\n",
       "      <td>0.823195</td>\n",
       "      <td>0.973148</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.935414</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0.882501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=3</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.915320</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943323</td>\n",
       "      <td>0.739982</td>\n",
       "      <td>0.918620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.849876</td>\n",
       "      <td>0.902917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945310</td>\n",
       "      <td>0.770581</td>\n",
       "      <td>0.911447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=6</td>\n",
       "      <td>0.857163</td>\n",
       "      <td>0.901688</td>\n",
       "      <td>0.923695</td>\n",
       "      <td>0.910616</td>\n",
       "      <td>0.878918</td>\n",
       "      <td>0.808390</td>\n",
       "      <td>0.880078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=7</td>\n",
       "      <td>0.981916</td>\n",
       "      <td>0.833274</td>\n",
       "      <td>0.852652</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.905625</td>\n",
       "      <td>0.680799</td>\n",
       "      <td>0.867378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.723332</td>\n",
       "      <td>0.630200</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.727664</td>\n",
       "      <td>0.592638</td>\n",
       "      <td>0.524651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.634115</td>\n",
       "      <td>0.487036</td>\n",
       "      <td>0.329077</td>\n",
       "      <td>0.194862</td>\n",
       "      <td>0.468355</td>\n",
       "      <td>0.725303</td>\n",
       "      <td>0.473125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Ensambler 1</td>\n",
       "      <td>0.875663</td>\n",
       "      <td>0.869943</td>\n",
       "      <td>0.383121</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.945115</td>\n",
       "      <td>0.735196</td>\n",
       "      <td>0.775951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ensambler 2</td>\n",
       "      <td>0.790304</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.700311</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.861840</td>\n",
       "      <td>0.916791</td>\n",
       "      <td>0.805152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ensambler 3</td>\n",
       "      <td>0.701580</td>\n",
       "      <td>0.968679</td>\n",
       "      <td>0.778720</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.890661</td>\n",
       "      <td>0.892838</td>\n",
       "      <td>0.866524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Search query</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.897058</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.948863</td>\n",
       "      <td>0.718350</td>\n",
       "      <td>0.867379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=3, GPT-4</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.898766</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946864</td>\n",
       "      <td>0.736261</td>\n",
       "      <td>0.918278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=5, GPT-4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823842</td>\n",
       "      <td>0.886250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949949</td>\n",
       "      <td>0.721710</td>\n",
       "      <td>0.896959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Chunk 2000, overlap 10%, GPT-4</td>\n",
       "      <td>0.695043</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.872279</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.956041</td>\n",
       "      <td>0.936567</td>\n",
       "      <td>0.892087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.725443</td>\n",
       "      <td>0.479055</td>\n",
       "      <td>0.723903</td>\n",
       "      <td>0.896721</td>\n",
       "      <td>0.579556</td>\n",
       "      <td>0.750275</td>\n",
       "      <td>0.692492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>0.621269</td>\n",
       "      <td>0.609448</td>\n",
       "      <td>0.776955</td>\n",
       "      <td>0.655679</td>\n",
       "      <td>0.806471</td>\n",
       "      <td>0.754321</td>\n",
       "      <td>0.704024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>MMR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.723678</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.928370</td>\n",
       "      <td>0.658104</td>\n",
       "      <td>0.837803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      System  Faithfulness  Answer Relevancy  \\\n",
       "0                                    GPT-3.5      1.000000          0.743175   \n",
       "1                                      GPT-4      0.971429          0.723279   \n",
       "2                                      Naive      0.686914          0.964286   \n",
       "3                                  Recursive      0.669849          0.964286   \n",
       "4                      Chunk 500, overlap 0%      0.953704          0.806519   \n",
       "5                      Chunk 500, overlap 5%      0.944444          0.933492   \n",
       "6                     Chunk 500, overlap 10%      0.705793          0.974965   \n",
       "7                     Chunk 500, overlap 15%      0.752637          0.907425   \n",
       "8                     Chunk 500, overlap 20%      0.984127          0.931073   \n",
       "9                     Chunk 1000, overlap 0%      1.000000          0.811036   \n",
       "10                    Chunk 1000, overlap 5%      0.877500          0.894880   \n",
       "11                   Chunk 1000, overlap 10%      0.875000          0.724113   \n",
       "12                   Chunk 1000, overlap 15%      0.687556          0.851450   \n",
       "13                   Chunk 1000, overlap 20%      0.920635          0.797818   \n",
       "14                    Chunk 2000, overlap 0%      0.679827          0.942857   \n",
       "15                    Chunk 2000, overlap 5%      1.000000          0.880269   \n",
       "16                   Chunk 2000, overlap 10%      0.716439          1.000000   \n",
       "17                   Chunk 2000, overlap 15%      0.701334          1.000000   \n",
       "18                   Chunk 2000, overlap 20%      0.815049          0.826622   \n",
       "19                    Chunk 3000, overlap 0%      0.696330          1.000000   \n",
       "20                    Chunk 3000, overlap 5%      0.697668          0.982143   \n",
       "21                   Chunk 3000, overlap 10%      0.712127          1.000000   \n",
       "22                   Chunk 3000, overlap 15%      0.690314          1.000000   \n",
       "23                   Chunk 3000, overlap 20%      0.671085          1.000000   \n",
       "24         Chunk size 2000, overlap 10%, K=2      0.937241          0.823195   \n",
       "25         Chunk size 2000, overlap 10%, K=3      0.946429          0.915320   \n",
       "26         Chunk size 2000, overlap 10%, K=5      1.000000          0.849876   \n",
       "27         Chunk size 2000, overlap 10%, K=6      0.857163          0.901688   \n",
       "28         Chunk size 2000, overlap 10%, K=7      0.981916          0.833274   \n",
       "29                 Parent Retriever 1000-200      0.723332          0.630200   \n",
       "31                                      BM25      0.634115          0.487036   \n",
       "32                               Ensambler 1      0.875663          0.869943   \n",
       "33                               Ensambler 2      0.790304          0.875000   \n",
       "34                               Ensambler 3      0.701580          0.968679   \n",
       "35                              Search query      0.873333          0.897058   \n",
       "36  Chunk size 2000, overlap 10%, K=3, GPT-4      0.944444          0.898766   \n",
       "37  Chunk size 2000, overlap 10%, K=5, GPT-4      1.000000          0.823842   \n",
       "38            Chunk 2000, overlap 10%, GPT-4      0.695043          0.953704   \n",
       "39                 Parent Retriever 1000-200      0.725443          0.479055   \n",
       "40                  Parent Retriever 500-100      0.621269          0.609448   \n",
       "42                                       MMR      1.000000          0.723678   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.915695            0.487775   \n",
       "1                 NaN             NaN           0.902019            0.484846   \n",
       "2            0.874123        0.905556           0.930638            0.909474   \n",
       "3            0.885355        0.905556           0.930638            0.909474   \n",
       "4            0.805556        0.900000           0.937440            0.683758   \n",
       "5            0.891667        0.900000           0.947441            0.742642   \n",
       "6            0.804580        0.872222           0.875755            0.881225   \n",
       "7            0.939409        0.850000           0.981290            0.825899   \n",
       "8            0.800000        0.875000           0.938601            0.722899   \n",
       "9            0.858333        0.867500           0.927681            0.667863   \n",
       "10           0.855556        0.848889           0.948182            0.701805   \n",
       "11           0.836111        0.863333           0.927738            0.671876   \n",
       "12           0.853393        0.825000           0.949690            0.865611   \n",
       "13           0.822222        0.900000           0.936076            0.679255   \n",
       "14           0.885720        0.922222           0.870638            0.918538   \n",
       "15           0.897222        0.835000           0.937845            0.726514   \n",
       "16           0.877715        0.938889           0.930638            0.918592   \n",
       "17           0.891013        0.880556           0.890638            0.918007   \n",
       "18           0.906254        0.928893           0.852860            0.914072   \n",
       "19           0.796392        0.922222           0.855638            0.916199   \n",
       "20           0.798822        0.822222           0.830638            0.912070   \n",
       "21           0.869710        0.947222           0.930638            0.918381   \n",
       "22           0.804194        0.905556           0.830638            0.912973   \n",
       "23           0.794390        0.797222           0.850638            0.913066   \n",
       "24           0.973148        0.920000           0.935414            0.706006   \n",
       "25           0.966667        1.000000           0.943323            0.739982   \n",
       "26           0.902917        1.000000           0.945310            0.770581   \n",
       "27           0.923695        0.910616           0.878918            0.808390   \n",
       "28           0.852652        0.950000           0.905625            0.680799   \n",
       "29           0.074074        0.400000           0.727664            0.592638   \n",
       "31           0.329077        0.194862           0.468355            0.725303   \n",
       "32           0.383121        0.846667           0.945115            0.735196   \n",
       "33           0.700311        0.686667           0.861840            0.916791   \n",
       "34           0.778720        0.966667           0.890661            0.892838   \n",
       "35           0.866667        0.900000           0.948863            0.718350   \n",
       "36           0.983333        1.000000           0.946864            0.736261   \n",
       "37           0.886250        1.000000           0.949949            0.721710   \n",
       "38           0.872279        0.938889           0.956041            0.936567   \n",
       "39           0.723903        0.896721           0.579556            0.750275   \n",
       "40           0.776955        0.655679           0.806471            0.754321   \n",
       "42           0.916667        0.800000           0.928370            0.658104   \n",
       "\n",
       "     Average  \n",
       "0   0.701108  \n",
       "1   0.690262  \n",
       "2   0.878499  \n",
       "3   0.877526  \n",
       "4   0.847829  \n",
       "5   0.893281  \n",
       "6   0.852423  \n",
       "7   0.876110  \n",
       "8   0.875283  \n",
       "9   0.855402  \n",
       "10  0.854469  \n",
       "11  0.816362  \n",
       "12  0.838783  \n",
       "13  0.842668  \n",
       "14  0.869967  \n",
       "15  0.879475  \n",
       "16  0.897046  \n",
       "17  0.880258  \n",
       "18  0.873959  \n",
       "19  0.864464  \n",
       "20  0.840594  \n",
       "21  0.896346  \n",
       "22  0.857279  \n",
       "23  0.837733  \n",
       "24  0.882501  \n",
       "25  0.918620  \n",
       "26  0.911447  \n",
       "27  0.880078  \n",
       "28  0.867378  \n",
       "29  0.524651  \n",
       "31  0.473125  \n",
       "32  0.775951  \n",
       "33  0.805152  \n",
       "34  0.866524  \n",
       "35  0.867379  \n",
       "36  0.918278  \n",
       "37  0.896959  \n",
       "38  0.892087  \n",
       "39  0.692492  \n",
       "40  0.704024  \n",
       "42  0.837803  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 2000, chunk_overlap = 200)\n",
    "chunks_2000 = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8703536299948803\n",
      "This is the new best value!\n",
      "BM25\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0   BM25          0.98          0.899716           0.708333             1.0   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.946193             0.68788  0.870354  \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever\n",
    "retriever_bm25 = BM25Retriever.from_documents(chunks_2000, search_kwargs={\"k\": 3})\n",
    "result_bm25, results_df = run_and_evaluate(f\"BM25\", retriever_bm25, prompt, llm, results_df)\n",
    "print(\"BM25\")\n",
    "print(result_bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensambler - Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = db_k.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   2%|▏         | 1/60 [00:02<01:59,  2.02s/it]Runner in Executor raised an exception\n",
      "ValueError: Azure has not provided the response due to a content filter being triggered\n",
      "Evaluating:  67%|██████▋   | 40/60 [00:05<00:01, 10.77it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating:  93%|█████████▎| 56/60 [00:07<00:00,  6.80it/s]Invalid JSON response. Expected dictionary with key 'Attributed'\n",
      "Evaluating: 100%|██████████| 60/60 [00:10<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8648139282427217\n",
      "Ensambler\n",
      "        System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Ensambler 1      0.691669          0.979557           0.798251   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.822619           0.991231            0.905557  0.864814  \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "ensemble_retriever_1 = EnsembleRetriever(retrievers=[retriever_bm25, ret], weights=[0.75, 0.25])\n",
    "result_ensemble1, results_df = run_and_evaluate(f\"Ensambler 1\", ensemble_retriever_1, prompt, llm, results_df)\n",
    "print(\"Ensambler\")\n",
    "print(result_ensemble1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  97%|█████████▋| 58/60 [00:08<00:00,  9.34it/s]Invalid JSON response. Expected dictionary with key 'Attributed'\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8733804689425805\n",
      "This is the new best value!\n",
      "Ensambler\n",
      "        System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Ensambler 2      0.987654          0.820264           0.800516   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness  Average  \n",
      "0             1.0           0.926481            0.705368  0.87338  \n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_2 = EnsembleRetriever(retrievers=[retriever_bm25, ret], weights=[0.5, 0.5])\n",
    "result_ensemble2, results_df = run_and_evaluate(f\"Ensambler 2\", ensemble_retriever_2, prompt, llm, results_df)\n",
    "print(\"Ensambler\")\n",
    "print(result_ensemble2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  95%|█████████▌| 57/60 [00:08<00:00,  7.77it/s]Invalid JSON response. Expected dictionary with key 'Attributed'\n",
      "Evaluating: 100%|██████████| 60/60 [00:24<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8600716511840957\n",
      "Ensambler\n",
      "        System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Ensambler 3           0.9          0.831635           0.843738   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             1.0           0.930265            0.654792  0.860072  \n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_3 = EnsembleRetriever(retrievers=[retriever_bm25, ret], weights=[0.25,0.75])\n",
    "result_ensemble3, results_df = run_and_evaluate(f\"Ensambler 3\", ensemble_retriever_3, prompt, llm, results_df)\n",
    "print(\"Ensambler\")\n",
    "print(result_ensemble3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-stage - reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_context = retriever_mmr\n",
    "compressor = CohereRerank(top_n=6)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever_context\n",
    ")\n",
    "\n",
    "result_compression = run_and_evaluate(compression_retriever, prompt, llm)\n",
    "print(\"Reranker\")\n",
    "print(result_compression)\n",
    "avg_result_compression = dictionary(result_compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating context by remaking the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=3</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.91532</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943323</td>\n",
       "      <td>0.739982</td>\n",
       "      <td>0.91862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               System  Faithfulness  Answer Relevancy  \\\n",
       "25  Chunk size 2000, overlap 10%, K=3      0.946429           0.91532   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "25           0.966667             1.0           0.943323            0.739982   \n",
       "\n",
       "    Average  \n",
       "25  0.91862  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest = results_df.nlargest(1, \"Average\")\n",
    "highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_context = \"Generate a search query to fetch the relevant documents using the user's {question}. Craft a query that specifically targets the keywords in the question. In the answer provide only the query.\"\n",
    "prompt_context = ChatPromptTemplate.from_template(template_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  18%|█▊        | 11/60 [00:02<00:07,  6.88it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.7441, 'answer_relevancy': 0.9269, 'context_precision': 0.8953, 'context_recall': 0.9333, 'answer_similarity': 0.8644, 'answer_correctness': 0.8067}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_final = []\n",
    "contexts_final = []\n",
    "# retriever_context_q = EnsembleRetriever(retrievers=[retriever_bm25, retriever_3], weights=[0.5, 0.5])\n",
    "llm_for_context =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt_context | llm}\n",
    ")\n",
    "for query in questions:\n",
    "    response_check = llm_for_context.invoke({\"question\": query})\n",
    "    search_query = response_check[\"response\"].content\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"context\"), \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "    docs = ret.get_relevant_documents(search_query)\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        resulting_doc = doc.page_content\n",
    "        formatted_docs.append(resulting_doc)\n",
    "    try:  \n",
    "            response = retrieval_augmented_qa_chain.invoke({\"context\": formatted_docs, \"question\": query})\n",
    "            # Access the response content\n",
    "            answers_final.append(response[\"response\"].content)\n",
    "            contexts_final.append(formatted_docs)  \n",
    "    except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers_final.append(\"No answer\")\n",
    "            contexts_final.append(formatted_docs)\n",
    "\n",
    "\n",
    "result_search_query = evaluation_rag(questions, answers_final, contexts_final, ground_truths)\n",
    "result_search_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8617799007305408\n"
     ]
    }
   ],
   "source": [
    "average = dictionary(result_search_query)\n",
    "    # Create a dictionary to store the results\n",
    "system_results = {\n",
    "        \"System\": \"Search query\",\n",
    "        \"Faithfulness\": result_search_query[\"faithfulness\"],\n",
    "        \"Answer Relevancy\": result_search_query[\"answer_relevancy\"],\n",
    "        \"Context Precision\": result_search_query[\"context_precision\"],\n",
    "        \"Context Recall\": result_search_query[\"context_recall\"],\n",
    "        \"Answer Similarity\": result_search_query[\"answer_similarity\"],\n",
    "        \"Answer Correctness\": result_search_query[\"answer_correctness\"],\n",
    "        \"Average\": average\n",
    "    }\n",
    "df_result_search_query = pd.DataFrame([system_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.743175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915695</td>\n",
       "      <td>0.487775</td>\n",
       "      <td>0.701108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.723279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.902019</td>\n",
       "      <td>0.484846</td>\n",
       "      <td>0.690262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.686914</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.874123</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>0.878499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.669849</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.885355</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.909474</td>\n",
       "      <td>0.877526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.806519</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.937440</td>\n",
       "      <td>0.683758</td>\n",
       "      <td>0.847829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.933492</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.947441</td>\n",
       "      <td>0.742642</td>\n",
       "      <td>0.893281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>0.705793</td>\n",
       "      <td>0.974965</td>\n",
       "      <td>0.804580</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.875755</td>\n",
       "      <td>0.881225</td>\n",
       "      <td>0.852423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.752637</td>\n",
       "      <td>0.907425</td>\n",
       "      <td>0.939409</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.981290</td>\n",
       "      <td>0.825899</td>\n",
       "      <td>0.876110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.931073</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.938601</td>\n",
       "      <td>0.722899</td>\n",
       "      <td>0.875283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811036</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.867500</td>\n",
       "      <td>0.927681</td>\n",
       "      <td>0.667863</td>\n",
       "      <td>0.855402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>0.894880</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.848889</td>\n",
       "      <td>0.948182</td>\n",
       "      <td>0.701805</td>\n",
       "      <td>0.854469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.724113</td>\n",
       "      <td>0.836111</td>\n",
       "      <td>0.863333</td>\n",
       "      <td>0.927738</td>\n",
       "      <td>0.671876</td>\n",
       "      <td>0.816362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>0.687556</td>\n",
       "      <td>0.851450</td>\n",
       "      <td>0.853393</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.949690</td>\n",
       "      <td>0.865611</td>\n",
       "      <td>0.838783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.797818</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.936076</td>\n",
       "      <td>0.679255</td>\n",
       "      <td>0.842668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.679827</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.885720</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.870638</td>\n",
       "      <td>0.918538</td>\n",
       "      <td>0.869967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880269</td>\n",
       "      <td>0.897222</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.937845</td>\n",
       "      <td>0.726514</td>\n",
       "      <td>0.879475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.716439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877715</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.918592</td>\n",
       "      <td>0.897046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.701334</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891013</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.890638</td>\n",
       "      <td>0.918007</td>\n",
       "      <td>0.880258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.815049</td>\n",
       "      <td>0.826622</td>\n",
       "      <td>0.906254</td>\n",
       "      <td>0.928893</td>\n",
       "      <td>0.852860</td>\n",
       "      <td>0.914072</td>\n",
       "      <td>0.873959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.696330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.796392</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.855638</td>\n",
       "      <td>0.916199</td>\n",
       "      <td>0.864464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.697668</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.798822</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.830638</td>\n",
       "      <td>0.912070</td>\n",
       "      <td>0.840594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.712127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869710</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.930638</td>\n",
       "      <td>0.918381</td>\n",
       "      <td>0.896346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.690314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.804194</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.830638</td>\n",
       "      <td>0.912973</td>\n",
       "      <td>0.857279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.671085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794390</td>\n",
       "      <td>0.797222</td>\n",
       "      <td>0.850638</td>\n",
       "      <td>0.913066</td>\n",
       "      <td>0.837733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=2</td>\n",
       "      <td>0.937241</td>\n",
       "      <td>0.823195</td>\n",
       "      <td>0.973148</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.935414</td>\n",
       "      <td>0.706006</td>\n",
       "      <td>0.882501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=3</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.915320</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943323</td>\n",
       "      <td>0.739982</td>\n",
       "      <td>0.918620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.849876</td>\n",
       "      <td>0.902917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945310</td>\n",
       "      <td>0.770581</td>\n",
       "      <td>0.911447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=6</td>\n",
       "      <td>0.857163</td>\n",
       "      <td>0.901688</td>\n",
       "      <td>0.923695</td>\n",
       "      <td>0.910616</td>\n",
       "      <td>0.878918</td>\n",
       "      <td>0.808390</td>\n",
       "      <td>0.880078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=7</td>\n",
       "      <td>0.981916</td>\n",
       "      <td>0.833274</td>\n",
       "      <td>0.852652</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.905625</td>\n",
       "      <td>0.680799</td>\n",
       "      <td>0.867378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.723332</td>\n",
       "      <td>0.630200</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.727664</td>\n",
       "      <td>0.592638</td>\n",
       "      <td>0.524651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=3, GPT-4</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.898766</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946864</td>\n",
       "      <td>0.736261</td>\n",
       "      <td>0.918278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=5, GPT-4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823842</td>\n",
       "      <td>0.886250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949949</td>\n",
       "      <td>0.721710</td>\n",
       "      <td>0.896959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Chunk 2000, overlap 10%, GPT-4</td>\n",
       "      <td>0.695043</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.872279</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.956041</td>\n",
       "      <td>0.936567</td>\n",
       "      <td>0.892087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.725443</td>\n",
       "      <td>0.479055</td>\n",
       "      <td>0.723903</td>\n",
       "      <td>0.896721</td>\n",
       "      <td>0.579556</td>\n",
       "      <td>0.750275</td>\n",
       "      <td>0.692492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>0.621269</td>\n",
       "      <td>0.609448</td>\n",
       "      <td>0.776955</td>\n",
       "      <td>0.655679</td>\n",
       "      <td>0.806471</td>\n",
       "      <td>0.754321</td>\n",
       "      <td>0.704024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MMR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.723678</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.928370</td>\n",
       "      <td>0.658104</td>\n",
       "      <td>0.837803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.899716</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.946193</td>\n",
       "      <td>0.687880</td>\n",
       "      <td>0.870354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Ensambler 1</td>\n",
       "      <td>0.691669</td>\n",
       "      <td>0.979557</td>\n",
       "      <td>0.798251</td>\n",
       "      <td>0.822619</td>\n",
       "      <td>0.991231</td>\n",
       "      <td>0.905557</td>\n",
       "      <td>0.864814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Ensambler 2</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.820264</td>\n",
       "      <td>0.800516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926481</td>\n",
       "      <td>0.705368</td>\n",
       "      <td>0.873380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ensambler 3</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.831635</td>\n",
       "      <td>0.843738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930265</td>\n",
       "      <td>0.654792</td>\n",
       "      <td>0.860072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.693712</td>\n",
       "      <td>0.904090</td>\n",
       "      <td>0.764877</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.827303</td>\n",
       "      <td>0.840885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.802671</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.934762</td>\n",
       "      <td>0.704652</td>\n",
       "      <td>0.867508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Search query</td>\n",
       "      <td>0.744064</td>\n",
       "      <td>0.926909</td>\n",
       "      <td>0.895312</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.864351</td>\n",
       "      <td>0.806710</td>\n",
       "      <td>0.861780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      System  Faithfulness  Answer Relevancy  \\\n",
       "0                                    GPT-3.5      1.000000          0.743175   \n",
       "1                                      GPT-4      0.971429          0.723279   \n",
       "2                                      Naive      0.686914          0.964286   \n",
       "3                                  Recursive      0.669849          0.964286   \n",
       "4                      Chunk 500, overlap 0%      0.953704          0.806519   \n",
       "5                      Chunk 500, overlap 5%      0.944444          0.933492   \n",
       "6                     Chunk 500, overlap 10%      0.705793          0.974965   \n",
       "7                     Chunk 500, overlap 15%      0.752637          0.907425   \n",
       "8                     Chunk 500, overlap 20%      0.984127          0.931073   \n",
       "9                     Chunk 1000, overlap 0%      1.000000          0.811036   \n",
       "10                    Chunk 1000, overlap 5%      0.877500          0.894880   \n",
       "11                   Chunk 1000, overlap 10%      0.875000          0.724113   \n",
       "12                   Chunk 1000, overlap 15%      0.687556          0.851450   \n",
       "13                   Chunk 1000, overlap 20%      0.920635          0.797818   \n",
       "14                    Chunk 2000, overlap 0%      0.679827          0.942857   \n",
       "15                    Chunk 2000, overlap 5%      1.000000          0.880269   \n",
       "16                   Chunk 2000, overlap 10%      0.716439          1.000000   \n",
       "17                   Chunk 2000, overlap 15%      0.701334          1.000000   \n",
       "18                   Chunk 2000, overlap 20%      0.815049          0.826622   \n",
       "19                    Chunk 3000, overlap 0%      0.696330          1.000000   \n",
       "20                    Chunk 3000, overlap 5%      0.697668          0.982143   \n",
       "21                   Chunk 3000, overlap 10%      0.712127          1.000000   \n",
       "22                   Chunk 3000, overlap 15%      0.690314          1.000000   \n",
       "23                   Chunk 3000, overlap 20%      0.671085          1.000000   \n",
       "24         Chunk size 2000, overlap 10%, K=2      0.937241          0.823195   \n",
       "25         Chunk size 2000, overlap 10%, K=3      0.946429          0.915320   \n",
       "26         Chunk size 2000, overlap 10%, K=5      1.000000          0.849876   \n",
       "27         Chunk size 2000, overlap 10%, K=6      0.857163          0.901688   \n",
       "28         Chunk size 2000, overlap 10%, K=7      0.981916          0.833274   \n",
       "29                 Parent Retriever 1000-200      0.723332          0.630200   \n",
       "30  Chunk size 2000, overlap 10%, K=3, GPT-4      0.944444          0.898766   \n",
       "31  Chunk size 2000, overlap 10%, K=5, GPT-4      1.000000          0.823842   \n",
       "32            Chunk 2000, overlap 10%, GPT-4      0.695043          0.953704   \n",
       "33                 Parent Retriever 1000-200      0.725443          0.479055   \n",
       "34                  Parent Retriever 500-100      0.621269          0.609448   \n",
       "35                                       MMR      1.000000          0.723678   \n",
       "36                                      BM25      0.980000          0.899716   \n",
       "37                               Ensambler 1      0.691669          0.979557   \n",
       "38                               Ensambler 2      0.987654          0.820264   \n",
       "39                               Ensambler 3      0.900000          0.831635   \n",
       "40                 Parent Retriever 1000-200      0.693712          0.904090   \n",
       "41                  Parent Retriever 500-100      0.962963          0.802671   \n",
       "42                              Search query      0.744064          0.926909   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.915695            0.487775   \n",
       "1                 NaN             NaN           0.902019            0.484846   \n",
       "2            0.874123        0.905556           0.930638            0.909474   \n",
       "3            0.885355        0.905556           0.930638            0.909474   \n",
       "4            0.805556        0.900000           0.937440            0.683758   \n",
       "5            0.891667        0.900000           0.947441            0.742642   \n",
       "6            0.804580        0.872222           0.875755            0.881225   \n",
       "7            0.939409        0.850000           0.981290            0.825899   \n",
       "8            0.800000        0.875000           0.938601            0.722899   \n",
       "9            0.858333        0.867500           0.927681            0.667863   \n",
       "10           0.855556        0.848889           0.948182            0.701805   \n",
       "11           0.836111        0.863333           0.927738            0.671876   \n",
       "12           0.853393        0.825000           0.949690            0.865611   \n",
       "13           0.822222        0.900000           0.936076            0.679255   \n",
       "14           0.885720        0.922222           0.870638            0.918538   \n",
       "15           0.897222        0.835000           0.937845            0.726514   \n",
       "16           0.877715        0.938889           0.930638            0.918592   \n",
       "17           0.891013        0.880556           0.890638            0.918007   \n",
       "18           0.906254        0.928893           0.852860            0.914072   \n",
       "19           0.796392        0.922222           0.855638            0.916199   \n",
       "20           0.798822        0.822222           0.830638            0.912070   \n",
       "21           0.869710        0.947222           0.930638            0.918381   \n",
       "22           0.804194        0.905556           0.830638            0.912973   \n",
       "23           0.794390        0.797222           0.850638            0.913066   \n",
       "24           0.973148        0.920000           0.935414            0.706006   \n",
       "25           0.966667        1.000000           0.943323            0.739982   \n",
       "26           0.902917        1.000000           0.945310            0.770581   \n",
       "27           0.923695        0.910616           0.878918            0.808390   \n",
       "28           0.852652        0.950000           0.905625            0.680799   \n",
       "29           0.074074        0.400000           0.727664            0.592638   \n",
       "30           0.983333        1.000000           0.946864            0.736261   \n",
       "31           0.886250        1.000000           0.949949            0.721710   \n",
       "32           0.872279        0.938889           0.956041            0.936567   \n",
       "33           0.723903        0.896721           0.579556            0.750275   \n",
       "34           0.776955        0.655679           0.806471            0.754321   \n",
       "35           0.916667        0.800000           0.928370            0.658104   \n",
       "36           0.708333        1.000000           0.946193            0.687880   \n",
       "37           0.798251        0.822619           0.991231            0.905557   \n",
       "38           0.800516        1.000000           0.926481            0.705368   \n",
       "39           0.843738        1.000000           0.930265            0.654792   \n",
       "40           0.764877        0.900000           0.955329            0.827303   \n",
       "41           0.900000        0.900000           0.934762            0.704652   \n",
       "42           0.895312        0.933333           0.864351            0.806710   \n",
       "\n",
       "     Average  \n",
       "0   0.701108  \n",
       "1   0.690262  \n",
       "2   0.878499  \n",
       "3   0.877526  \n",
       "4   0.847829  \n",
       "5   0.893281  \n",
       "6   0.852423  \n",
       "7   0.876110  \n",
       "8   0.875283  \n",
       "9   0.855402  \n",
       "10  0.854469  \n",
       "11  0.816362  \n",
       "12  0.838783  \n",
       "13  0.842668  \n",
       "14  0.869967  \n",
       "15  0.879475  \n",
       "16  0.897046  \n",
       "17  0.880258  \n",
       "18  0.873959  \n",
       "19  0.864464  \n",
       "20  0.840594  \n",
       "21  0.896346  \n",
       "22  0.857279  \n",
       "23  0.837733  \n",
       "24  0.882501  \n",
       "25  0.918620  \n",
       "26  0.911447  \n",
       "27  0.880078  \n",
       "28  0.867378  \n",
       "29  0.524651  \n",
       "30  0.918278  \n",
       "31  0.896959  \n",
       "32  0.892087  \n",
       "33  0.692492  \n",
       "34  0.704024  \n",
       "35  0.837803  \n",
       "36  0.870354  \n",
       "37  0.864814  \n",
       "38  0.873380  \n",
       "39  0.860072  \n",
       "40  0.840885  \n",
       "41  0.867508  \n",
       "42  0.861780  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.concat([results_df, df_result_search_query], ignore_index=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change model to GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.9182783048436894\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chunk 1000, overlap 0%, GPT-4</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.898766</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946864</td>\n",
       "      <td>0.736261</td>\n",
       "      <td>0.918278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          System  Faithfulness  Answer Relevancy  \\\n",
       "0  Chunk 1000, overlap 0%, GPT-4      0.944444          0.898766   \n",
       "\n",
       "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0           0.983333             1.0           0.946864            0.736261   \n",
       "\n",
       "    Average  \n",
       "0  0.918278  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_k3_2000_10_gpt4, results_df = run_and_evaluate(f\"Chunk size 2000, overlap 10%, K=3, GPT-4\", ret, prompt, llm_gpt4, results_df)\n",
    "result_k3_2000_10_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    'Chunk 1000, overlap 0%, GPT-4': 'Chunk size 2000, overlap 10%, K=3, GPT-4'\n",
    "}\n",
    "results_df['System'] = results_df['System'].replace(replacement_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_5 = db_k.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:11<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8969585598993436\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chunk size 2000, overlap 10%, K=5, GPT-4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.823842</td>\n",
       "      <td>0.88625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949949</td>\n",
       "      <td>0.72171</td>\n",
       "      <td>0.896959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     System  Faithfulness  Answer Relevancy  \\\n",
       "0  Chunk size 2000, overlap 10%, K=5, GPT-4           1.0          0.823842   \n",
       "\n",
       "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0            0.88625             1.0           0.949949             0.72171   \n",
       "\n",
       "    Average  \n",
       "0  0.896959  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_k5_2000_10_gpt4, results_df = run_and_evaluate(f\"Chunk size 2000, overlap 10%, K=5, GPT-4\", ret_5, prompt, llm_gpt4, results_df)\n",
    "result_k5_2000_10_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_4 = db_k.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  18%|█▊        | 11/60 [00:05<00:23,  2.05it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:12<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8920871684905182\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chunk 2000, overlap 10%, GPT-4</td>\n",
       "      <td>0.695043</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.872279</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.956041</td>\n",
       "      <td>0.936567</td>\n",
       "      <td>0.892087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           System  Faithfulness  Answer Relevancy  \\\n",
       "0  Chunk 2000, overlap 10%, GPT-4      0.695043          0.953704   \n",
       "\n",
       "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0           0.872279        0.938889           0.956041            0.936567   \n",
       "\n",
       "    Average  \n",
       "0  0.892087  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2000_10_gpt4, results_df = run_and_evaluate(f\"Chunk 2000, overlap 10%, GPT-4\", ret_4, prompt, llm_gpt4, results_df)\n",
    "result_2000_10_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"../papers/results/results_summarize.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
