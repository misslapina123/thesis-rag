{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "dotenv.load_dotenv()\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "import getpass\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\")\n",
    "OPENAI_DEPLOYMENT = os.environ.get(\"OPENAI_DEPLOYMENT\")\n",
    "EMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "EMBEDDING_DEPLOYMENT = os.environ.get(\"EMBEDDING_DEPLOYMENT\")\n",
    "OPENAI_MODEL_GPT4 = os.environ.get(\"OPENAI_MODEL_GPT4\")\n",
    "OPENAI_DEPLOYMENT_GPT4 = os.environ.get(\"OPENAI_DEPLOYMENT_GPT4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for upload and 'D' stands for delete.\",\n",
    "    \"The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 4.89 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.\",\n",
    "    \"Model Layer: This particular layer is instrumental in powering AI products delivered via either proprietary APIs or open-source checkpoints (which necessitate hosting solutions). Within the Foundation Models category, there exist both proprietary models from private sources (such as chatGPT) and open-source models (like Stable Diffusion). Additionally, there are model hubs dedicated to the sharing and hosting of Foundation Models.\",\n",
    "    \"After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 8B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.\",\n",
    "    \"Pessimistic Rejection serves as an evaluation metric to assess the ability of Language Models (LLMs) to refuse answering a question in instances where none of the provided contexts offer pertinent information. In practical scenarios, search engines frequently encounter difficulties in retrieving documents that contain the required answers. In such situations, it becomes crucial for the model to possess the capability to decline recognition, thereby preventing the generation of inaccurate or misleading content.\",\n",
    "    \"This metacognition space primarily encompasses three main phases: (1) Deduction; (2) Evaluating; (3) Planning.\",\n",
    "    \"Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-4 model (Radford et al. 2019) for this task.\",\n",
    "    \"The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a client-side memory-augmented model (client), a retriever model (on-prem), and a memory generator based on Large Language Models (LLM) (cloud).\",\n",
    "    \"A general query is characterized by the necessity to retrieve and reason over multiple pieces of supporting evidence in order to furnish an answer. To put it differently, for a given multi-hop query denoted as q, the components within the retrieval set Rq collaboratively supply an answer to q.\",\n",
    "    \"Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, “Fermat” is the correct word, and REALITY (row (c)) gives the word a much high probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve document to fill in the masked word even though it is trained with unsupervised text only.\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    [\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\"],\n",
    "    [\"The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.\"],\n",
    "    [\"Model Layer: This particular layer is instrumental in powering AI products delivered via either proprietary APIs or open-source checkpoints (which necessitate hosting solutions). Within the Foundation Models category, there exist both proprietary models from private sources (such as GPT-4) and open-source models (like Stable Diffusion). Additionally, there are model hubs dedicated to the sharing and hosting of Foundation Models.\"],\n",
    "    [\"After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.\"],\n",
    "    [\"Negative Rejection serves as an evaluation metric to assess the ability of Language Models (LLMs) to refuse answering a question in instances where none of the provided contexts offer pertinent information. In practical scenarios, search engines frequently encounter difficulties in retrieving documents that contain the required answers. In such situations, it becomes crucial for the model to possess the capability to decline recognition, thereby preventing the generation of inaccurate or misleading content.\"],\n",
    "    [\"This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.\"],\n",
    "    [\"Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.\"],\n",
    "    [\"The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a client-side memory-augmented model (client), a retriever model (cloud), and a memory generator based on Large Language Models (LLM) (cloud).\"],\n",
    "    [\"A multi-hop query is characterized by the necessity to retrieve and reason over multiple pieces of supporting evidence in order to furnish an answer. To put it differently, for a given multi-hop query denoted as q, the components within the retrieval set Rq collaboratively supply an answer to q.\"],\n",
    "    [\"Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, “Fermat” is the correct word, and REALM (row (c)) gives the word a much high probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve document to fill in the masked word even though it is trained with unsupervised text only.\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_client = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDING_DEPLOYMENT,\n",
    "    openai_api_version=OPENAI_API_VERSION)\n",
    "llm = AzureChatOpenAI(model_name=OPENAI_MODEL, azure_deployment=OPENAI_DEPLOYMENT,temperature=0)\n",
    "llm_gpt4 = AzureChatOpenAI(model_name=OPENAI_MODEL_GPT4, azure_deployment=OPENAI_DEPLOYMENT_GPT4,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_rag(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  \n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "        raise_exceptions=False,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(lst):\n",
    "    return [item.replace('\"', '').replace(\"'\", '').replace('“','').replace('”', '').replace('\\n','') for sublist in lst for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "def calculate_accuracy(answers, ground_truths):\n",
    "    # Convert both answers and ground truths to lowercase for case-insensitive comparison\n",
    "    answers_lower = flatten_list([answer.lower() for answer in answers])\n",
    "    ground_truths_lower = flatten_list([[truth.lower() for truth in sublist] for sublist in ground_truths])\n",
    "    # Check if each answer corresponds with the ground truth exactly\n",
    "    accuracy = [1 if ans == truth else 0 for ans, truth in zip(answers_lower, ground_truths_lower)]\n",
    "    # Calculate the average accuracy\n",
    "    average_accuracy = sum(accuracy) / len(accuracy)\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General answers by LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" Fix an error in the following query. The answer should be exactly the same as query but with corrected mistake. For example:\n",
    "'input': 'There are 10 continents in the world.',\n",
    "'output': 'There are 7 continents in the world.',\n",
    "'input': 'The elephant is the largest animal of all time.',\n",
    "'output': 'The blue whale is the largest animal of all time.'.\n",
    "This is the input query: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm}\n",
    ")\n",
    "llm_chain_gpt4 =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm_gpt4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_llm = []\n",
    "contexts_llm = [[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in questions:\n",
    "    response = llm_chain.invoke({\"question\": query})\n",
    "    answers_llm.append(response[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is no error in the query.',\n",
       " 'There is no error in the given query.',\n",
       " 'There are no errors in the query.',\n",
       " 'There is no error in the query.',\n",
       " 'There is no error in the given query.',\n",
       " 'There is no error in the query.',\n",
       " 'There is no error in the query.',\n",
       " 'There is no error in the query.',\n",
       " 'There is no error in the query.',\n",
       " 'There is no error in the given query.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "llm_results = calculate_accuracy(answers_llm, ground_truths)\n",
    "print(llm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "answers_llm_gpt4 = []\n",
    "for query in questions:\n",
    "    response = llm_chain_gpt4.invoke({\"question\": query})\n",
    "    answers_llm_gpt4.append(response[\"response\"].content)\n",
    "llm_results_gpt4 = calculate_accuracy(answers_llm_gpt4, ground_truths)\n",
    "print(llm_results_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\",\n",
       " 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 4.89 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.',\n",
       " 'This particular layer is instrumental in powering AI products delivered via either proprietary APIs or open-source checkpoints (which necessitate hosting solutions). Within the Foundation Models category, there exist both proprietary models from private sources (such as GPT-3) and open-source models (like BERT). Additionally, there are model hubs dedicated to the sharing and hosting of Foundation Models.',\n",
       " 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-L) and 8B parameter models (i.e., FiD-XL) that are fine-tuned on the training split of target datasets.',\n",
       " 'The input query is correct. There is no error to fix.',\n",
       " 'This metacognition process primarily encompasses three main phases: (1) Planning; (2) Monitoring; (3) Evaluating.',\n",
       " 'Output: Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-3 model (Radford et al. 2019) for this task.',\n",
       " 'The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a client-side memory-augmented model (client), a retriever model (cloud), and a memory generator based on Large Language Models (LLM) (cloud).',\n",
       " 'A multi-hop query is characterized by the necessity to retrieve and reason over multiple pieces of supporting evidence in order to furnish an answer. To put it differently, for a given multi-hop query denoted as q, the components within the retrieval set Rq collaboratively supply an answer to q.',\n",
       " 'Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, “Fermat” is the correct word, and REALM (row (c)) gives the word a much high probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve document to fill in the masked word even though it is trained with unsupervised text only.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_llm_gpt4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_chain(prompt, retriever, llm):\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    return retrieval_augmented_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../papers', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "db_naive = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"../papers/vectordb/naive\")\n",
    "db_naive.persist()\n",
    "retriever_naive = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_naive = Chroma(persist_directory = \"../papers/vectordb/naive\", embedding_function=embeddings_client)\n",
    "retriever_naive = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Fix an error in the following query. The answer should be exactly the same as query but with corrected mistake. For example:\n",
    "'input': 'There are 10 continents in the world.',\n",
    "'output': 'There are 7 continents in the world.',\n",
    "'input': 'The elephant is the largest animal of all time.',\n",
    "'output': 'The blue whale is the largest animal of all time.'.\n",
    "This is the input query: {question}. \n",
    "Here is some provided context {context}.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_naive = []\n",
    "contexts_naive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_naive, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_naive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_naive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_naive.append(\"No answer\")\n",
    "        context_full = retriever_naive.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_naive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is no error in the given query.',\n",
       " 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth. \\n\\nThe corrected query is: The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 4.89 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.',\n",
       " 'There is no error in the given query.',\n",
       " 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 8B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets. \\n\\nCorrection: After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.',\n",
       " 'There is no error in the given query.',\n",
       " 'This metacognition space primarily encompasses three main phases: (1) Deduction; (2) Evaluating; (3) Planning. \\n\\nThere is no error in the query.',\n",
       " 'There is no error in the given query.',\n",
       " 'The input query is already correct. No error needs to be fixed.',\n",
       " 'There is no error in the given query.',\n",
       " 'There is no error in the given query.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:03<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.8333, 'context_recall': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "result_naive_rag_context = evaluation_rag(questions, answers_naive, contexts_naive, ground_truths)\n",
    "print(result_naive_rag_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "result_naive_rag = calculate_accuracy(answers_naive, ground_truths)\n",
    "print(result_naive_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recursive splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder()\n",
    "chunks_r = text_splitter.split_documents(documents)\n",
    "db_basic = Chroma.from_documents(chunks_r, embeddings_client, persist_directory = \"../papers/vectordb/recursive_basic\")\n",
    "db_basic.persist()\n",
    "retriever_basic = db_basic.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_basic = Chroma(persist_directory = \"../papers/vectordb/recursive_basic\", embedding_function=embeddings_client)\n",
    "retriever_basic = db_basic.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is no error in the given query.',\n",
       " 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth. \\n\\nThe corrected query is: The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 4.89 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.',\n",
       " 'There is no error in the given query.',\n",
       " 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 8B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets. \\n\\nCorrection: After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.',\n",
       " 'There is no error in the given query.',\n",
       " 'This metacognition space primarily encompasses three main phases: (1) Deduction; (2) Evaluating; (3) Planning. \\n\\nThere is no error in the query.',\n",
       " 'There is no error in the given query.',\n",
       " 'The input query is already correct and does not contain any errors.',\n",
       " 'There is no error in the given query.',\n",
       " 'No error found in the given query.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_recursive = []\n",
    "contexts_recursive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_basic, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_recursive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_recursive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_recursive.append(\"No answer\")\n",
    "        context_full = retriever_basic.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_recursive.append(context_content)\n",
    "answers_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:03<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.8000, 'context_recall': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "result_recursive_context = evaluation_rag(questions, answers_recursive, contexts_recursive, ground_truths)\n",
    "print(result_recursive_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "result_recursive = calculate_accuracy(answers_recursive, ground_truths)\n",
    "print(result_recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_evaluate(retriever, prompt, llm):\n",
    "    answers_recursive = []\n",
    "    contexts_recursive = []\n",
    "\n",
    "    for query in questions:\n",
    "        try:  \n",
    "            response = retrieval_chain(prompt, retriever, llm).invoke({\"question\": query})\n",
    "            # Access the response content\n",
    "            answers_recursive.append(response[\"response\"].content)\n",
    "            # Access the context content\n",
    "            context_content = [context.page_content for context in response[\"context\"]]\n",
    "            contexts_recursive.append(context_content)  \n",
    "        except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers_recursive.append(\"No answer\")\n",
    "            context_full = retriever.get_relevant_documents(query)\n",
    "            context_content = [context.page_content for context in context_full]\n",
    "            contexts_recursive.append(context_content)\n",
    "\n",
    "\n",
    "    result = evaluation_rag(questions, answers_recursive, contexts_recursive, ground_truths)\n",
    "    accuracy = calculate_accuracy(answers_recursive, ground_truths)\n",
    "    print(answers_recursive)\n",
    "    return result, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 100)\n",
    "chunks_1000 = text_splitter.split_documents(documents)\n",
    "print(len(chunks_1000))\n",
    "db_1000 = Chroma.from_documents(chunks_1000, embeddings_client, persist_directory = \"../papers/vectordb/recursive_1000\")\n",
    "db_1000.persist()\n",
    "retriever_1000 = db_1000.as_retriever()\n",
    "result_1000_context = run_and_evaluate(retriever_1000, prompt, llm)\n",
    "print(\"CHUNK SIZE 1000\")\n",
    "print(result_1000_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_1000 = Chroma(persist_directory = \"../papers/vectordb/recursive_1000\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:03<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'There is no error in the given query.', 'The input query is already correct. No error needs to be fixed.', 'No error found in the query.', 'No error found.']\n",
      "CHUNK SIZE 1000\n",
      "{'context_precision': 0.8139, 'context_recall': 0.9500}\n",
      "AVG accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "retriever_1000 = db_1000.as_retriever()\n",
    "result_1000_context, result_1000 = run_and_evaluate(retriever_1000, prompt, llm)\n",
    "print(\"CHUNK SIZE 1000\")\n",
    "print(result_1000_context)\n",
    "print(\"AVG accuracy \"+ str(result_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500, chunk_overlap = 50)\n",
    "chunks_500 = text_splitter.split_documents(documents)\n",
    "print(len(chunks_500))\n",
    "db_500 = Chroma.from_documents(chunks_500, embeddings_client, persist_directory = \"../vectordb/recursive_500\")\n",
    "db_500.persist()\n",
    "retriever_500 = db_500.as_retriever()\n",
    "result_500 = run_and_evaluate(retriever_500, prompt, llm)\n",
    "print(\"CHUNK SIZE 500\")\n",
    "print(result_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_500 = Chroma(persist_directory = \"../papers/vectordb/recursive_500\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:03<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The error in the query is that it states \"an average of 4.89 / 5 G-EVAL score\" instead of \"an average of 2.41 / 5 G-EVAL score\". The corrected query is:\\n\\nThe results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'No error found.']\n",
      "CHUNK SIZE 500\n",
      "{'context_precision': 0.8417, 'context_recall': 1.0000}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "retriever_500 = db_500.as_retriever()\n",
    "result_500, acc_500 = run_and_evaluate(retriever_500, prompt, llm)\n",
    "print(\"CHUNK SIZE 500\")\n",
    "print(result_500)\n",
    "print(\"AVG accuracy \"+ str(acc_500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_2000 = Chroma(persist_directory = \"../papers/vectordb/recursive_2000\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:04<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the query.', 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth. \\n\\nThe corrected query is: The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 4.89 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'There is no error in the given query.', 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 8B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets. \\n\\nCorrection: After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.', \"I'm sorry, but there is no error in the given input query. It is a complete and grammatically correct sentence.\", 'This metacognition space primarily encompasses three main phases: (1) Deduction; (2) Evaluating; (3) Planning.', 'There is no error in the given query.', 'The input query is already correct. No error needs to be fixed.', 'There is no error in the given query.', 'No error found in the given query.']\n",
      "CHUNK SIZE 2000\n",
      "{'context_precision': 0.8222, 'context_recall': 0.9000}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "retriever_2000 = db_2000.as_retriever()\n",
    "result_2000, acc_2000 = run_and_evaluate(retriever_2000, prompt, llm)\n",
    "print(\"CHUNK SIZE 2000\")\n",
    "print(result_2000)\n",
    "print(\"AVG accuracy \"+ str(acc_2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 400, chunk_overlap = 100)\n",
    "chunks_400 = text_splitter.split_documents(documents)\n",
    "print(len(chunks_400))\n",
    "db_400 = Chroma.from_documents(chunks_400, embeddings_client, persist_directory = \"../papers/vectordb/recursive_400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:03<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\", 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'No error found in the query.', 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The query is correct and does not contain any errors.', 'There is no error in the given query.', 'No error found.']\n",
      "CHUNK SIZE 400\n",
      "{'context_precision': 0.9250, 'context_recall': 0.8833}\n",
      "AVG accuracy 0.3\n"
     ]
    }
   ],
   "source": [
    "retriever_400 = db_400.as_retriever()\n",
    "result_400, acc_400 = run_and_evaluate(retriever_400, prompt, llm)\n",
    "print(\"CHUNK SIZE 400\")\n",
    "print(result_400)\n",
    "print(\"AVG accuracy \"+ str(acc_400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now time to look for different top-k\n",
    "\n",
    "Note: We continue with the size chunk of 2000 as it had the highest average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:03<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'There is no error in the query.', 'No error found in the query.', 'There is no error in the query.', 'There is no error in the given query.', 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', \"Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task. We fine-tune a pre-trained BART model (Lewis et al. 2020a) which is, unlike GPT-2, an encoder-decoder model. In contrast to Kim et al. who use Nucleus' model that uses separate encoders, one for the dialogue context and one for knowledge documents. Relevant documents are retrieved by named entity and keyword matching. Kim, Ahn, and Kim (2020) jointly model knowledge selection and response generation using a sequential knowledge transformer. Zhao et al. (2020) suggest an unsupervised approach for retrieval which does not require annotations of relevant knowledge documents. Unlike in the current track of DSTC 9, in these benchmarks, typically, multiple knowledge documents are relevant to create a response. Additionally, conversations in open domain dialogs often lack a clear goal and the state of the dialog is less constrained by the domain.\", 'The query is correct and does not contain any errors.', 'There is no error in the given query.', 'No error found.']\n",
      "CHUNK SIZE 1000, K=2\n",
      "{'context_precision': 0.9500, 'context_recall': 0.9500}\n",
      "AVG accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "retriever_2 = db_400.as_retriever(search_kwargs={\"k\": 2})\n",
    "result_2, acc_2 = run_and_evaluate(retriever_2, prompt, llm)\n",
    "print(\"CHUNK SIZE 1000, K=2\")\n",
    "print(result_2)\n",
    "print(\"AVG accuracy \"+ str(acc_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:04<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The error in the query is that it states \"an average of 4.89 / 5 G-EVAL score\" instead of \"an average of 2.41 / 5 G-EVAL score\". The corrected query is:\\n\\nThe results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The query is already correct. No error found.', 'There is no error in the given query.', 'No error found in the query.']\n",
      "CHUNK SIZE 500, K=3\n",
      "{'context_precision': 0.9833, 'context_recall': 0.9083}\n",
      "AVG accuracy 0.2\n"
     ]
    }
   ],
   "source": [
    "retriever_3 = db_400.as_retriever(search_kwargs={\"k\": 3})\n",
    "result_3, acc_3 = run_and_evaluate(retriever_3, prompt, llm)\n",
    "print(\"CHUNK SIZE 500, K=3\")\n",
    "print(result_3)\n",
    "print(\"AVG accuracy \"+ str(acc_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:04<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'There is no error in the given query.', 'No error found in the query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The query is already correct. No error found.', 'There is no error in the given query.', 'No error found in the query.']\n",
      "CHUNK SIZE 500, K=5\n",
      "{'context_precision': 0.9221, 'context_recall': 0.8500}\n",
      "AVG accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "retriever_5 = db_400.as_retriever(search_kwargs={\"k\": 5})\n",
    "result_5, acc_5 = run_and_evaluate(retriever_5, prompt, llm)\n",
    "print(\"CHUNK SIZE 500, K=5\")\n",
    "print(result_5)\n",
    "print(\"AVG accuracy \"+ str(acc_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:04<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'There is no error in the given query.', 'No error found in the query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'The input query is already correct. No error found.', 'There is no error in the given query.', 'There is no error in the given query.']\n",
      "CHUNK SIZE 500, K=6\n",
      "{'context_precision': 0.8960, 'context_recall': 0.8550}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "retriever_6 = db_400.as_retriever(search_kwargs={\"k\": 6})\n",
    "result_6, acc_6 = run_and_evaluate(retriever_6, prompt, llm)\n",
    "print(\"CHUNK SIZE 500, K=6\")\n",
    "print(result_6)\n",
    "print(\"AVG accuracy \"+ str(acc_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:04<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'There is no error in the given query.', 'No error found.', 'There is no error in the given query.', 'There is no error in the given query.', 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'There is no error in the given query.', 'The query is already correct. No error found.', 'No error found in the query.', 'No error found in the query.']\n",
      "CHUNK SIZE 500, K=7\n",
      "{'context_precision': 0.8854, 'context_recall': 1.0000}\n",
      "AVG accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "retriever_7 = db_400.as_retriever(search_kwargs={\"k\": 7})\n",
    "result_7, acc_7 = run_and_evaluate(retriever_7, prompt, llm)\n",
    "print(\"CHUNK SIZE 500, K=7\")\n",
    "print(result_7)\n",
    "print(\"AVG accuracy \"+ str(acc_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look for different retrievers\n",
    "\n",
    "3 chunks was the best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parent document retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:02<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'There is no error in the given query.', 'There is no error in the given query.', 'No error found.']\n",
      "{'context_precision': 0.9000, 'context_recall': 0.9000}\n",
      "AVG accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "parent_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=2000)\n",
    "child_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap = 0)\n",
    "\n",
    "# vectorstore = Chroma(collection_name=\"split_parents\", persist_directory = \"../papers/vectordb/parent_summary\", embedding_function=embeddings_client)\n",
    "# vectorstore.persist()\n",
    "vectorstore = Chroma(persist_directory = \"../papers/vectordb/parent_summary\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "parent_document_retriever.add_documents(documents)\n",
    "result_parent, acc_parent = run_and_evaluate(parent_document_retriever, prompt, llm)\n",
    "print(result_parent)\n",
    "print(\"AVG accuracy \"+ str(acc_parent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:04<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No error found in the query.', 'The query is already correct. No error found.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'The query is correct. No error found.', 'There is no error in the given query.', 'No error found in the given query.']\n",
      "{'context_precision': 0.9250, 'context_recall': 0.9500}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "parent_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap = 50)\n",
    "child_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap = 0)\n",
    "\n",
    "# vectorstore = Chroma(collection_name=\"split_parents_small\",persist_directory = \"../papers/vectordb/parent_small_summary\", embedding_function=embeddings_client)\n",
    "# vectorstore.persist()\n",
    "vectorstore = Chroma(persist_directory = \"../papers/vectordb/parent_small_summary\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_small = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_small,\n",
    "    parent_splitter=parent_splitter_small\n",
    ")\n",
    "parent_document_retriever_small.add_documents(documents)\n",
    "result_parent_small, acc_parent_small = run_and_evaluate(parent_document_retriever_small, prompt, llm)\n",
    "print(result_parent_small)\n",
    "print(\"AVG accuracy \"+ str(acc_parent_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:03<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\", 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'The input query is already correct. No error needs to be fixed.', 'There is no error in the given query.', 'No error found.']\n",
      "{'context_precision': 0.9167, 'context_recall': 0.8000}\n",
      "AVG accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "parent_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=3000)\n",
    "child_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap = 0)\n",
    "\n",
    "# vectorstore = Chroma(collection_name=\"split_parents_large\", persist_directory = \"../papers/vectordb/parent_large_summary\", embedding_function=embeddings_client)\n",
    "# vectorstore.persist()\n",
    "vectorstore = Chroma(persist_directory = \"../papers/vectordb/parent_large_summary\", embedding_function=embeddings_client)\n",
    "\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_large = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_large,\n",
    "    parent_splitter=parent_splitter_large\n",
    ")\n",
    "parent_document_retriever_large.add_documents(documents)\n",
    "result_parent_large, acc_parent_large = run_and_evaluate(parent_document_retriever_large, prompt, llm)\n",
    "print(result_parent_large)\n",
    "print(\"AVG accuracy \"+ str(acc_parent_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum marginal relevance retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:03<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The error in the query is that it states \"an average of 4.89 / 5 G-EVAL score\" instead of \"an average of 2.41 / 5 G-EVAL score\". The corrected query is:\\n\\nThe results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'There is no error in the given query.', 'The query is correct and does not contain any errors.', 'There is no error in the given query.', 'No error found in the query.']\n",
      "Marginal relevance\n",
      "{'context_precision': 0.9083, 'context_recall': 0.8750}\n",
      "AVG accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "retriever_mmr = db_400.as_retriever(search_type=\"mmr\")\n",
    "result_mmr, acc_mmr = run_and_evaluate(retriever_mmr, prompt, llm)\n",
    "print(\"Marginal relevance\")\n",
    "print(result_mmr)\n",
    "print(\"AVG accuracy \"+ str(acc_mmr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 400, chunk_overlap = 50)\n",
    "chunks_400 = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:02<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The error in the query is that it states \"an average of 4.89 / 5 G-EVAL score\", whereas the correct value is \"an average of 2.41 / 5 G-EVAL score\". \\n\\nThe corrected query is: The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'No error found.', 'The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a client-side memory-augmented model (client), a retriever model (cloud), and a memory generator based on Large Language Models (LLM) (cloud).', 'There is no error in the given query.', 'No error found.']\n",
      "BM25\n",
      "{'context_precision': 0.8722, 'context_recall': 1.0000}\n",
      "AVG accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "retriever_bm25 = BM25Retriever.from_documents(chunks_400)\n",
    "result_bm25, acc_bm25 = run_and_evaluate(retriever_bm25, prompt, llm)\n",
    "print(\"BM25\")\n",
    "print(result_bm25)\n",
    "print(\"AVG accuracy \"+ str(acc_bm25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensambler - Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:05<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The query is already correct. No error found.', 'The query is already correct. No error found.', 'There is no error in the given query.', 'There is no error in the given query.', 'No error found.', 'There is no error in the given query.', 'The input query is already correct. No error found.', 'There is no error in the given query.', 'No error found.']\n",
      "Ensambler 75/25\n",
      "{'context_precision': 0.7935, 'context_recall': 0.9500}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_1 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_400], weights=[0.75, 0.25])\n",
    "result_ensemble1, acc_retriever_1 = run_and_evaluate(ensemble_retriever_1, prompt, llm)\n",
    "print(\"Ensambler 75/25\")\n",
    "print(result_ensemble1)\n",
    "print(\"AVG accuracy \"+ str(acc_retriever_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:05<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The query is already correct. No error found.', 'No error found in the query.', 'The query is already correct. No error found.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'No error found.']\n",
      "Ensambler 50/50\n",
      "{'context_precision': 0.9252, 'context_recall': 0.9500}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_2 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_400], weights=[0.5, 0.5])\n",
    "result_ensemble2, acc_ens_2 = run_and_evaluate(ensemble_retriever_2, prompt, llm)\n",
    "print(\"Ensambler 50/50\")\n",
    "print(result_ensemble2)\n",
    "print(\"AVG accuracy \"+ str(acc_ens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:06<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The query is already correct. No error found.', 'No error found.', 'The query is already correct. No error found.', 'No error found in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'No error found.']\n",
      "Ensambler 25/75\n",
      "{'context_precision': 0.9224, 'context_recall': 0.9500}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_3 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_400], weights=[0.25,0.75])\n",
    "result_ensemble3, acc_ens_3 = run_and_evaluate(ensemble_retriever_3, prompt, llm)\n",
    "print(\"Ensambler 25/75\")\n",
    "print(result_ensemble3)\n",
    "print(\"AVG accuracy \"+ str(acc_ens_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:05<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The query is already correct. No error found.', 'The query is already correct. No error found.', 'No error found in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'No error found.']\n",
      "Ensambler 75/25\n",
      "{'context_precision': 0.7725, 'context_recall': 0.8750}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_4 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_mmr], weights=[0.75, 0.25])\n",
    "result_ensemble4, acc_retriever_4 = run_and_evaluate(ensemble_retriever_4, prompt, llm)\n",
    "print(\"Ensambler 75/25\")\n",
    "print(result_ensemble4)\n",
    "print(\"AVG accuracy \"+ str(acc_retriever_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:05<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The query is already correct. No error found.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'There is no error in the given query.', 'The query is already correct.', 'There is no error in the given query.', 'No error found.']\n",
      "Ensambler 50/50\n",
      "{'context_precision': 0.9178, 'context_recall': 0.9000}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_5 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_mmr], weights=[0.5, 0.5])\n",
    "result_ensemble5, acc_retriever_5 = run_and_evaluate(ensemble_retriever_5, prompt, llm)\n",
    "print(\"Ensambler 50/50\")\n",
    "print(result_ensemble5)\n",
    "print(\"AVG accuracy \"+ str(acc_retriever_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:06<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The query is already correct. No error found.', 'No error found.', 'The query does not have any errors.', 'No error found.', 'There is no error in the given query.', 'There is no error in the given query.', 'The query is already correct. No error found.', 'There is no error in the given query.', 'No error found.']\n",
      "Ensambler 25/75\n",
      "{'context_precision': 0.8446, 'context_recall': 1.0000}\n",
      "AVG accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_6 = EnsembleRetriever(retrievers=[retriever_bm25, retriever_mmr], weights=[0.25, 0.75])\n",
    "result_ensemble6, acc_retriever_6 = run_and_evaluate(ensemble_retriever_6, prompt, llm)\n",
    "print(\"Ensambler 25/75\")\n",
    "print(result_ensemble6)\n",
    "print(\"AVG accuracy \"+ str(acc_retriever_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-stage - reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:02<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There is no error in the given query.', 'The query is already correct.', 'There is no error in the given query.', 'There is no error in the given query.', 'No error found.', 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The query is already correct. No error found.', 'There is no error in the given query.', 'No error found.']\n",
      "Reranker\n",
      "{'context_precision': 0.9833, 'context_recall': 0.9083}\n",
      "AVG accuracy 0.2\n"
     ]
    }
   ],
   "source": [
    "retriever_context = retriever_400\n",
    "compressor = CohereRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever_context\n",
    ")\n",
    "\n",
    "result_compression, acc_compression = run_and_evaluate(compression_retriever, prompt, llm)\n",
    "print(\"Reranker\")\n",
    "print(result_compression)\n",
    "print(\"AVG accuracy \"+ str(acc_compression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating context by remaking the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_context = \"Generate a search query to fetch the relevant documents using the user's {question}. Craft a query that specifically targets the keywords in the question. In the answer provide only the query.\"\n",
    "prompt_context = ChatPromptTemplate.from_template(template_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:04<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG accuracy 0.1\n"
     ]
    }
   ],
   "source": [
    "answers_final = []\n",
    "contexts_final = []\n",
    "llm_for_context =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt_context | llm}\n",
    ")\n",
    "for query in questions:\n",
    "    response_check = llm_for_context.invoke({\"question\": query})\n",
    "    search_query = response_check[\"response\"].content\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"context\"), \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "    docs = retriever_400.get_relevant_documents(search_query)\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        resulting_doc = doc.page_content\n",
    "        formatted_docs.append(resulting_doc)\n",
    "    try:  \n",
    "            response = retrieval_augmented_qa_chain.invoke({\"context\": formatted_docs, \"question\": query})\n",
    "            # Access the response content\n",
    "            answers_final.append(response[\"response\"].content)\n",
    "            contexts_final.append(formatted_docs)  \n",
    "    except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers_final.append(\"No answer\")\n",
    "            contexts_final.append(formatted_docs)\n",
    "\n",
    "\n",
    "result_search_query = evaluation_rag(questions, answers_final, contexts_final, ground_truths)\n",
    "acc_search_query = calculate_accuracy(answers_final, ground_truths)\n",
    "print(\"AVG accuracy \"+ str(acc_search_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change model to GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:02<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\", 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'Model Layer: This particular layer is instrumental in powering AI products delivered via either proprietary APIs or open-source checkpoints (which necessitate hosting solutions). Within the Foundation Models category, there exist both proprietary models from private sources (such as GPT-4) and open-source models (like Stable Diffusion). Additionally, there are model hubs dedicated to the sharing and hosting of Foundation Models.', 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.', \"Pessimistic Rejection serves as an evaluation metric to assess the ability of Large Language Models (LLMs) to refuse answering a question in instances where none of the provided contexts offer pertinent information. In practical scenarios, search engines frequently encounter difficulties in retrieving documents that contain the required answers. In such situations, it becomes crucial for the model to possess the capability to decline recognition, thereby preventing the generation of inaccurate or misleading content.. \\nHere is some provided context [Document(page_content='the question. This can cause confusion for LLMs and make\\\\nLLMs generate incorrect answers. In Table 2, the model an-\\\\nswer focuses on the concept “automotive revenue” in the\\\\ndocument rather than “revenue” in the question.\\\\nBased on the analysis above, we have identified certain\\\\nlimitations in LLMs regarding retrieval-augmented genera-\\\\ntion. To effectively handle the vast amount of noise present\\\\non the internet, further detailed enhancements are required\\\\nfor the model such as long documents modeling and precise\\\\nconcept comprehension.\\\\nResults on Negative Rejection testbed\\\\nWe evaluated the rejection rate when only noise documents\\\\nwere provided. The results are shown in Table 3. In addi-\\\\ntion to evaluating the rejection rate through exact matching\\\\n(Rej in Table 3), we also utilize ChatGPT to determine if\\\\nthe responses from the LLMs contain any rejection informa-\\\\ntion (Rej∗in Table 3). We can see that: Negative Rejection\\\\nposes a challenge for RAG in LLMs. The highest rejection\\\\nrates for LLMs in English and Chinese were only 45% and\\\\n43.33%, respectively. This suggests that LLMs can be easily\\\\nmisled by noisy documents, leading to incorrect answers.\\\\nIn addition, through comparing Rej and Rej∗, we found\\\\nthat LLMs fail to strictly follow instructions, and they often\\\\ngenerate unpredictable responses, which make it hard to use\\\\nthem as state triggers (such as for recognizing rejection).\\\\nWe conduct case studies in Table 4. The first error is\\\\nbecause of Evidence uncertainty . Although the document\\\\nonly mentions contact with “Adam McKay” and does not\\\\nexplicitly state that he is the director of the movie, the\\\\nmodel still concludes that he holds this role. The first er-\\\\nror is because of Concept confusion . The information pro-', metadata={'page': 5, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'}), Document(page_content='iology and Medicine?”, the model will generate the known\\\\nquestion “Who was awarded the 2021 Nobel Prize in Lit-\\\\nerature?” and answer “ Abdulrazak Gurnah ”. We then man-\\\\nually verified the generated answers, and retrieve relevant\\\\ndocuments as described above. In order to make documents\\\\ncontain factual errors, we manually modify the answers and\\\\nreplace the corresponding parts in the document.\\\\nFinally, we collect totally 600 base questions in RGB,\\\\nand 200 additional questions for the information integration\\\\nability and 200 additional questions for counterfactual ro-\\\\nbustness ability. Half of the instances are in English, and the\\\\nother half are in Chinese.\\\\nEvaluation metrics\\\\nThe core of this benchmark is to evaluate whether LLMs can\\\\nutilize the provided external documents to acquire knowl-\\\\nedge and generate reasonable answers. We evaluate the re-\\\\nsponses of LLMs in order to measure above-mentioned four\\\\nabilities of them.\\\\nAccuracy is used to measure noise robustness and infor-\\\\nmation integration. We employ an exact matching approach\\\\nwhere if the generated text contains an exact match to the\\\\nanswer, it is considered as a correct answer.\\\\nRejection rate is used to measure negative rejection.\\\\nWhen only noisy documents are provided, LLMs should\\\\noutput the specific content – “I can not answer the question\\\\nbecause of the insufficient information in documents.” (We\\\\nuse instructions to inform the model.). If the model gener-\\\\nates this content, it indicates a successful rejection.\\\\nError detection rate measures whether the model can\\\\ndetect the factual errors in the documents for counterfactual\\\\nrobustness. When the provided documents contain factual\\\\nerrors, the model should output the specific content – “There\\\\nare factual errors in the provided documents.” (We use in-', metadata={'page': 3, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'}), Document(page_content='useful information entailed in the external documents. Ad-\\\\nditionally, LLMs can easily be misled by incorrect infor-\\\\nmation in the document. To this end, we build Retrieval-\\\\nAugmented Generation Benchmark (RGB) to evaluate the\\\\nretrieval-augmented generation of LLMs, and we concern\\\\nabout 4 specific abilities:\\\\nNoise Robustness is the robustness of LLMs in noisy\\\\ndocuments. As retrievers are not perfect, the external knowl-\\\\nedge they retrieve often contains a significant amount of\\\\nnoise, i.e., documents which are relevant to the question but\\\\ndo not contain any information about the answer. To effec-\\\\ntively answer user questions, LLMs must be able to extract\\\\nthe necessary information from documents despite there are\\\\nnoisy documents.\\\\nNegative Rejection is a measure of whether LLMs can\\\\ndecline to answer a question when none of the contexts pro-\\\\nvide useful information. In real-world situations, the search\\\\nengine often fails to retrieve documents containing the an-\\\\nswers. In these cases, it is important for the model to have\\\\nthe capability to reject recognition and avoid generating mis-\\\\nleading content.\\\\nInformation Integration is a capacity to integrate an-\\\\nswers from multiple documents. In many cases, the an-\\\\nswer to a question may be contained in multiple documents.\\\\nFor example, for the question “Who are the champions of\\\\nthe U.S. Open 2022 men’s and women’s singles?” , the two\\\\nchampions may be mentioned in different documents. In or-\\\\nder to provide better answers to complex questions, it is nec-\\\\nessary for LLMs to have the ability to integrate information.\\\\nCounterfactual Robustness refers to a capacity to han-', metadata={'page': 2, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'}), Document(page_content='jection rate (%) and Rej∗means the rejection rate evaluated\\\\nby ChatGPT. We can see that negative rejection poses a chal-\\\\nlenge for RAG in LLMs.\\\\nQuestion Answer Response\\\\nwho will direct\\\\nIrredeemable film?Jeymes\\\\nSamuelThe information provided in the document\\\\nis that Fox has attached “The Big Short”\\\\nwriter-director Adam McKay to its movie\\\\nadaptation of “Irredeemable” from Boom!\\\\nComics. Therefore, the answer is Adam McKay.\\\\nWhich country won the\\\\nmost medals at the\\\\n2022 Winter Olympics?NorwayBased on the information provided in the\\\\ndocument, the country that won the most medals\\\\nat the 2022 Winter Olympics is Germany.\\\\nGermany has won a total of 31 medals, including 14\\\\ngold medals, 14 bronze medals, and 31 silver medals.\\\\nTable 4: Error cases of negative rejection generated by\\\\nChatGLM2-6B. The red text highlights the error answers.\\\\nand result in incorrect responses. In future developments, it\\\\nwill be crucial for LLMs to enhance their ability to accu-\\\\nrately match questions with the appropriate documents.\\\\nResults on Information Integration testbed\\\\nWe evaluated the accuracy based on the different noise ratios\\\\nin external documents, and the results are shown in Table 5.\\\\nWhen comparing the model to Table 1, we observed that\\\\nit has a weak information integration ability, which in turn\\\\naffects its noise robustness. We can see that:\\\\n(1) Information integration poses a challenge for RAG\\\\nin LLMs. Even without noise, the highest accuracy of LLMs\\\\ncan only reach 60% and 67% for English and Chinese,\\\\nrespectively. After adding noise, the highest accuracy de-\\\\ncreases to 43% and 55%. These results suggest that LLMs', metadata={'page': 5, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'})].\", 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a memory-augmented client model (client), a retriever model (cloud), and a memory generator based on Large Language Models (LLM) (cloud).', 'A general query is characterized by the necessity to retrieve and reason over multiple pieces of supporting evidence in order to furnish an answer. To put it differently, for a given multi-hop query denoted as q, the components within the retrieval set Rq collaboratively supply an answer to q.', 'Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, “Fermat” is the correct word, and REALM (row (c)) gives the word a much higher probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve documents to fill in the masked word even though it is trained with unsupervised text only.']\n",
      "CHUNK SIZE 400\n",
      "{'context_precision': 0.9222, 'context_recall': 0.8833}\n",
      "AVG accuracy 0.6\n"
     ]
    }
   ],
   "source": [
    "result_400_gpt4, acc_400_gpt4 = run_and_evaluate(retriever_400, prompt, llm_gpt4)\n",
    "print(\"CHUNK SIZE 400\")\n",
    "print(result_400_gpt4)\n",
    "print(\"AVG accuracy \"+ str(acc_400_gpt4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:05<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\", 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'Model Layer: This particular layer is instrumental in powering AI products delivered via either proprietary APIs or open-source checkpoints (which necessitate hosting solutions). Within the Foundation Models category, there exist both proprietary models from private sources (such as GPT-4) and open-source models (like Stable Diffusion). Additionally, there are model hubs dedicated to the sharing and hosting of Foundation Models.', 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.', \"Pessimistic Rejection serves as an evaluation metric to assess the ability of Language Models (LLMs) to refuse answering a question in instances where none of the provided contexts offer pertinent information. In practical scenarios, search engines frequently encounter difficulties in retrieving documents that contain the required answers. In such situations, it becomes crucial for the model to possess the capability to decline recognition, thereby preventing the generation of inaccurate or misleading content.. \\nHere is some provided context [Document(page_content='useful information entailed in the external documents. Ad-\\\\nditionally, LLMs can easily be misled by incorrect infor-\\\\nmation in the document. To this end, we build Retrieval-\\\\nAugmented Generation Benchmark (RGB) to evaluate the\\\\nretrieval-augmented generation of LLMs, and we concern\\\\nabout 4 specific abilities:\\\\nNoise Robustness is the robustness of LLMs in noisy\\\\ndocuments. As retrievers are not perfect, the external knowl-\\\\nedge they retrieve often contains a significant amount of\\\\nnoise, i.e., documents which are relevant to the question but\\\\ndo not contain any information about the answer. To effec-\\\\ntively answer user questions, LLMs must be able to extract\\\\nthe necessary information from documents despite there are\\\\nnoisy documents.\\\\nNegative Rejection is a measure of whether LLMs can\\\\ndecline to answer a question when none of the contexts pro-\\\\nvide useful information. In real-world situations, the search\\\\nengine often fails to retrieve documents containing the an-\\\\nswers. In these cases, it is important for the model to have\\\\nthe capability to reject recognition and avoid generating mis-\\\\nleading content.\\\\nInformation Integration is a capacity to integrate an-\\\\nswers from multiple documents. In many cases, the an-\\\\nswer to a question may be contained in multiple documents.\\\\nFor example, for the question “Who are the champions of\\\\nthe U.S. Open 2022 men’s and women’s singles?” , the two\\\\nchampions may be mentioned in different documents. In or-\\\\nder to provide better answers to complex questions, it is nec-\\\\nessary for LLMs to have the ability to integrate information.\\\\nCounterfactual Robustness refers to a capacity to han-', metadata={'page': 2, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'relevance_score': 0.99667555}), Document(page_content='iology and Medicine?”, the model will generate the known\\\\nquestion “Who was awarded the 2021 Nobel Prize in Lit-\\\\nerature?” and answer “ Abdulrazak Gurnah ”. We then man-\\\\nually verified the generated answers, and retrieve relevant\\\\ndocuments as described above. In order to make documents\\\\ncontain factual errors, we manually modify the answers and\\\\nreplace the corresponding parts in the document.\\\\nFinally, we collect totally 600 base questions in RGB,\\\\nand 200 additional questions for the information integration\\\\nability and 200 additional questions for counterfactual ro-\\\\nbustness ability. Half of the instances are in English, and the\\\\nother half are in Chinese.\\\\nEvaluation metrics\\\\nThe core of this benchmark is to evaluate whether LLMs can\\\\nutilize the provided external documents to acquire knowl-\\\\nedge and generate reasonable answers. We evaluate the re-\\\\nsponses of LLMs in order to measure above-mentioned four\\\\nabilities of them.\\\\nAccuracy is used to measure noise robustness and infor-\\\\nmation integration. We employ an exact matching approach\\\\nwhere if the generated text contains an exact match to the\\\\nanswer, it is considered as a correct answer.\\\\nRejection rate is used to measure negative rejection.\\\\nWhen only noisy documents are provided, LLMs should\\\\noutput the specific content – “I can not answer the question\\\\nbecause of the insufficient information in documents.” (We\\\\nuse instructions to inform the model.). If the model gener-\\\\nates this content, it indicates a successful rejection.\\\\nError detection rate measures whether the model can\\\\ndetect the factual errors in the documents for counterfactual\\\\nrobustness. When the provided documents contain factual\\\\nerrors, the model should output the specific content – “There\\\\nare factual errors in the provided documents.” (We use in-', metadata={'page': 3, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'relevance_score': 0.99490017}), Document(page_content='the question. This can cause confusion for LLMs and make\\\\nLLMs generate incorrect answers. In Table 2, the model an-\\\\nswer focuses on the concept “automotive revenue” in the\\\\ndocument rather than “revenue” in the question.\\\\nBased on the analysis above, we have identified certain\\\\nlimitations in LLMs regarding retrieval-augmented genera-\\\\ntion. To effectively handle the vast amount of noise present\\\\non the internet, further detailed enhancements are required\\\\nfor the model such as long documents modeling and precise\\\\nconcept comprehension.\\\\nResults on Negative Rejection testbed\\\\nWe evaluated the rejection rate when only noise documents\\\\nwere provided. The results are shown in Table 3. In addi-\\\\ntion to evaluating the rejection rate through exact matching\\\\n(Rej in Table 3), we also utilize ChatGPT to determine if\\\\nthe responses from the LLMs contain any rejection informa-\\\\ntion (Rej∗in Table 3). We can see that: Negative Rejection\\\\nposes a challenge for RAG in LLMs. The highest rejection\\\\nrates for LLMs in English and Chinese were only 45% and\\\\n43.33%, respectively. This suggests that LLMs can be easily\\\\nmisled by noisy documents, leading to incorrect answers.\\\\nIn addition, through comparing Rej and Rej∗, we found\\\\nthat LLMs fail to strictly follow instructions, and they often\\\\ngenerate unpredictable responses, which make it hard to use\\\\nthem as state triggers (such as for recognizing rejection).\\\\nWe conduct case studies in Table 4. The first error is\\\\nbecause of Evidence uncertainty . Although the document\\\\nonly mentions contact with “Adam McKay” and does not\\\\nexplicitly state that he is the director of the movie, the\\\\nmodel still concludes that he holds this role. The first er-\\\\nror is because of Concept confusion . The information pro-', metadata={'page': 5, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'relevance_score': 0.99465674})].\", 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a memory-augmented client model (client), a retriever model (cloud), and a memory generator based on Large Language Models (LLM) (cloud).', 'A general query is characterized by the necessity to retrieve and reason over multiple pieces of supporting evidence in order to furnish an answer. To put it differently, for a given multi-hop query denoted as q, the components within the retrieval set Rq collaboratively supply an answer to q. \\nHere is some provided context [Document(page_content=\\'this challenging MultiHop-RAG dataset and hope it\\\\nwill be a valuable resource for the community in de-\\\\nveloping and benchmarking RAG systems, thereby\\\\nunleashing the great potential of generative AI in\\\\npractice.\\\\n2 RAG with multi-Hop queries\\\\n2.1 Retrieval-augmented Generation (RAG)\\\\nIn an RAG application, we utilize an external cor-\\\\npus, denoted as D, which comprises multiple docu-\\\\nments and serves as the knowledge base. Each doc-\\\\nument within this corpus, represented as di∈ D, is\\\\nsegmented into a set of chunks.These chunks are\\\\nthen transformed into vector representations using\\\\nan embedding model and stored in an embedding\\\\ndatabase. Given a user query q, the system typi-\\\\ncally retrieves the top-K chunks that best match the\\\\nquery. These chunks constitute the retrieval set\\\\nfor query q, represented as Rq={r1, r2, ..., r K}.\\\\nThe retrieved chunks, combined with the query\\\\nand an optional prompt, are then fed into an LLM\\\\nto generate a final answer, following the format:\\\\nLLM(q,Rq,prompt )→answer .\\\\n2.2 Multi-Hop Query\\\\nWe define a multi-hop query as one that requires\\\\nretrieving and reasoning over multiple pieces of\\\\nsupporting evidence to provide an answer. In other\\\\nwords, for a multi-hop query q, the chunks in the\\\\nretrieval set Rq collectively provide an answer\\\\nto q. For example, the query \"Which company\\\\namong Google, Apple, and Nvidia reported the\\\\nlargest profit margins in their third-quarter reports\\\\nfor 2023?\" requires 1) retrieving relevant pieces of\\\\nevidence related to profit margins from the reports\\\\nof the three companies; 2) generating an answer by\\', metadata={\\'page\\': 2, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf\\', \\'relevance_score\\': 0.91862637}), Document(page_content=\\'words, for a multi-hop query q, the chunks in the\\\\nretrieval set Rq collectively provide an answer\\\\nto q. For example, the query \"Which company\\\\namong Google, Apple, and Nvidia reported the\\\\nlargest profit margins in their third-quarter reports\\\\nfor 2023?\" requires 1) retrieving relevant pieces of\\\\nevidence related to profit margins from the reports\\\\nof the three companies; 2) generating an answer by\\\\ncomparing and reasoning from the multiple pieces\\\\nof retrieved evidence. This differs from a single-\\\\nhop query such as \"What is Google’s profit margin\\\\nin the third-quarter reports for 2023,\" where the\\\\nanswer can be directly derived from a single piece\\\\nof evidence.\\\\nBased on the queries commonly used in real-\\\\nworld RAG systems, we identify four types of\\\\nmulti-hop queries. For each type, we present a\\\\nhypothetical query within the context of a financial\\\\nRAG system, where the knowledge base consists\\\\nof a collection of annual reports.\\\\nInference query: For such a query q, the answer\\\\nis deduced through reasoning from the retrieval\\\\nset Rq. An example of an inference query might be: Which report discusses the supply chain risk of\\\\nApple, the 2019 annual report or the 2020 annual\\\\nreport?\\\\nComparison query: For such a query q, the an-\\\\nswer requires a comparison of evidence within the\\\\nretrieval set Rq. For instance, a comparison query\\\\nmight ask: Did Netflix or Google report higher\\\\nrevenue for the year 2023? \"\\\\nTemporal query: For such a query q, the answer\\\\nrequires an analysis of the temporal information\\\\nof the retrieved chunks. For example, a temporal\\\\nquery may ask: Did Apple introduce the AirTag\\\\ntracking device before or after the launch of the 5th\\\\ngeneration iPad Pro?\\', metadata={\\'page\\': 2, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf\\', \\'relevance_score\\': 0.7841538}), Document(page_content=\\'as \"when.\" Moreover, the number of evidence re-\\\\nquired to answer a multi-hop query varies. Table\\\\n4 shows the distribution of evidence numbers for\\\\neach query in the dataset. Around 42% of queries\\\\ncan be answered using two pieces of evidence,\\\\nwhile approximately 30% and 15% of queries can\\\\nbe answered using three or four pieces of evidence,\\\\nrespectively.\\\\n4 Benchmarking RAG system using\\\\nMultiHop-RAG\\\\nMultiHop-RAG can be used as a benchmark for var-\\\\nious RAG-related tasks. Broadly speaking, RAG-\\', metadata={\\'page\\': 4, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf\\', \\'relevance_score\\': 0.7071014})].', 'Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, “Fermat” is the correct word, and REALM (row (c)) gives the word a much higher probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve documents to fill in the masked word even though it is trained with unsupervised text only.']\n",
      "Reranker\n",
      "{'context_precision': 0.9833, 'context_recall': 0.9467}\n",
      "AVG accuracy 0.6\n"
     ]
    }
   ],
   "source": [
    "result_compression_gpt4, acc_compression_gpt4 = run_and_evaluate(compression_retriever, prompt, llm_gpt4)\n",
    "print(\"Reranker\")\n",
    "print(result_compression_gpt4)\n",
    "print(\"AVG accuracy \"+ str(acc_compression_gpt4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:04<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\", 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'Model Layer: This particular layer is instrumental in powering AI products delivered via either proprietary APIs or open-source checkpoints (which necessitate hosting solutions). Within the Foundation Models category, there exist both proprietary models from private sources (such as GPT-4) and open-source models (like Stable Diffusion). Additionally, there are model hubs dedicated to the sharing and hosting of Foundation Models.', 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.', \"Pessimistic Rejection serves as an evaluation metric to assess the ability of Large Language Models (LLMs) to refuse answering a question in instances where none of the provided contexts offer pertinent information. In practical scenarios, search engines frequently encounter difficulties in retrieving documents that contain the required answers. In such situations, it becomes crucial for the model to possess the capability to decline recognition, thereby preventing the generation of inaccurate or misleading content.. \\nHere is some provided context [Document(page_content='the question. This can cause confusion for LLMs and make\\\\nLLMs generate incorrect answers. In Table 2, the model an-\\\\nswer focuses on the concept “automotive revenue” in the\\\\ndocument rather than “revenue” in the question.\\\\nBased on the analysis above, we have identified certain\\\\nlimitations in LLMs regarding retrieval-augmented genera-\\\\ntion. To effectively handle the vast amount of noise present\\\\non the internet, further detailed enhancements are required\\\\nfor the model such as long documents modeling and precise\\\\nconcept comprehension.\\\\nResults on Negative Rejection testbed\\\\nWe evaluated the rejection rate when only noise documents\\\\nwere provided. The results are shown in Table 3. In addi-\\\\ntion to evaluating the rejection rate through exact matching\\\\n(Rej in Table 3), we also utilize ChatGPT to determine if\\\\nthe responses from the LLMs contain any rejection informa-\\\\ntion (Rej∗in Table 3). We can see that: Negative Rejection\\\\nposes a challenge for RAG in LLMs. The highest rejection\\\\nrates for LLMs in English and Chinese were only 45% and\\\\n43.33%, respectively. This suggests that LLMs can be easily\\\\nmisled by noisy documents, leading to incorrect answers.\\\\nIn addition, through comparing Rej and Rej∗, we found\\\\nthat LLMs fail to strictly follow instructions, and they often\\\\ngenerate unpredictable responses, which make it hard to use\\\\nthem as state triggers (such as for recognizing rejection).\\\\nWe conduct case studies in Table 4. The first error is\\\\nbecause of Evidence uncertainty . Although the document\\\\nonly mentions contact with “Adam McKay” and does not\\\\nexplicitly state that he is the director of the movie, the\\\\nmodel still concludes that he holds this role. The first er-\\\\nror is because of Concept confusion . The information pro-', metadata={'page': 5, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'}), Document(page_content='iology and Medicine?”, the model will generate the known\\\\nquestion “Who was awarded the 2021 Nobel Prize in Lit-\\\\nerature?” and answer “ Abdulrazak Gurnah ”. We then man-\\\\nually verified the generated answers, and retrieve relevant\\\\ndocuments as described above. In order to make documents\\\\ncontain factual errors, we manually modify the answers and\\\\nreplace the corresponding parts in the document.\\\\nFinally, we collect totally 600 base questions in RGB,\\\\nand 200 additional questions for the information integration\\\\nability and 200 additional questions for counterfactual ro-\\\\nbustness ability. Half of the instances are in English, and the\\\\nother half are in Chinese.\\\\nEvaluation metrics\\\\nThe core of this benchmark is to evaluate whether LLMs can\\\\nutilize the provided external documents to acquire knowl-\\\\nedge and generate reasonable answers. We evaluate the re-\\\\nsponses of LLMs in order to measure above-mentioned four\\\\nabilities of them.\\\\nAccuracy is used to measure noise robustness and infor-\\\\nmation integration. We employ an exact matching approach\\\\nwhere if the generated text contains an exact match to the\\\\nanswer, it is considered as a correct answer.\\\\nRejection rate is used to measure negative rejection.\\\\nWhen only noisy documents are provided, LLMs should\\\\noutput the specific content – “I can not answer the question\\\\nbecause of the insufficient information in documents.” (We\\\\nuse instructions to inform the model.). If the model gener-\\\\nates this content, it indicates a successful rejection.\\\\nError detection rate measures whether the model can\\\\ndetect the factual errors in the documents for counterfactual\\\\nrobustness. When the provided documents contain factual\\\\nerrors, the model should output the specific content – “There\\\\nare factual errors in the provided documents.” (We use in-', metadata={'page': 3, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'}), Document(page_content='useful information entailed in the external documents. Ad-\\\\nditionally, LLMs can easily be misled by incorrect infor-\\\\nmation in the document. To this end, we build Retrieval-\\\\nAugmented Generation Benchmark (RGB) to evaluate the\\\\nretrieval-augmented generation of LLMs, and we concern\\\\nabout 4 specific abilities:\\\\nNoise Robustness is the robustness of LLMs in noisy\\\\ndocuments. As retrievers are not perfect, the external knowl-\\\\nedge they retrieve often contains a significant amount of\\\\nnoise, i.e., documents which are relevant to the question but\\\\ndo not contain any information about the answer. To effec-\\\\ntively answer user questions, LLMs must be able to extract\\\\nthe necessary information from documents despite there are\\\\nnoisy documents.\\\\nNegative Rejection is a measure of whether LLMs can\\\\ndecline to answer a question when none of the contexts pro-\\\\nvide useful information. In real-world situations, the search\\\\nengine often fails to retrieve documents containing the an-\\\\nswers. In these cases, it is important for the model to have\\\\nthe capability to reject recognition and avoid generating mis-\\\\nleading content.\\\\nInformation Integration is a capacity to integrate an-\\\\nswers from multiple documents. In many cases, the an-\\\\nswer to a question may be contained in multiple documents.\\\\nFor example, for the question “Who are the champions of\\\\nthe U.S. Open 2022 men’s and women’s singles?” , the two\\\\nchampions may be mentioned in different documents. In or-\\\\nder to provide better answers to complex questions, it is nec-\\\\nessary for LLMs to have the ability to integrate information.\\\\nCounterfactual Robustness refers to a capacity to han-', metadata={'page': 2, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'})].\", 'The corrected query is: This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a memory-augmented model (client), a retriever model (cloud), and a memory generator based on Large Language Models (LLM) (cloud).', 'A general query is characterized by the necessity to retrieve and reason over multiple pieces of supporting evidence in order to furnish an answer. To put it differently, for a given multi-hop query denoted as q, the components within the retrieval set Rq collaboratively supply an answer to q. \\nHere is some provided context [Document(page_content=\\'words, for a multi-hop query q, the chunks in the\\\\nretrieval set Rq collectively provide an answer\\\\nto q. For example, the query \"Which company\\\\namong Google, Apple, and Nvidia reported the\\\\nlargest profit margins in their third-quarter reports\\\\nfor 2023?\" requires 1) retrieving relevant pieces of\\\\nevidence related to profit margins from the reports\\\\nof the three companies; 2) generating an answer by\\\\ncomparing and reasoning from the multiple pieces\\\\nof retrieved evidence. This differs from a single-\\\\nhop query such as \"What is Google’s profit margin\\\\nin the third-quarter reports for 2023,\" where the\\\\nanswer can be directly derived from a single piece\\\\nof evidence.\\\\nBased on the queries commonly used in real-\\\\nworld RAG systems, we identify four types of\\\\nmulti-hop queries. For each type, we present a\\\\nhypothetical query within the context of a financial\\\\nRAG system, where the knowledge base consists\\\\nof a collection of annual reports.\\\\nInference query: For such a query q, the answer\\\\nis deduced through reasoning from the retrieval\\\\nset Rq. An example of an inference query might be: Which report discusses the supply chain risk of\\\\nApple, the 2019 annual report or the 2020 annual\\\\nreport?\\\\nComparison query: For such a query q, the an-\\\\nswer requires a comparison of evidence within the\\\\nretrieval set Rq. For instance, a comparison query\\\\nmight ask: Did Netflix or Google report higher\\\\nrevenue for the year 2023? \"\\\\nTemporal query: For such a query q, the answer\\\\nrequires an analysis of the temporal information\\\\nof the retrieved chunks. For example, a temporal\\\\nquery may ask: Did Apple introduce the AirTag\\\\ntracking device before or after the launch of the 5th\\\\ngeneration iPad Pro?\\', metadata={\\'page\\': 2, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf\\'}), Document(page_content=\\'retrieval set Rq. For instance, a comparison query\\\\nmight ask: Did Netflix or Google report higher\\\\nrevenue for the year 2023? \"\\\\nTemporal query: For such a query q, the answer\\\\nrequires an analysis of the temporal information\\\\nof the retrieved chunks. For example, a temporal\\\\nquery may ask: Did Apple introduce the AirTag\\\\ntracking device before or after the launch of the 5th\\\\ngeneration iPad Pro?\\\\nNull query: For such a query q, the answer cannot\\\\nbe derived from the retrieved set Rq. We include\\\\nthe null query to assess the generation quality, es-\\\\npecially regarding the issue of hallucination. For a\\\\nnull query, even though a retrieved set is provided,\\\\nan LLM should produce a null response instead\\\\nof hallucinating an answer. For example, assum-\\\\ning ABCD is a non-existent company, a null query\\\\nmight ask: What are the sales of company ABCD\\\\nas reported in its 2022 and 2023 annual reports?\\\\n2.3 Evaluation Metrics\\\\nAn RAG system handling multi-hop queries can be\\\\nassessed from two key aspects: retrieval evaluation\\\\nand generation evaluation.\\\\nRetrieval Evaluation: Evidently, the quality of\\\\nthe retrieval set Rq determines the final genera-\\\\ntion quality. We compare the retrieved set with\\\\nthe ground truth evidence associated with each\\\\nquery, except for the null queries, as they have\\\\nno evidence to derive from. Assuming the top-\\\\nK chunks are retrieved, i.e., |Rq|=K, we use\\\\nretrieval evaluation metrics including Mean Aver-\\\\nage Precision at K (MAP@K), Mean Reciprocal\\\\nRank at K (MRR@K), and Hit Rate at K (Hit@K).\\\\nMAP@K measures the average top-K retrieval pre-\\', metadata={\\'page\\': 2, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf\\'}), Document(page_content=\\'as \"when.\" Moreover, the number of evidence re-\\\\nquired to answer a multi-hop query varies. Table\\\\n4 shows the distribution of evidence numbers for\\\\neach query in the dataset. Around 42% of queries\\\\ncan be answered using two pieces of evidence,\\\\nwhile approximately 30% and 15% of queries can\\\\nbe answered using three or four pieces of evidence,\\\\nrespectively.\\\\n4 Benchmarking RAG system using\\\\nMultiHop-RAG\\\\nMultiHop-RAG can be used as a benchmark for var-\\\\nious RAG-related tasks. Broadly speaking, RAG-\\', metadata={\\'page\\': 4, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf\\'})].', 'Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, “Fermat” is the correct word, and REALM (row (c)) gives the word a much higher probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve documents to fill in the masked word even though it is trained with unsupervised text only.']\n",
      "Ensambler 75/25\n",
      "{'context_precision': 0.9333, 'context_recall': 0.9083}\n",
      "AVG accuracy 0.5\n"
     ]
    }
   ],
   "source": [
    "result_3_gpt4, acc_3_gpt4 = run_and_evaluate(retriever_3, prompt, llm_gpt4)\n",
    "print(\"Ensambler 75/25\")\n",
    "print(result_3_gpt4)\n",
    "print(\"AVG accuracy \"+ str(acc_3_gpt4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:03<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\", 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'Model Layer: This particular layer is instrumental in powering AI products delivered via either proprietary APIs or open-source checkpoints (which necessitate hosting solutions). Within the Foundation Models category, there exist both proprietary models from private sources (such as GPT-4) and open-source models (like Stable Diffusion). Additionally, there are model hubs dedicated to the sharing and hosting of Foundation Models.', 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-large) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.', \"Pessimistic Rejection serves as an evaluation metric to assess the ability of Language Models (LLMs) to refuse answering a question in instances where none of the provided contexts offer pertinent information. In practical scenarios, search engines frequently encounter difficulties in retrieving documents that contain the required answers. In such situations, it becomes crucial for the model to possess the capability to decline recognition, thereby preventing the generation of inaccurate or misleading content.. \\nHere is some provided context [Document(page_content='documents. As retrievers are not perfect, the external knowl-\\\\nedge they retrieve often contains a significant amount of\\\\nnoise, i.e., documents which are relevant to the question but\\\\ndo not contain any information about the answer. To effec-\\\\ntively answer user questions, LLMs must be able to extract\\\\nthe necessary information from documents despite there are\\\\nnoisy documents.\\\\nNegative Rejection is a measure of whether LLMs can\\\\ndecline to answer a question when none of the contexts pro-\\\\nvide useful information. In real-world situations, the search\\\\nengine often fails to retrieve documents containing the an-\\\\nswers. In these cases, it is important for the model to have\\\\nthe capability to reject recognition and avoid generating mis-\\\\nleading content.\\\\nInformation Integration is a capacity to integrate an-\\\\nswers from multiple documents. In many cases, the an-\\\\nswer to a question may be contained in multiple documents.\\\\nFor example, for the question “Who are the champions of\\\\nthe U.S. Open 2022 men’s and women’s singles?” , the two\\\\nchampions may be mentioned in different documents. In or-\\\\nder to provide better answers to complex questions, it is nec-\\\\nessary for LLMs to have the ability to integrate information.\\\\nCounterfactual Robustness refers to a capacity to han-\\\\ndle errors in external knowledge. In the real world, there is\\\\nan abundance of false information on the internet. Please', metadata={'page': 2, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'}), Document(page_content='ject to answer the question when the required knowledge\\\\nis not present in any retrieved document. The testbed for\\\\nnegative rejection contains instances whose external doc-\\\\numents are only with noisy documents. LLMs are ex-\\\\npected to indicate “insufficient information” or other re-\\\\njection signals.\\\\n•Information Integration , which evaluates whether\\\\nLLMs can answer complex questions that require inte-\\\\ngrating information from multiple documents. For the in-\\\\nstance in Figure 1, for the question “When were the Chat-\\\\nGPT app for iOS and ChatGPT api launched?”, LLMs\\\\nare expected to provide information of the launch dates\\\\nfor both the ChatGPT iOS app and ChatGPT API. The\\\\ntestbed for information integration contains instances\\\\nthat can only be answered using multiple documents.\\\\n•Counterfactual Robustness , which evaluates whether\\\\nLLMs can identify risks of known factual errors in the\\\\nretrieved documents when the LLMs are given warnings\\\\nabout potential risks in the retrieved information through\\\\ninstruction. The testbed for counterfactual robustness in-\\\\ncludes instances that can be answered directly by the\\\\nLLMs, but the external documents contain factual errors.Based on RGB, we conduct evaluation on 6 state-of-\\\\nthe-art large language models including ChatGPT (Ope-\\\\nnAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-\\\\n6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\\\\nQwen-7B-Chat (QwenLM 2023), BELLE-7B (Yunjie Ji\\\\n2023). We found that even though RAG can improve the re-\\\\nsponse accuracy of LLMs, they still suffer from the above-\\\\nmentioned challenges significantly. Specifically, we found\\\\nthat even though LLMs demonstrate some level of noise ro-\\\\nbustness, they tend to confuse similar information and fre-\\\\nquently generate inaccurate answers when relevant informa-\\\\ntion exists. For example, when faced with a question about\\\\nthe 2022 Nobel Prize in Literature, if there are noisy docu-\\\\nments about the 2021 Nobel Prize in Literature in external\\\\ndocuments, LLMs may become confused and provide inac-', metadata={'page': 1, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'}), Document(page_content='tion to evaluating the rejection rate through exact matching\\\\n(Rej in Table 3), we also utilize ChatGPT to determine if\\\\nthe responses from the LLMs contain any rejection informa-\\\\ntion (Rej∗in Table 3). We can see that: Negative Rejection\\\\nposes a challenge for RAG in LLMs. The highest rejection\\\\nrates for LLMs in English and Chinese were only 45% and\\\\n43.33%, respectively. This suggests that LLMs can be easily\\\\nmisled by noisy documents, leading to incorrect answers.\\\\nIn addition, through comparing Rej and Rej∗, we found\\\\nthat LLMs fail to strictly follow instructions, and they often\\\\ngenerate unpredictable responses, which make it hard to use\\\\nthem as state triggers (such as for recognizing rejection).\\\\nWe conduct case studies in Table 4. The first error is\\\\nbecause of Evidence uncertainty . Although the document\\\\nonly mentions contact with “Adam McKay” and does not\\\\nexplicitly state that he is the director of the movie, the\\\\nmodel still concludes that he holds this role. The first er-\\\\nror is because of Concept confusion . The information pro-\\\\nvided in the answer pertains to “the 2018 Winter Olympics”\\\\ninstead of “the 2022 Olympics” mentioned in the question.\\\\nRetrieval-augmented generation poses a greater challenge of\\\\nnegative rejection compared to answer directly as it presents\\\\nrelevant documents that could potentially mislead the LLMsLanguages English Chinese\\\\nRej Rej∗Rej Rej∗\\\\nChatGPT 24.67 45.00 5.33 43.33\\\\nChatGLM-6B 9.00 25.00 6.33 17.00\\\\nChatGLM2-6B 10.33 41.33 6.33 36.33\\\\nVicuna-7B-v1.3 17.00 33.33 3.37 24.67\\\\nQwen-7B-Chat 31.00 35.67 8.67 25.33\\\\nBELLE-7B-2M 5.67 32.33 5.33 13.67\\\\nTable 3: The result of negative rejection. Rej means the re-\\\\njection rate (%) and Rej∗means the rejection rate evaluated\\\\nby ChatGPT. We can see that negative rejection poses a chal-', metadata={'page': 5, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'}), Document(page_content='Table 3: The result of negative rejection. Rej means the re-\\\\njection rate (%) and Rej∗means the rejection rate evaluated\\\\nby ChatGPT. We can see that negative rejection poses a chal-\\\\nlenge for RAG in LLMs.\\\\nQuestion Answer Response\\\\nwho will direct\\\\nIrredeemable film?Jeymes\\\\nSamuelThe information provided in the document\\\\nis that Fox has attached “The Big Short”\\\\nwriter-director Adam McKay to its movie\\\\nadaptation of “Irredeemable” from Boom!\\\\nComics. Therefore, the answer is Adam McKay.\\\\nWhich country won the\\\\nmost medals at the\\\\n2022 Winter Olympics?NorwayBased on the information provided in the\\\\ndocument, the country that won the most medals\\\\nat the 2022 Winter Olympics is Germany.\\\\nGermany has won a total of 31 medals, including 14\\\\ngold medals, 14 bronze medals, and 31 silver medals.\\\\nTable 4: Error cases of negative rejection generated by\\\\nChatGLM2-6B. The red text highlights the error answers.\\\\nand result in incorrect responses. In future developments, it\\\\nwill be crucial for LLMs to enhance their ability to accu-\\\\nrately match questions with the appropriate documents.\\\\nResults on Information Integration testbed\\\\nWe evaluated the accuracy based on the different noise ratios\\\\nin external documents, and the results are shown in Table 5.\\\\nWhen comparing the model to Table 1, we observed that\\\\nit has a weak information integration ability, which in turn\\\\naffects its noise robustness. We can see that:\\\\n(1) Information integration poses a challenge for RAG\\\\nin LLMs. Even without noise, the highest accuracy of LLMs\\\\ncan only reach 60% and 67% for English and Chinese,\\\\nrespectively. After adding noise, the highest accuracy de-\\\\ncreases to 43% and 55%. These results suggest that LLMs\\\\nstruggle with integrating information effectively and are not\\\\nwell-suited for directly answering complex questions.\\\\n(2) Complex questions are more challenging for RAG\\\\nwith noisy documents. Performance decline becomes sig-\\\\nnificant when the noise ratio is 0.4, but for simple problems,\\\\na significant decline occurs only at a noise ratio of 0.8 at a\\\\nsignificance level of 0.05. This indicates that complex prob-', metadata={'page': 5, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'})].\", 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a memory-augmented client model (client), a retriever model (cloud), and a memory generator based on Large Language Models (LLM) (cloud).', 'A general query is characterized by the necessity to retrieve and reason over multiple pieces of supporting evidence in order to furnish an answer. To put it differently, for a given multi-hop query denoted as q, the components within the retrieval set Rq collaboratively supply an answer to q.', 'Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, “Fermat” is the correct word, and REALM (row (c)) gives the word a much higher probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve documents to fill in the masked word even though it is trained with unsupervised text only.']\n",
      "CHUNK SIZE 500 gpt4\n",
      "{'context_precision': 0.8222, 'context_recall': 1.0000}\n",
      "AVG accuracy 0.5\n"
     ]
    }
   ],
   "source": [
    "result_500_gpt4, acc_500_gpt4 = run_and_evaluate(retriever_500, prompt, llm_gpt4)\n",
    "print(\"CHUNK SIZE 500 gpt4\")\n",
    "print(result_500_gpt4)\n",
    "print(\"AVG accuracy \"+ str(acc_500_gpt4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:05<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\", 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'Model Layer: This particular layer is instrumental in powering AI products delivered via either proprietary APIs or open-source checkpoints (which necessitate hosting solutions). Within the Foundation Models category, there exist both proprietary models from private sources (such as GPT-4) and open-source models (like Stable Diffusion). Additionally, there are model hubs dedicated to the sharing and hosting of Foundation Models.', 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.', \"Pessimistic Rejection serves as an evaluation metric to assess the ability of Large Language Models (LLMs) to refuse answering a question in instances where none of the provided contexts offer pertinent information. In practical scenarios, search engines frequently encounter difficulties in retrieving documents that contain the required answers. In such situations, it becomes crucial for the model to possess the capability to decline recognition, thereby preventing the generation of inaccurate or misleading content.. \\nHere is some provided context [Document(page_content='the question. This can cause confusion for LLMs and make\\\\nLLMs generate incorrect answers. In Table 2, the model an-\\\\nswer focuses on the concept “automotive revenue” in the\\\\ndocument rather than “revenue” in the question.\\\\nBased on the analysis above, we have identified certain\\\\nlimitations in LLMs regarding retrieval-augmented genera-\\\\ntion. To effectively handle the vast amount of noise present\\\\non the internet, further detailed enhancements are required\\\\nfor the model such as long documents modeling and precise\\\\nconcept comprehension.\\\\nResults on Negative Rejection testbed\\\\nWe evaluated the rejection rate when only noise documents\\\\nwere provided. The results are shown in Table 3. In addi-\\\\ntion to evaluating the rejection rate through exact matching\\\\n(Rej in Table 3), we also utilize ChatGPT to determine if\\\\nthe responses from the LLMs contain any rejection informa-\\\\ntion (Rej∗in Table 3). We can see that: Negative Rejection\\\\nposes a challenge for RAG in LLMs. The highest rejection\\\\nrates for LLMs in English and Chinese were only 45% and\\\\n43.33%, respectively. This suggests that LLMs can be easily\\\\nmisled by noisy documents, leading to incorrect answers.\\\\nIn addition, through comparing Rej and Rej∗, we found\\\\nthat LLMs fail to strictly follow instructions, and they often\\\\ngenerate unpredictable responses, which make it hard to use\\\\nthem as state triggers (such as for recognizing rejection).\\\\nWe conduct case studies in Table 4. The first error is\\\\nbecause of Evidence uncertainty . Although the document\\\\nonly mentions contact with “Adam McKay” and does not\\\\nexplicitly state that he is the director of the movie, the\\\\nmodel still concludes that he holds this role. The first er-\\\\nror is because of Concept confusion . The information pro-', metadata={'page': 5, 'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf'}), Document(page_content='Metacognitive Retrieval-Augmented Large Language Models WWW ’24, May 13–17, 2024, Singapore, Singapore\\\\nPrompt Eval:Please act as an evaluator-critic system,\\\\ndetermine if you can provide a reliable answer\\\\nto the {question} based on your own knowledge?\\\\n•External Knowledge Evaluating : To gauge the adequacy of ex-\\\\nternal knowledge sources, we deploy an advanced NLI model\\\\nTRUE [ 8] to examine if the retrieved documents, represented\\\\nas𝐷𝑞, provide enough information to answer the question. The\\\\nprocess is formulated as:\\\\n𝑓\\\\x10h\\\\n{𝑑𝑖}|𝐷|\\\\n𝑖=1i\\\\n,𝑞\\\\x11\\\\n, (4)\\\\nwith𝑓(premise,hypothesis)being the function of the NLI model.\\\\nIt returns a value of 1 if the premise entails the hypothesis, oth-\\\\nerwise, it returns 0.\\\\nUpon evaluating through the aforementioned model-based eval-\\\\nuating methods, we can classify the situation into four categories\\\\n(as in Sec. 3.2) in an automatic manner. Each situation highlights\\\\nspecific potential sources of errors, leading to varying strategies\\\\nemployed for future planning depending on the identified category.\\\\nDeclarative Knowledge . Addressing question (b), declarative\\\\nknowledge within MetaRAG is directed towards identifying preva-\\\\nlent error patterns. This aids in identifying possible pitfalls in the\\\\nreasoning process. We categorize typical mistakes into three types:\\\\n•Incomplete Reasoning : This error is the most prevalent in multi-\\\\nhop QA. It arises when the model fails to utilize all relevant\\\\nfragments from the given context or does not follow a compre-\\\\nhensive chain of thought to arrive at the correct answer.', metadata={'page': 4, 'source': '..\\\\\\\\papers\\\\\\\\Metacognitive Retrieval-Augmented Large Language Models.pdf'}), Document(page_content='Figure 6: RAG compared with other model optimization methods\\\\nand avoiding contradictions.\\\\nAnswer Relevance requires that the generated answers are\\\\ndirectly pertinent to the posed questions, effectively address-\\\\ning the core inquiry.\\\\nRequired Abilities\\\\nRAG evaluation also encompasses four abilities indicative of\\\\nits adaptability and efficiency: noise robustness, negative re-\\\\njection, information integration, and counterfactual robust-\\\\nness [Chen et al. , 2023b, Liu et al. , 2023b ]. These abilities\\\\nare critical for the model’s performance under various chal-\\\\nlenges and complex scenarios, impacting the quality scores.\\\\nNoise Robustness appraises the model’s capability to man-\\\\nage noise documents that are question-related but lack sub-\\\\nstantive information.\\\\nNegative Rejection assesses the model’s discernment in re-\\\\nfraining from responding when the retrieved documents do\\\\nnot contain the necessary knowledge to answer a question.\\\\nInformation Integration evaluates the model’s proficiency\\\\nin synthesizing information from multiple documents to ad-\\\\ndress complex questions.\\\\nCounterfactual Robustness tests the model’s ability to rec-\\\\nognize and disregard known inaccuracies within documents,\\\\neven when instructed about potential misinformation.\\\\nContext relevance and noise robustness are important for\\\\nevaluating the quality of retrieval, while answer faithfulness,\\\\nanswer relevance, negative rejection, information integration,\\\\nand counterfactual robustness are important for evaluating thequality of generation.\\\\nThe specific metrics for each evaluation aspect are summa-\\\\nrized in Table 2. It is essential to recognize that these metrics,\\\\nderived from related work, are traditional measures and do\\\\nnot yet represent a mature or standardized approach for quan-\\\\ntifying RAG evaluation aspects. Custom metrics tailored to\\\\nthe nuances of RAG models, though not included here, have', metadata={'page': 17, 'source': '..\\\\\\\\papers\\\\\\\\Retrieval-Augmented Generation for Large Language Models A Survey.pdf'}), Document(page_content='answers towards more common entities and terms from the training data. Finally, we conducted\\\\nexperiments on only three large language models. It is possible that some of our conclusions or\\\\nobservations may not necessarily hold for other models trained with different data or objectives.\\\\nRegarding ethical solutions, future work includes (i) further exploring potential bias and intentional\\\\nor unintentional harm that may result from using generated contextual documents; (ii) better aligning\\\\nlanguage models with user intent to generate less biased contents and fewer fabricated facts.\\\\n10', metadata={'page': 9, 'source': '..\\\\\\\\papers\\\\\\\\GENERATE RATHER THAN RETRIEVE LARGE LANGUAGE MODELS ARE STRONG CONTEXT GENERATORS.pdf'})].\", 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a memory-augmented client model (client), a retriever model (cloud), and a memory generator based on Large Language Models (LLM) (cloud).', 'A general query is characterized by the necessity to retrieve and reason over multiple pieces of supporting evidence in order to furnish an answer. To put it differently, for a given multi-hop query denoted as q, the components within the retrieval set Rq collaboratively supply an answer to q. \\nHere is some provided context [Document(page_content=\\'In other words, for a multi-hop query q, the chunks in the\\\\nretrieval set Rq collectively provide an answer\\\\nto q. For example, the query \"Which company\\\\namong Google, Apple, and Nvidia reported the\\\\nlargest profit margins in their third-quarter reports\\\\nfor 2023?\" requires 1) retrieving relevant pieces of\\\\nevidence related to profit margins from the reports\\\\nof the three companies; 2) generating an answer by\\\\ncomparing and reasoning from the multiple pieces\\\\nof retrieved evidence. This differs from a single-\\\\nhop query such as \"What is Google’s profit margin\\\\nin the third-quarter reports for 2023,\" where the\\\\nanswer can be directly derived from a single piece\\\\nof evidence.\\\\nBased on the queries commonly used in real-\\\\nworld RAG systems, we identify four types of\\\\nmulti-hop queries. For each type, we present a\\\\nhypothetical query within the context of a financial\\\\nRAG system, where the knowledge base consists\\\\nof a collection of annual reports.\\\\nInference query: For such a query q, the answer\\\\nis deduced through reasoning from the retrieval\\\\nsetRq. An example of an inference query might be: Which report discusses the supply chain risk of\\\\nApple, the 2019 annual report or the 2020 annual\\\\nreport?\\\\nComparison query: For such a query q, the an-\\\\nswer requires a comparison of evidence within the\\\\nretrieval set Rq. For instance, a comparison query\\\\nmight ask: Did Netflix or Google report higher\\\\nrevenue for the year 2023? \"\\\\nTemporal query: For such a query q, the answer\\\\nrequires an analysis of the temporal information\\\\nof the retrieved chunks. For example, a temporal\\\\nquery may ask: Did Apple introduce the AirTag\\\\ntracking device before or after the launch of the 5th\\\\ngeneration iPad Pro?\\', metadata={\\'page\\': 2, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf\\'}), Document(page_content=\\'as \"when.\" Moreover, the number of evidence re-\\\\nquired to answer a multi-hop query varies. Table\\\\n4 shows the distribution of evidence numbers for\\\\neach query in the dataset. Around 42% of queries\\\\ncan be answered using two pieces of evidence,\\\\nwhile approximately 30% and 15% of queries can\\\\nbe answered using three or four pieces of evidence,\\\\nrespectively.\\\\n4 Benchmarking RAG system using\\\\nMultiHop-RAG\\\\nMultiHop-RAG can be used as a benchmark for var-\\\\nious RAG-related tasks. Broadly speaking, RAG-\\', metadata={\\'page\\': 4, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf\\'}), Document(page_content=\\'information from different locations or sources to arrive at an answer. Considering you have read\\\\nat least two news articles on <entity> , construct a multi-hop question that incorporates all the news\\\\nsources. The source of the news should be stated in the question. Also, ensure that the answer to the\\\\nquestion is a single word/entity. Do not answer this question directly. Just give me the question:\\\\nTable 11: Null Query Generation Prompting\\', metadata={\\'page\\': 11, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf\\'}), Document(page_content=\\'exploration of The fundamental reasons for causing the model to\\\\nanswer incorrectly in RAG. Drawing inspiration from the domain\\\\nof cognitive psychology, we integrate metacognitive ability into\\\\nLLMs to enable the model to be aware of its reasoning process, thus\\\\nenhancing the quality of answer generation.\\\\n3 PRELIMINARY\\\\nIn this section, we formulate the task of retrieval-augmented gen-\\\\neration and investigate its limitations on multi-hop QA.\\\\n3.1 Task Definition\\\\nGiven a question 𝑞and a retrieval corpus 𝐷={𝑑𝑖}|𝐷|\\\\n𝑖=1(with Wikipedia\\\\narticles serving as the primary data source in this study), the goal\\\\nof retrieval-augmented LLMs is to generate an answer 𝑦based on\\\\nthe question as well as the documents retrieved in relation to it.\\\\nThis can be represented as:\\\\n𝑦=LLM QA([𝐷𝑞,𝑞],Prompt QA), (1)\\\\nPrompt QA:Please act as a question-answering system, answer\\\\nthe {question} based on the {retrieved documents}\\\\nwhere𝐷𝑞is the retrieved documents for the query 𝑞.[·,·]is con-\\\\ncatenation following designated prompts. LLM QAis the role of the\\\\nLLM, which concentrates on question answering tasks.\\\\n3.2 Task Exploration\\\\nWe conduct an empirical study to evaluate the effectiveness of\\\\nretrieval-augmented LLMs under various knowledge conditions.\\\\nOur main aim is to ascertain whether a question can be answered\\\\nutilizing either the intrinsic knowledge within the LLM or via ex-\\\\nternally retrieved documents. Through human annotation of (a) the\\', metadata={\\'page\\': 2, \\'source\\': \\'..\\\\\\\\papers\\\\\\\\Metacognitive Retrieval-Augmented Large Language Models.pdf\\'})].', 'Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, “Fermat” is the correct word, and REALM (row (c)) gives the word a much higher probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve documents to fill in the masked word even though it is trained with unsupervised text only.']\n",
      "Marginal relevance\n",
      "{'context_precision': 0.8917, 'context_recall': 0.8750}\n",
      "AVG accuracy 0.6\n"
     ]
    }
   ],
   "source": [
    "result_mmr_gpt4, acc_mmr_gpt4 = run_and_evaluate(retriever_mmr, prompt, llm_gpt4)\n",
    "print(\"Marginal relevance\")\n",
    "print(result_mmr_gpt4)\n",
    "print(\"AVG accuracy \"+ str(acc_mmr_gpt4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 20/20 [00:02<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In CRUD, the letter 'C' stands for create, 'R' stands for read, 'U' stands for update and 'D' stands for delete.\", 'The results of the experiment show that the version of the Forms Copilot that leverages all data sources and responses, uses chain of thought prompting, asks the model to cite the sources, and provides a description of the form has achieved an average of 2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the suggestions are similar to the ground truth.', 'Model Layer: This particular layer is instrumental in powering AI products delivered via either proprietary APIs or open-source checkpoints (which necessitate hosting solutions). Within the Foundation Models category, there exist both proprietary models from private sources (such as GPT-4) and open-source models (like Stable Diffusion). Additionally, there are model hubs dedicated to the sharing and hosting of Foundation Models.', 'After contextual documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e., FiD-l) and 3B parameter models (i.e., FiD-xl) that are fine-tuned on the training split of target datasets.', \"Pessimistic Rejection serves as an evaluation metric to assess the ability of Language Models (LLMs) to refuse answering a question in instances where none of the provided contexts offer pertinent information. In practical scenarios, search engines frequently encounter difficulties in retrieving documents that contain the required answers. In such situations, it becomes crucial for the model to possess the capability to decline recognition, thereby preventing the generation of inaccurate or misleading content.. \\nHere is some provided context [Document(page_content='the necessary information from documents despite there are\\\\nnoisy documents.\\\\nNegative Rejection is a measure of whether LLMs can\\\\ndecline to answer a question when none of the contexts pro-\\\\nvide useful information. In real-world situations, the search\\\\nengine often fails to retrieve documents containing the an-\\\\nswers. In these cases, it is important for the model to have\\\\nthe capability to reject recognition and avoid generating mis-\\\\nleading content.\\\\nInformation Integration is a capacity to integrate an-\\\\nswers from multiple documents. In many cases, the an-\\\\nswer to a question may be contained in multiple documents.\\\\nFor example, for the question “Who are the champions of\\\\nthe U.S. Open 2022 men’s and women’s singles?” , the two\\\\nchampions may be mentioned in different documents. In or-\\\\nder to provide better answers to complex questions, it is nec-\\\\nessary for LLMs to have the ability to integrate information.\\\\nCounterfactual Robustness refers to a capacity to han-\\\\ndle errors in external knowledge. In the real world, there is\\\\nan abundance of false information on the internet. Please', metadata={'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 2}), Document(page_content='cludes instances that can be answered directly by the\\\\nLLMs, but the external documents contain factual errors.Based on RGB, we conduct evaluation on 6 state-of-\\\\nthe-art large language models including ChatGPT (Ope-\\\\nnAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-\\\\n6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\\\\nQwen-7B-Chat (QwenLM 2023), BELLE-7B (Yunjie Ji\\\\n2023). We found that even though RAG can improve the re-\\\\nsponse accuracy of LLMs, they still suffer from the above-\\\\nmentioned challenges significantly. Specifically, we found\\\\nthat even though LLMs demonstrate some level of noise ro-\\\\nbustness, they tend to confuse similar information and fre-\\\\nquently generate inaccurate answers when relevant informa-\\\\ntion exists. For example, when faced with a question about\\\\nthe 2022 Nobel Prize in Literature, if there are noisy docu-\\\\nments about the 2021 Nobel Prize in Literature in external\\\\ndocuments, LLMs may become confused and provide inac-\\\\ncurate answers. Besides, LLMs frequently fail to reject an-\\\\nswering and generate incorrect answers when none of the\\\\nexternal documents contain relevant information. Further-\\\\nmore, LLMs lack the ability to summarize from multiple\\\\ndocuments, and therefore if multiple documents are needed\\\\nto answer a question, LLMs often fail to provide accurate\\\\nanswer. Finally, we found that even when the LLMs contain\\\\nthe required knowledge and are given warnings about po-\\\\ntential risks in the retrieved information through instruction,\\\\nthey still tend to trust and prioritize the retrieved information\\\\nover their own existing knowledge. The experimental results\\\\nmentioned above highlight the need for further resolution of', metadata={'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 1}), Document(page_content='augmented generation abilities we aim to evaluate. Next, we\\\\noutline the process of constructing the RAG benchmark for\\\\nevaluation. Lastly, we present the evaluation metrics.\\\\nRequired abilities of RAG\\\\nExternal knowledge is the key to resolving the problems\\\\nof LLMs such as hallucination and outdated knowledge,\\\\nwhich can make LLMs generate more accurate and reliable\\\\nresponses through retrieval-augmented generation (RAG).\\\\nHowever, LLMs cannot always response as expected with\\\\nRAG. For one thing, there are numerous irrelevant docu-\\\\nments and false information on the Internet. Incorporating\\\\nthese external documents into LLMs could have a detrimen-\\\\ntal effect. For anthoer, LLMs suffer from the unreliable gen-\\\\neration challenge. The generation of LLMs is often unpre-\\\\ndictable, and we cannot guarantee that they will utilize the\\\\nuseful information entailed in the external documents. Ad-\\\\nditionally, LLMs can easily be misled by incorrect infor-\\\\nmation in the document. To this end, we build Retrieval-\\\\nAugmented Generation Benchmark (RGB) to evaluate the\\\\nretrieval-augmented generation of LLMs, and we concern\\\\nabout 4 specific abilities:\\\\nNoise Robustness is the robustness of LLMs in noisy\\\\ndocuments. As retrievers are not perfect, the external knowl-\\\\nedge they retrieve often contains a significant amount of\\\\nnoise, i.e., documents which are relevant to the question but\\\\ndo not contain any information about the answer. To effec-\\\\ntively answer user questions, LLMs must be able to extract\\\\nthe necessary information from documents despite there are\\\\nnoisy documents.\\\\nNegative Rejection is a measure of whether LLMs can\\\\ndecline to answer a question when none of the contexts pro-', metadata={'source': '..\\\\\\\\papers\\\\\\\\Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf', 'page': 2}), Document(page_content='defined as the fraction of the relevant documents for a given query qithat are successfully retrieved in a ranked list\\\\nR[21]. This metric is based on binary relevance judgments, assuming that documents are either relevant or not [21].\\\\nIn this paper, each chunk is considered a document and only the respective chunk diis considered relevant to the query\\\\nqi. While recall is easy to interpret, it does not consider the specific rank positions in which the relevant chunk appears\\\\ninR.\\\\nTo overcome this limitation, we introduce Reciprocal Rank (RR) into our analysis. In this metric, the rank of the\\\\nfirst relevant document to the query in Ris used to compute the RR score [21]. Therefore, Reciprocal Rank offers a\\\\nmore nuanced evaluation by assigning a higher value when the relevant chunk is returned in the early positions of our\\\\nretrievers given the respective query.\\\\nRecall and Reciprocal Rank were evaluated at a specific cutoff so the measures are presented as R@k and MRR@k.\\\\nFor each query, its results are evaluated and their mean serves as an aggregated measure of effectiveness of a given\\\\nretriever method. The retrievers are introduced below.\\\\nIn the category of sparse retrievers, we emphasize the BM25, a technique grounded in statistical weighting to assess\\\\nrelevance between search terms and documents. BM25 employs a scoring function that takes into account term\\\\nfrequency and document length, offering an efficient approach for retrieving pertinent information and is typically\\\\nused as a strong baseline. However, it is exact-match based and can be powerless when query and document are\\\\nrelevant to each other but has no common words.\\\\nOn the other hand, when exploring dense retrievers, we often encounter approaches based on the called bi-encoder\\\\ndesign [22]. The bi-encoder independently encodes queries and documents, creating separate vector representations', metadata={'source': '..\\\\\\\\papers\\\\\\\\THE CHRONICLES OF RAG THE RETRIEVER, THE CHUNK AND THE GENERATOR.pdf', 'page': 5})].\", 'This metacognition space primarily encompasses three main phases: (1) Monitoring; (2) Evaluating; (3) Planning.', 'Response Generation Finally, for response generation, Kim et al. suggest to use an autoregressive sequence generation model conditioned on the dialog context and the knowledge snippet selected in the previous subtask. Kim et al. propose to fine-tune a pre-trained GPT-2 model (Radford et al. 2019) for this task.', 'The HybridRAG framework is composed of four primary components: an augmentation coordinator (client), a memory-augmented model (client), a retriever model (cloud), and a memory generator based on Large Language Models (LLM) (cloud).', 'A general query is characterized by the necessity to retrieve and reason over multiple pieces of supporting evidence in order to furnish an answer. To put it differently, for a given multi-hop query denoted as q, the components within the retrieval set Rq collaboratively supply an answer to q.', 'Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, “Fermat” is the correct word, and REALM (row (c)) gives the word a much higher probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve documents to fill in the masked word even though it is trained with unsupervised text only.']\n",
      "BM25\n",
      "{'context_precision': 0.8722, 'context_recall': 1.0000}\n",
      "AVG accuracy 0.6\n"
     ]
    }
   ],
   "source": [
    "result_bm25_gpt4, acc_bm25_gpt4 = run_and_evaluate(retriever_bm25, prompt, llm_gpt4)\n",
    "print(\"BM25\")\n",
    "print(result_bm25_gpt4)\n",
    "print(\"AVG accuracy \"+ str(acc_bm25_gpt4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
