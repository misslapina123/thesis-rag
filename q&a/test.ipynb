{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain ragas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "dotenv.load_dotenv()\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from ragas import evaluate# 0.0.22\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\")\n",
    "OPENAI_DEPLOYMENT = os.environ.get(\"OPENAI_DEPLOYMENT\")\n",
    "EMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "EMBEDDING_DEPLOYMENT = os.environ.get(\"EMBEDDING_DEPLOYMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"vectordb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Who was Alexei Navalny supposed to be exchanged for in the planned prisoner swap, according to his colleague Maria Pevchikh?\",\n",
    "    \"What major issues with Boeing's safety culture were highlighted in the recent FAA report, and how is Boeing responding to the findings?\",\n",
    "    \"What financial challenges is Europe facing in sustaining support for Ukraine, and how are officials suggesting addressing these challenges in the next 12 months?\",\n",
    "    \"What is Sora AI, how does it work, and when can the general public expect to use it?\",\n",
    "    \"What are the potential key issues that could impact the 2024 U.S. presidential election between Joe Biden and Donald Trump, and how might they impact the outcome?\",\n",
    "    \"Why does Elon Musk want a bigger stake in Tesla, and what concerns does he express about the current structure of the company?\",\n",
    "    \"What are the shopping habits of Gen Alpha, and why are mature brands interested in targeting this demographic?\",\n",
    "    \"What is the main goal of Hamas in the Israel-Gaza conflict?\",\n",
    "    \"What has Kim Jong Un recently declared about North Korea's relationship with South Korea, and what actions has he taken that raise concerns about potential military conflict?\",\n",
    "    \"Why did Prince William pull out of the godfather's memorial service on 27.02.2024?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    [\"Alexei Navalny was supposed to be exchanged for Vadim Krasikov, a Russian hitman serving a life sentence for murder in Germany, as claimed by Maria Pevchikh, his colleague and chairwoman of his Anti-Corruption Foundation.\"],\n",
    "    [\"The FAA report on Boeing's safety culture found significant problems, including a 'disconnect' between senior management and employees, fear of retaliation when reporting safety concerns, confusion in safety reporting channels, and inadequate human factors in aviation safety. Boeing's response acknowledges the gaps, expressing commitment to learning from the findings, fostering a safety culture, and thoroughly reviewing the recommendations. The report identified 27 findings and 53 recommendations for improvement, which the FAA will review to ensure Boeing addresses them comprehensively.\"],\n",
    "    [\"Europe is grappling with the practicality of sustaining draining financial support for Ukraine amidst the deadlocked war and delayed aid. As spending on Ukraine faces political challenges, officials suggest exploring alternative funding sources, such as using frozen Russian assets to cover compensation costs. While some caution against setting a precedent, officials believe the EU's fundraising capabilities and potential arms deals, coupled with reduced reliance on the US, could help Europe continue supporting Ukraine in the coming year.\"],\n",
    "    [\"Sora AI is an artificial intelligence tool developed by OpenAI that can generate realistic videos up to 1 minute long based on textual prompts. It utilizes diffusion models, a method used in AI image generators, to create videos by understanding the association between images and accompanying alt text. As of now, Sora is not available to the general public; OpenAI is following a cautious approach by first testing it with a small group of 'red teamers' for potential harm or risks, followed by availability to visual artists, designers, and filmmakers before a potential public release, likely under a pay-to-use model similar to GPT. While Sora has shown significant advancements in AI video generation compared to previous attempts, it is not perfect, with occasional flaws and limitations seen in hand-picked videos released by OpenAI.\"],\n",
    "    [\"The potential key issues that could impact the 2024 U.S. presidential election between Joe Biden and Donald Trump include contrasting economic views, with Americans historically voting based on their economic well-being; the contentious topics of abortion and immigration, with Democrats focusing on abortion rights and Republicans emphasizing border security; and external factors like the Gaza War and its impact on Biden's support base. Other factors include crime rates, environmental concerns, and foreign policy. Additionally, the health and age of both candidates could be scrutinized, and the potential emergence of third-party candidates or independents might introduce unpredictability. Trump's legal challenges, facing 91 charges and four criminal trials, could influence public opinion, and the specter of the January 2021 Capitol attack may resonate differently among Republican and Democratic voters. Overall, these factors contribute to the complexity and uncertainty of the upcoming presidential race.\"],\n",
    "    [\"Elon Musk wants a bigger stake in Tesla to gain more control over the company's direction, especially regarding its investments in artificial intelligence (AI) features. He is concerned about Tesla's vulnerability to a 'takeover by dubious interests' and aims to have 25% voting control to ensure the company's leadership in AI and robotics. Musk suggests that without this control, he would prefer to develop products outside of Tesla.\"],\n",
    "    [\"Gen Alpha, born between 2010 and 2024, prefers to shop at adult brands like Lululemon, Sephora, Walmart, and Target, following the trends of their millennial parents. Adult brands are interested in targeting Gen Alpha due to their economic potential, with an estimated economic footprint of $5.46 trillion by 2029. These brands can secure the loyalty of the next generation by expanding their offerings to cater to the specific needs of Gen Alpha.\"],\n",
    "    [\"Hamas aims to create an Islamic state in place of Israel and rejects Israel's right to exist, justifying its attacks as responses to what it perceives as Israeli crimes against the Palestinian people. Additionally, Hamas seeks the release of Palestinian prisoners, an end to the blockade of the Gaza Strip, and the establishment of an independent Palestinian state.\"],\n",
    "    [\"Kim Jong Un recently declared that North Korea would no longer pursue reconciliation with South Korea, framing South Korea as the 'principal enemy.' He intensified nuclear threats, conducted missile tests, and abolished government agencies promoting cooperation and reunification. Analysts, including Robert L. Carlin and Siegfried Hecker, suggest he may be preparing for a military attack on South Korea.\"],\n",
    "    [\"Due to 'personal matter'.\"]\n",
    "]\n",
    "answers_llm = []\n",
    "contexts_llm = [[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_client = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDING_DEPLOYMENT,\n",
    "    openai_api_version=OPENAI_API_VERSION)\n",
    "llm = AzureChatOpenAI(model_name=OPENAI_MODEL, azure_deployment=OPENAI_DEPLOYMENT,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_llm(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  \n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_rag(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  # most likely\n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "        raise_exceptions=False,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General answers by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was Alexei Navalny supposed to be exchanged for in the planned prisoner swap, according to his colleague Maria Pevchikh?\n",
      "What major issues with Boeing's safety culture were highlighted in the recent FAA report, and how is Boeing responding to the findings?\n",
      "What financial challenges is Europe facing in sustaining support for Ukraine, and how are officials suggesting addressing these challenges in the next 12 months?\n",
      "What is Sora AI, how does it work, and when can the general public expect to use it?\n",
      "What are the potential key issues that could impact the 2024 U.S. presidential election between Joe Biden and Donald Trump, and how might they impact the outcome?\n",
      "Why does Elon Musk want a bigger stake in Tesla, and what concerns does he express about the current structure of the company?\n",
      "What are the shopping habits of Gen Alpha, and why are mature brands interested in targeting this demographic?\n",
      "What is the main goal of Hamas in the Israel-Gaza conflict?\n",
      "What has Kim Jong Un recently declared about North Korea's relationship with South Korea, and what actions has he taken that raise concerns about potential military conflict?\n",
      "Why did Prince William pull out of the godfather's memorial service on 27.02.2024?\n"
     ]
    }
   ],
   "source": [
    "for query in questions:\n",
    "    response = llm_chain.invoke({\"question\": query})\n",
    "    answers_llm.append(response[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 40/40 [00:11<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9583, 'answer_relevancy': 0.4750, 'answer_similarity': 0.9093, 'answer_correctness': 0.5995}\n"
     ]
    }
   ],
   "source": [
    "llm_results = evaluation_llm(questions, answers_llm, contexts_llm, ground_truths)\n",
    "print(llm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../news', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 4589, which is longer than the specified 4000\n",
      "Created a chunk of size 4082, which is longer than the specified 4000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "db_naive = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"q&a/vectordb/naive\")\n",
    "retriever = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"User input {question}. \n",
    "context {context}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Azure has not provided the response due to a content filter being triggeredon the following question: What is the main goal of Hamas in the Israel-Gaza conflict?\n"
     ]
    }
   ],
   "source": [
    "answers_naive = []\n",
    "contexts_naive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_augmented_qa_chain.invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_naive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_naive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_naive.append(\"No answer\")\n",
    "        context_full = retriever.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_naive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 1.0000, 'answer_relevancy': 0.7608, 'context_precision': 0.9500, 'context_recall': 1.0000, 'answer_similarity': 0.9232, 'answer_correctness': 0.6148}\n"
     ]
    }
   ],
   "source": [
    "result_naive_rag = evaluation_rag(questions, answers_naive, contexts_naive, ground_truths)\n",
    "print(result_naive_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = result.to_pandas()\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try recursive text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder()\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "db = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"q&a/vectordb/recursive_basic\")\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Azure has not provided the response due to a content filter being triggeredon the following question: What is the main goal of Hamas in the Israel-Gaza conflict?\n"
     ]
    }
   ],
   "source": [
    "answers_recursive = []\n",
    "contexts_recursive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_augmented_qa_chain.invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_recursive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_recursive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_recursive.append(\"No answer\")\n",
    "        context_full = retriever.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_recursive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:10<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 1.0000, 'answer_relevancy': 0.5571, 'context_precision': 1.0000, 'context_recall': 1.0000, 'answer_similarity': 0.9134, 'answer_correctness': 0.6099}\n"
     ]
    }
   ],
   "source": [
    "result_recursive = evaluation_rag(questions, answers_recursive, contexts_recursive, ground_truths)\n",
    "print(result_recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunk size change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_chunk_size(db):\n",
    "    retriever = db.as_retriever()\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    answers_recursive = []\n",
    "    contexts_recursive = []\n",
    "\n",
    "    for query in questions:\n",
    "        try:  \n",
    "            response = retrieval_augmented_qa_chain.invoke({\"question\": query})\n",
    "            # Access the response content\n",
    "            answers_recursive.append(response[\"response\"].content)\n",
    "            # Access the context content\n",
    "            context_content = [context.page_content for context in response[\"context\"]]\n",
    "            contexts_recursive.append(context_content)  \n",
    "        except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers_recursive.append(\"No answer\")\n",
    "            context_full = retriever.get_relevant_documents(query)\n",
    "            context_content = [context.page_content for context in context_full]\n",
    "            contexts_recursive.append(context_content)\n",
    "\n",
    "\n",
    "    result = evaluation_rag(questions, answers_recursive, contexts_recursive, ground_truths)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Azure has not provided the response due to a content filter being triggeredon the following question: What is the main goal of Hamas in the Israel-Gaza conflict?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  22%|██▏       | 13/60 [00:02<00:07,  6.36it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-650' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 2018, in aclose\n",
      "    await self._transport.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 385, in aclose\n",
      "    await self._pool.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 313, in aclose\n",
      "    await self._close_connections(closing_connections)\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 305, in _close_connections\n",
      "    await connection.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 171, in aclose\n",
      "    await self._connection.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 265, in aclose\n",
      "    await self._network_stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 54, in aclose\n",
      "    await self._stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 202, in aclose\n",
      "    await self.transport_stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1191, in aclose\n",
      "    self._transport.close()\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\selector_events.py\", line 860, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 761, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 519, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-651' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 2018, in aclose\n",
      "    await self._transport.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 385, in aclose\n",
      "    await self._pool.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 313, in aclose\n",
      "    await self._close_connections(closing_connections)\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 305, in _close_connections\n",
      "    await connection.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 171, in aclose\n",
      "    await self._connection.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 265, in aclose\n",
      "    await self._network_stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 54, in aclose\n",
      "    await self._stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 202, in aclose\n",
      "    await self.transport_stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1191, in aclose\n",
      "    self._transport.close()\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\selector_events.py\", line 860, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 761, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 519, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-652' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 2018, in aclose\n",
      "    await self._transport.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 385, in aclose\n",
      "    await self._pool.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 313, in aclose\n",
      "    await self._close_connections(closing_connections)\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 305, in _close_connections\n",
      "    await connection.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 171, in aclose\n",
      "    await self._connection.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 265, in aclose\n",
      "    await self._network_stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 54, in aclose\n",
      "    await self._stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 202, in aclose\n",
      "    await self.transport_stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1191, in aclose\n",
      "    self._transport.close()\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\selector_events.py\", line 860, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 761, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 519, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-653' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 2018, in aclose\n",
      "    await self._transport.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 385, in aclose\n",
      "    await self._pool.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 313, in aclose\n",
      "    await self._close_connections(closing_connections)\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 305, in _close_connections\n",
      "    await connection.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 171, in aclose\n",
      "    await self._connection.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py\", line 265, in aclose\n",
      "    await self._network_stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 54, in aclose\n",
      "    await self._stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 202, in aclose\n",
      "    await self.transport_stream.aclose()\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1191, in aclose\n",
      "    self._transport.close()\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\selector_events.py\", line 860, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 761, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 519, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000\n",
      "{'faithfulness': 0.9333, 'answer_relevancy': 0.7054, 'context_precision': 0.9500, 'context_recall': 0.9750, 'answer_similarity': 0.9271, 'answer_correctness': 0.6078}\n"
     ]
    }
   ],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 100)\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "# print(len(chunks))\n",
    "# db = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"q&a/vectordb/recursive_1000\")\n",
    "result_1000 = change_chunk_size(db)\n",
    "print(\"CHUNK SIZE 1000\")\n",
    "print(result_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "Warning: Azure has not provided the response due to a content filter being triggeredon the following question: What is the main goal of Hamas in the Israel-Gaza conflict?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:10<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 500\n",
      "{'faithfulness': 0.9125, 'answer_relevancy': 0.8386, 'context_precision': 0.9583, 'context_recall': 1.0000, 'answer_similarity': 0.9290, 'answer_correctness': 0.6328}\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500, chunk_overlap = 50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(len(chunks))\n",
    "db = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"q&a/vectordb/recursive_500\")\n",
    "result_500 = change_chunk_size(db)\n",
    "print(\"CHUNK SIZE 500\")\n",
    "print(result_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Warning: Azure has not provided the response due to a content filter being triggeredon the following question: What is the main goal of Hamas in the Israel-Gaza conflict?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:10<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 2000\n",
      "{'faithfulness': 0.9792, 'answer_relevancy': 0.7226, 'context_precision': 1.0000, 'context_recall': 1.0000, 'answer_similarity': 0.9125, 'answer_correctness': 0.6143}\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 2000, chunk_overlap = 200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(len(chunks))\n",
    "db = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"q&a/vectordb/recursive_2000\")\n",
    "result_2000 = change_chunk_size(db)\n",
    "print(\"CHUNK SIZE 2000\")\n",
    "print(result_2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now time to look for different top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
