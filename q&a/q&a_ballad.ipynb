{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "dotenv.load_dotenv()\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\")\n",
    "OPENAI_DEPLOYMENT = os.environ.get(\"OPENAI_DEPLOYMENT\")\n",
    "EMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "EMBEDDING_DEPLOYMENT = os.environ.get(\"EMBEDDING_DEPLOYMENT\")\n",
    "OPENAI_MODEL_GPT4 = os.environ.get(\"OPENAI_MODEL_GPT4\")\n",
    "OPENAI_DEPLOYMENT_GPT4 = os.environ.get(\"OPENAI_DEPLOYMENT_GPT4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Who wrote 'The Hanging Tree' song?\",\n",
    "    \"Which District was the source of the Snow family's wealth before the war?\",\n",
    "    \"Which district did the Plinth family call home before moving to the Capitol?\",\n",
    "    \"What is Lucy Gray wearing when she first appears in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"Where are the tributes sent immediately after they arrive in the Capitol in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"What does Coriolanus do to keep Lucy Gray safe when he realizes Dr. Gaul is putting her hybrid snakes in the arena in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"What does Coriolanus do that causes Sejanus's execution in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"What is one of the main reasons Coriolanus is chosen to sing the national anthem at Arachne's funeral in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"Who arrives at the District 12 Peacekeeper base soon after Coriolanus in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"Where did the jabberjays originate in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"Why does Coriolanus feel he needs to kill Lucy Gray?\",\n",
    "    \"What does Coriolanus do to keep Lucy Gray safe when he realizes Dr. Gaul is putting her hybrid snakes in the arena?\",\n",
    "    \"Coriolanus regularly receives a box from Mrs. Plinth in 'The Ballad of Songbirds and Snakes'. What is in the box?\",\n",
    "    \"What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"What does Coriolanus wear in his lapel at the interview to remind everyone that Lucy Gray 'belongs to' him in 'The Ballad of Songbirds and Snakes'?\"\n",
    "]\n",
    "ground_truths = [\n",
    "    [\"Lucy Gray Baird from District 12.\"],\n",
    "    [\"District 13.\"],\n",
    "    [\"District 2.\"],\n",
    "    [\"A ruffled dress in rainbow colors.\"],\n",
    "    [\"The monkey house in the zoo.\"],\n",
    "    [\"He drops a handkerchief with Lucy Gray's scent into the snake tank.\"],\n",
    "    [\"He uses a jabberjay to record Sejanus talking about his part in a rebel plan.\"],\n",
    "    [\"He knows all the words.\"],\n",
    "    [\"Sejanus.\"],\n",
    "    [\"In Dr. Gaul's lab.\"],\n",
    "    [\"She can tie him to Mayfair's murder.\"],\n",
    "    [\"Drops a handkerchief with Lucy Gray's scent into the snake tank.\"],\n",
    "    [\"Baked goods.\"],\n",
    "    [\"Breadcrumbs so he will have food to eat during his journey.\"],\n",
    "    [\"A rose that matches the one in her hair.\"]\n",
    "]\n",
    "answers_llm = []\n",
    "contexts_llm = [[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_client = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDING_DEPLOYMENT,\n",
    "    openai_api_version=OPENAI_API_VERSION)\n",
    "llm = AzureChatOpenAI(model_name=OPENAI_MODEL, azure_deployment=OPENAI_DEPLOYMENT,temperature=0)\n",
    "llm_gpt4 = AzureChatOpenAI(model_name=OPENAI_MODEL_GPT4, azure_deployment=OPENAI_DEPLOYMENT_GPT4,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_llm(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  \n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_rag(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  # most likely\n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "        raise_exceptions=False,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"System\", \"Faithfulness\", \"Answer Relevancy\", \"Context Precision\", \"Context Recall\", \"Answer Similarity\", \"Answer Correctness\"]\n",
    "results_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_average = 0\n",
    "def find_highest(average_score):\n",
    "    global max_average\n",
    "    if average_score > max_average:\n",
    "        max_average = average_score\n",
    "        print(\"This is the new best value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary(result):\n",
    "    dict_result = dict(result)\n",
    "    average_score = sum(dict_result.values()) / len(dict_result)\n",
    "    print(f\"The average score is: {average_score}\")\n",
    "    find_highest(average_score)\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(system_name, questions, answers, contexts, ground_truths):\n",
    "    result = evaluation_rag(questions, answers, contexts, ground_truths)\n",
    "    average = dictionary(result)\n",
    "    # Create a dictionary to store the results\n",
    "    system_results = {\n",
    "        \"System\": system_name,\n",
    "        \"Faithfulness\": result[\"faithfulness\"],\n",
    "        \"Answer Relevancy\": result[\"answer_relevancy\"],\n",
    "        \"Context Precision\": result[\"context_precision\"],\n",
    "        \"Context Recall\": result[\"context_recall\"],\n",
    "        \"Answer Similarity\": result[\"answer_similarity\"],\n",
    "        \"Answer Correctness\": result[\"answer_correctness\"],\n",
    "        \"Average\": average\n",
    "    }\n",
    "    df_system_results = pd.DataFrame([system_results])\n",
    "    return df_system_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LLM(system_name, questions, answers, contexts, ground_truths):\n",
    "    result = evaluation_rag(questions, answers, contexts, ground_truths)\n",
    "    average = dictionary(result)\n",
    "    # Create a dictionary to store the results\n",
    "    system_results = {\n",
    "        \"System\": system_name,\n",
    "        \"Faithfulness\": result[\"faithfulness\"],\n",
    "        \"Answer Relevancy\": result[\"answer_relevancy\"],\n",
    "        \"Context Precision\": np.nan,\n",
    "        \"Context Recall\": np.nan,\n",
    "        \"Answer Similarity\": result[\"answer_similarity\"],\n",
    "        \"Answer Correctness\": result[\"answer_correctness\"],\n",
    "        \"Average\": average\n",
    "    }\n",
    "    df_llm_results = pd.DataFrame([system_results])\n",
    "    return df_llm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General answers by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Write a concise answer to the following question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm}\n",
    ")\n",
    "llm_chain_gpt4 =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm_gpt4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in questions:\n",
    "    response = llm_chain.invoke({\"question\": query})\n",
    "    answers_llm.append(response[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 90/90 [00:06<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.3905568657193135\n",
      "This is the new best value!\n",
      "    System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0  GPT-3.5      0.083333          0.810294                NaN             NaN   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.793104            0.256609  0.390557  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sigitalapina\\AppData\\Local\\Temp\\ipykernel_16008\\800954320.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, llm_results], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "llm_results = evaluate_LLM(\"GPT-3.5\", questions, answers_llm, contexts_llm, ground_truths)\n",
    "results_df = pd.concat([results_df, llm_results], ignore_index=True)\n",
    "print(llm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 90/90 [00:06<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5072359313320577\n",
      "This is the new best value!\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0  GPT-4      0.666667          0.887922                NaN             NaN   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.801728            0.287099  0.507236  \n"
     ]
    }
   ],
   "source": [
    "answers_llm_gpt4 = []\n",
    "for query in questions:\n",
    "    response = llm_chain_gpt4.invoke({\"question\": query})\n",
    "    answers_llm_gpt4.append(response[\"response\"].content)\n",
    "llm_results_gpt4 = evaluate_LLM(\"GPT-4\",questions, answers_llm_gpt4, contexts_llm, ground_truths)\n",
    "results_df = pd.concat([results_df, llm_results_gpt4], ignore_index=True)\n",
    "print(llm_results_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.810294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.793104</td>\n",
       "      <td>0.256609</td>\n",
       "      <td>0.390557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.887922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.801728</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.507236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
       "0  GPT-3.5      0.083333          0.810294                NaN             NaN   \n",
       "1    GPT-4      0.666667          0.887922                NaN             NaN   \n",
       "\n",
       "   Answer Similarity  Answer Correctness   Average  \n",
       "0           0.793104            0.256609  0.390557  \n",
       "1           0.801728            0.287099  0.507236  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"..\\ballad\\the_ballad_of_songbirds_and_snakes.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "db_naive = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"../ballad/vectordb-edit/naive\")\n",
    "db_naive.persist()\n",
    "retriever_naive = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"User input {question}. \n",
    "Context {context}.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_chain(prompt, retriever, llm):\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    return retrieval_augmented_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_naive = []\n",
    "contexts_naive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_naive, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_naive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_naive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_naive.append(\"No answer\")\n",
    "        context_full = retriever_naive.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_naive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  84%|████████▍ | 76/90 [00:07<00:00, 19.53it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:13<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5548861942589303\n",
      "This is the new best value!\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0  Naive      0.558016          0.659556           0.787525        0.514306   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.280777            0.529137  0.554886  \n"
     ]
    }
   ],
   "source": [
    "result_naive_rag = evaluate_system(\"Naive\", questions, answers_naive, contexts_naive, ground_truths)\n",
    "results_df = pd.concat([results_df, result_naive_rag], ignore_index=True)\n",
    "print(result_naive_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try recursive text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder()\n",
    "chunks_r = text_splitter.split_documents(documents)\n",
    "db_basic = Chroma.from_documents(chunks_r, embeddings_client, persist_directory = \"../ballad/vectordb-edit/recursive_basic\")\n",
    "db_basic.persist()\n",
    "retriever_basic = db_basic.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_recursive = []\n",
    "contexts_recursive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_basic, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_recursive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_recursive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_recursive.append(\"No answer\")\n",
    "        context_full = retriever_basic.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_recursive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:14<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.531828253100994\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0.744159</td>\n",
       "      <td>0.721819</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.67437</td>\n",
       "      <td>0.531828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "0  Recursive      0.379104          0.744159           0.721819   \n",
       "\n",
       "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "0        0.383333           0.288185             0.67437  0.531828  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_recursive = evaluate_system(\"Recursive\", questions, answers_naive, contexts_naive, ground_truths)\n",
    "results_df = pd.concat([results_df, result_recursive], ignore_index=True)\n",
    "result_recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunk size change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_evaluate(name, retriever, prompt, llm, results_df):\n",
    "    answers = []\n",
    "    contexts_extra = []\n",
    "\n",
    "    for query in questions:\n",
    "        try:  \n",
    "            response = retrieval_chain(prompt, retriever, llm).invoke({\"question\": query})\n",
    "            # Access the response content\n",
    "            answers.append(response[\"response\"].content)\n",
    "            # Access the context content\n",
    "            context_content = [context.page_content for context in response[\"context\"]]\n",
    "            contexts_extra.append(context_content)  \n",
    "        except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers.append(\"No answer\")\n",
    "            context_full = retriever.get_relevant_documents(query)\n",
    "            context_content = [context.page_content for context in context_full]\n",
    "            contexts_extra.append(context_content)\n",
    "\n",
    "    result = evaluate_system(name, questions, answers, contexts_extra, ground_truths)\n",
    "    results_df = pd.concat([results_df, result], ignore_index=True)\n",
    "    return result, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks for chunk size 500, overlap 0%: 735\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   1%|          | 1/90 [00:01<02:31,  1.70s/it]Task exception was never retrieved\n",
      "future: <Task finished name='Task-585' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-586' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-587' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating:   3%|▎         | 3/90 [00:05<02:08,  1.48s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  81%|████████  | 73/90 [00:06<00:00, 37.61it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  99%|█████████▉| 89/90 [00:29<00:00,  5.51it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [02:03<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.46003883362226006\n",
      "CHUNK SIZE 500, 0% overlap:\n",
      "                  System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 0%      0.517857          0.088874            0.78231   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.277026           0.513533            0.580632  0.460039  \n",
      "Number of chunks for chunk size 500, overlap 5%: 735\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   2%|▏         | 2/90 [00:04<03:47,  2.58s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  81%|████████  | 73/90 [00:06<00:00, 25.35it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  92%|█████████▏| 83/90 [00:22<00:01,  6.26it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [02:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5292776333401444\n",
      "CHUNK SIZE 500, 5% overlap:\n",
      "                  System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 5%      0.565476          0.218776           0.801542   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.346715           0.585408            0.657749  0.529278  \n",
      "Number of chunks for chunk size 500, overlap 10%: 735\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-863' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-864' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-865' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-866' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   2%|▏         | 2/90 [00:04<03:41,  2.52s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  98%|█████████▊| 88/90 [00:11<00:00,  6.14it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  98%|█████████▊| 88/90 [00:29<00:00,  6.14it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [03:42<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5695708844336719\n",
      "This is the new best value!\n",
      "CHUNK SIZE 500, 10% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 10%      0.652778          0.230681           0.793275   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.326791           0.746519            0.667383  0.569571  \n",
      "Number of chunks for chunk size 500, overlap 15%: 735\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   2%|▏         | 2/90 [00:04<03:44,  2.55s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  77%|███████▋  | 69/90 [00:11<00:04,  5.15it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  99%|█████████▉| 89/90 [00:54<00:00,  1.28it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [01:27<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5361153121892916\n",
      "CHUNK SIZE 500, 15% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 15%      0.841746          0.535714           0.107143   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.812384           0.341578            0.578127  0.536115  \n",
      "Number of chunks for chunk size 500, overlap 20%: 735\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   7%|▋         | 6/90 [00:05<01:08,  1.22it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  21%|██        | 19/90 [00:06<00:16,  4.41it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  93%|█████████▎| 84/90 [00:29<00:00,  6.13it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [03:06<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.4721818772509434\n",
      "CHUNK SIZE 500, 20% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 20%       0.63609          0.486717           0.237498   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.684846           0.471942            0.315999  0.472182  \n",
      "Number of chunks for chunk size 1000, overlap 0%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:   1%|          | 1/90 [00:01<02:48,  1.89s/it]Task exception was never retrieved\n",
      "future: <Task finished name='Task-1412' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1413' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1414' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1415' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating: 100%|██████████| 90/90 [00:13<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5481516604206526\n",
      "CHUNK SIZE 1000, 0% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 0%      0.364984          0.852743           0.731757   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0         0.37963           0.288185            0.671611  0.548152  \n",
      "Number of chunks for chunk size 1000, overlap 5%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   6%|▌         | 5/90 [00:05<01:35,  1.13s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:14<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5067525562246075\n",
      "CHUNK SIZE 1000, 5% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 5%       0.39897          0.648575           0.641253   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.398148           0.383179            0.570392  0.506753  \n",
      "Number of chunks for chunk size 1000, overlap 10%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-1614' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1615' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1616' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1617' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:14<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.533673776620536\n",
      "CHUNK SIZE 1000, 10% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 10%      0.362868           0.77005           0.729893   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0         0.37963           0.288185            0.671417  0.533674  \n",
      "Number of chunks for chunk size 1000, overlap 15%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  80%|████████  | 72/90 [00:06<00:00, 32.76it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:11<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5529962579810227\n",
      "CHUNK SIZE 1000, 15% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 15%      0.544835          0.650798           0.793733   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.519767           0.277074            0.531771  0.552996  \n",
      "Number of chunks for chunk size 1000, overlap 20%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  87%|████████▋ | 78/90 [00:11<00:01,  6.30it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 90/90 [00:13<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5154238086290909\n",
      "CHUNK SIZE 1000, 20% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 20%      0.642857          0.741209           0.412963   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.233333           0.795649            0.266531  0.515424  \n",
      "Number of chunks for chunk size 2000, overlap 0%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:11<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5302768916054704\n",
      "CHUNK SIZE 2000, 0% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 0%      0.346149          0.829901           0.662909   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0         0.37963           0.288185            0.674888  0.530277  \n",
      "Number of chunks for chunk size 2000, overlap 5%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 90/90 [00:15<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5411518162008683\n",
      "CHUNK SIZE 2000, 5% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 5%      0.686905          0.817278           0.407407   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.3           0.790828            0.244493  0.541152  \n",
      "Number of chunks for chunk size 2000, overlap 10%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  44%|████▍     | 40/90 [00:06<00:03, 14.37it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 90/90 [00:12<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5286489780317086\n",
      "CHUNK SIZE 2000, 10% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 10%      0.665873          0.815407           0.374074   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.3           0.787232            0.229308  0.528649  \n",
      "Number of chunks for chunk size 2000, overlap 15%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  79%|███████▉  | 71/90 [00:10<00:01,  9.62it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 90/90 [00:13<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5531542398095859\n",
      "CHUNK SIZE 2000, 15% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 15%      0.739286          0.801774            0.37963   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.3           0.791637              0.3066  0.553154  \n",
      "Number of chunks for chunk size 2000, overlap 20%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   6%|▌         | 5/90 [00:02<00:30,  2.79it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  16%|█▌        | 14/90 [00:05<00:30,  2.49it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:13<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5719441699627786\n",
      "This is the new best value!\n",
      "CHUNK SIZE 2000, 20% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 20%       0.75821          0.526721           0.722255   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.730503           0.456451            0.237525  0.571944  \n",
      "Number of chunks for chunk size 3000, overlap 0%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:14<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5098492236396049\n",
      "CHUNK SIZE 3000, 0% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 0%      0.336367          0.707624           0.672505   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0         0.37963           0.288185            0.674785  0.509849  \n",
      "Number of chunks for chunk size 3000, overlap 5%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:14<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5200598611406061\n",
      "CHUNK SIZE 3000, 5% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 5%      0.336423          0.794321           0.669045   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness  Average  \n",
      "0        0.357407           0.288185            0.674978  0.52006  \n",
      "Number of chunks for chunk size 3000, overlap 10%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:19<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5302058967814511\n",
      "CHUNK SIZE 3000, 10% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 10%      0.387645          0.713186           0.734147   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0         0.37963           0.291758             0.67487  0.530206  \n",
      "Number of chunks for chunk size 3000, overlap 15%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  93%|█████████▎| 84/90 [00:12<00:00, 18.09it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 90/90 [00:16<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5504475406090757\n",
      "CHUNK SIZE 3000, 15% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 15%       0.78836          0.741365           0.374074   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.3           0.792157             0.30673  0.550448  \n",
      "Number of chunks for chunk size 3000, overlap 20%: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  89%|████████▉ | 80/90 [00:22<00:04,  2.35it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 90/90 [00:23<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5278333436218005\n",
      "CHUNK SIZE 3000, 20% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 20%       0.80754          0.677429           0.357407   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.266667           0.793699            0.264258  0.527833  \n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [500, 1000, 2000, 3000]\n",
    "overlap_percentages = [0, 5, 10, 15, 20]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    for overlap_percentage in overlap_percentages:\n",
    "        # Calculate overlap based on percentage\n",
    "        chunk_overlap = int(chunk_size * overlap_percentage / 100)\n",
    "        \n",
    "        # Create text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        \n",
    "        # Split documents\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Print number of chunks\n",
    "        print(f\"Number of chunks for chunk size {chunk_size}, overlap {overlap_percentage}%: {len(chunks)}\")\n",
    "        \n",
    "        # Create Chroma database\n",
    "        db = Chroma.from_documents(chunks, embeddings_client, persist_directory=f\"../ballad/vectordb-edit/chunking_{chunk_size}_{overlap_percentage}\")\n",
    "        db.persist()\n",
    "        \n",
    "        # Create retriever\n",
    "        retriever = db.as_retriever()\n",
    "        \n",
    "        # Run and evaluate\n",
    "        result,results_df = run_and_evaluate(f\"Chunk {chunk_size}, overlap {overlap_percentage}%\", retriever, prompt, llm, results_df)\n",
    "        print(f\"CHUNK SIZE {chunk_size}, {overlap_percentage}% overlap:\")\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 100)\n",
    "# chunks_1000 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_1000))\n",
    "# db_1000 = Chroma.from_documents(chunks_1000, embeddings_client, persist_directory = \"../ballad/vectordb-edit/recursive_1000\")\n",
    "# db_1000.persist()\n",
    "# retriever_1000 = db_1000.as_retriever()\n",
    "# result_1000 = change_chunk_size(retriever_1000)\n",
    "# print(\"CHUNK SIZE 1000\")\n",
    "# print(result_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500, chunk_overlap = 50)\n",
    "# chunks_500 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_500))\n",
    "# db_500 = Chroma.from_documents(chunks_500, embeddings_client, persist_directory = \"../ballad/vectordb-edit/recursive_500\")\n",
    "# db_500.persist()\n",
    "# retriever_500 = db_500.as_retriever()\n",
    "# result_500 = change_chunk_size(retriever_500)\n",
    "# print(\"CHUNK SIZE 500\")\n",
    "# print(result_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 2000, chunk_overlap = 200)\n",
    "# chunks_2000 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_2000))\n",
    "# db_2000 = Chroma.from_documents(chunks_2000, embeddings_client, persist_directory = \"../ballad/vectordb-edit/recursive_2000\")\n",
    "# db_2000.persist()\n",
    "# retriever_2000 = db_2000.as_retriever()\n",
    "# result_2000 = change_chunk_size(retriever_2000)\n",
    "# print(\"CHUNK SIZE 2000\")\n",
    "# print(result_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 3000, chunk_overlap = 300)\n",
    "# chunks_3000 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_3000))\n",
    "# db_3000 = Chroma.from_documents(chunks_3000, embeddings_client, persist_directory = \"../ballad/vectordb-edit/recursive_3000\")\n",
    "# db_3000.persist()\n",
    "# retriever_3000 = db_3000.as_retriever()\n",
    "# result_3000 = change_chunk_size(retriever_3000)\n",
    "# print(\"CHUNK SIZE 3000\")\n",
    "# print(result_3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.810294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.793104</td>\n",
       "      <td>0.256609</td>\n",
       "      <td>0.390557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.887922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.801728</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.507236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.558016</td>\n",
       "      <td>0.659556</td>\n",
       "      <td>0.787525</td>\n",
       "      <td>0.514306</td>\n",
       "      <td>0.280777</td>\n",
       "      <td>0.529137</td>\n",
       "      <td>0.554886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0.744159</td>\n",
       "      <td>0.721819</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674370</td>\n",
       "      <td>0.531828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.088874</td>\n",
       "      <td>0.782310</td>\n",
       "      <td>0.277026</td>\n",
       "      <td>0.513533</td>\n",
       "      <td>0.580632</td>\n",
       "      <td>0.460039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.565476</td>\n",
       "      <td>0.218776</td>\n",
       "      <td>0.801542</td>\n",
       "      <td>0.346715</td>\n",
       "      <td>0.585408</td>\n",
       "      <td>0.657749</td>\n",
       "      <td>0.529278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.230681</td>\n",
       "      <td>0.793275</td>\n",
       "      <td>0.326791</td>\n",
       "      <td>0.746519</td>\n",
       "      <td>0.667383</td>\n",
       "      <td>0.569571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.841746</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.812384</td>\n",
       "      <td>0.341578</td>\n",
       "      <td>0.578127</td>\n",
       "      <td>0.536115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.636090</td>\n",
       "      <td>0.486717</td>\n",
       "      <td>0.237498</td>\n",
       "      <td>0.684846</td>\n",
       "      <td>0.471942</td>\n",
       "      <td>0.315999</td>\n",
       "      <td>0.472182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>0.364984</td>\n",
       "      <td>0.852743</td>\n",
       "      <td>0.731757</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671611</td>\n",
       "      <td>0.548152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>0.398970</td>\n",
       "      <td>0.648575</td>\n",
       "      <td>0.641253</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.383179</td>\n",
       "      <td>0.570392</td>\n",
       "      <td>0.506753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>0.362868</td>\n",
       "      <td>0.770050</td>\n",
       "      <td>0.729893</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671417</td>\n",
       "      <td>0.533674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>0.544835</td>\n",
       "      <td>0.650798</td>\n",
       "      <td>0.793733</td>\n",
       "      <td>0.519767</td>\n",
       "      <td>0.277074</td>\n",
       "      <td>0.531771</td>\n",
       "      <td>0.552996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.741209</td>\n",
       "      <td>0.412963</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.795649</td>\n",
       "      <td>0.266531</td>\n",
       "      <td>0.515424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.346149</td>\n",
       "      <td>0.829901</td>\n",
       "      <td>0.662909</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674888</td>\n",
       "      <td>0.530277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>0.686905</td>\n",
       "      <td>0.817278</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.790828</td>\n",
       "      <td>0.244493</td>\n",
       "      <td>0.541152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.665873</td>\n",
       "      <td>0.815407</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.787232</td>\n",
       "      <td>0.229308</td>\n",
       "      <td>0.528649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.739286</td>\n",
       "      <td>0.801774</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.791637</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.553154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.758210</td>\n",
       "      <td>0.526721</td>\n",
       "      <td>0.722255</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>0.456451</td>\n",
       "      <td>0.237525</td>\n",
       "      <td>0.571944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.336367</td>\n",
       "      <td>0.707624</td>\n",
       "      <td>0.672505</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674785</td>\n",
       "      <td>0.509849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.336423</td>\n",
       "      <td>0.794321</td>\n",
       "      <td>0.669045</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674978</td>\n",
       "      <td>0.520060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.387645</td>\n",
       "      <td>0.713186</td>\n",
       "      <td>0.734147</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.291758</td>\n",
       "      <td>0.674870</td>\n",
       "      <td>0.530206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.788360</td>\n",
       "      <td>0.741365</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.306730</td>\n",
       "      <td>0.550448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.807540</td>\n",
       "      <td>0.677429</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.793699</td>\n",
       "      <td>0.264258</td>\n",
       "      <td>0.527833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     System  Faithfulness  Answer Relevancy  \\\n",
       "0                   GPT-3.5      0.083333          0.810294   \n",
       "1                     GPT-4      0.666667          0.887922   \n",
       "2                     Naive      0.558016          0.659556   \n",
       "3                 Recursive      0.379104          0.744159   \n",
       "4     Chunk 500, overlap 0%      0.517857          0.088874   \n",
       "5     Chunk 500, overlap 5%      0.565476          0.218776   \n",
       "6    Chunk 500, overlap 10%      0.652778          0.230681   \n",
       "7    Chunk 500, overlap 15%      0.841746          0.535714   \n",
       "8    Chunk 500, overlap 20%      0.636090          0.486717   \n",
       "9    Chunk 1000, overlap 0%      0.364984          0.852743   \n",
       "10   Chunk 1000, overlap 5%      0.398970          0.648575   \n",
       "11  Chunk 1000, overlap 10%      0.362868          0.770050   \n",
       "12  Chunk 1000, overlap 15%      0.544835          0.650798   \n",
       "13  Chunk 1000, overlap 20%      0.642857          0.741209   \n",
       "14   Chunk 2000, overlap 0%      0.346149          0.829901   \n",
       "15   Chunk 2000, overlap 5%      0.686905          0.817278   \n",
       "16  Chunk 2000, overlap 10%      0.665873          0.815407   \n",
       "17  Chunk 2000, overlap 15%      0.739286          0.801774   \n",
       "18  Chunk 2000, overlap 20%      0.758210          0.526721   \n",
       "19   Chunk 3000, overlap 0%      0.336367          0.707624   \n",
       "20   Chunk 3000, overlap 5%      0.336423          0.794321   \n",
       "21  Chunk 3000, overlap 10%      0.387645          0.713186   \n",
       "22  Chunk 3000, overlap 15%      0.788360          0.741365   \n",
       "23  Chunk 3000, overlap 20%      0.807540          0.677429   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.793104            0.256609   \n",
       "1                 NaN             NaN           0.801728            0.287099   \n",
       "2            0.787525        0.514306           0.280777            0.529137   \n",
       "3            0.721819        0.383333           0.288185            0.674370   \n",
       "4            0.782310        0.277026           0.513533            0.580632   \n",
       "5            0.801542        0.346715           0.585408            0.657749   \n",
       "6            0.793275        0.326791           0.746519            0.667383   \n",
       "7            0.107143        0.812384           0.341578            0.578127   \n",
       "8            0.237498        0.684846           0.471942            0.315999   \n",
       "9            0.731757        0.379630           0.288185            0.671611   \n",
       "10           0.641253        0.398148           0.383179            0.570392   \n",
       "11           0.729893        0.379630           0.288185            0.671417   \n",
       "12           0.793733        0.519767           0.277074            0.531771   \n",
       "13           0.412963        0.233333           0.795649            0.266531   \n",
       "14           0.662909        0.379630           0.288185            0.674888   \n",
       "15           0.407407        0.300000           0.790828            0.244493   \n",
       "16           0.374074        0.300000           0.787232            0.229308   \n",
       "17           0.379630        0.300000           0.791637            0.306600   \n",
       "18           0.722255        0.730503           0.456451            0.237525   \n",
       "19           0.672505        0.379630           0.288185            0.674785   \n",
       "20           0.669045        0.357407           0.288185            0.674978   \n",
       "21           0.734147        0.379630           0.291758            0.674870   \n",
       "22           0.374074        0.300000           0.792157            0.306730   \n",
       "23           0.357407        0.266667           0.793699            0.264258   \n",
       "\n",
       "     Average  \n",
       "0   0.390557  \n",
       "1   0.507236  \n",
       "2   0.554886  \n",
       "3   0.531828  \n",
       "4   0.460039  \n",
       "5   0.529278  \n",
       "6   0.569571  \n",
       "7   0.536115  \n",
       "8   0.472182  \n",
       "9   0.548152  \n",
       "10  0.506753  \n",
       "11  0.533674  \n",
       "12  0.552996  \n",
       "13  0.515424  \n",
       "14  0.530277  \n",
       "15  0.541152  \n",
       "16  0.528649  \n",
       "17  0.553154  \n",
       "18  0.571944  \n",
       "19  0.509849  \n",
       "20  0.520060  \n",
       "21  0.530206  \n",
       "22  0.550448  \n",
       "23  0.527833  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest average value: 0.5719441699627786\n"
     ]
    }
   ],
   "source": [
    "highest_average = results_df[\"Average\"].max()\n",
    "print(\"Highest average value:\", highest_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"../ballad/results/results_qa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now time to look for different top-k\n",
    "\n",
    "Note: We continue with the size chunk of 500 as it had the highest average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_k = Chroma(persist_directory = \"../ballad/vectordb-edit/chunking_2000_20\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Why does Coriolanus feel he needs to kill Lucy Gray?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   1%|          | 1/90 [00:04<06:56,  4.68s/it]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating:  79%|███████▉  | 71/90 [00:05<00:00, 22.18it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 90/90 [00:06<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.41005265498550814\n",
      "Results for K=2:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 3000, overlap 20%, K=2       0.41197          0.282099   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.579457        0.266667           0.326902            0.593221   \n",
      "\n",
      "    Average  \n",
      "0  0.410053  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-3487' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3488' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3489' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3490' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:14<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.4294029879648697\n",
      "Results for K=3:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 3000, overlap 20%, K=3      0.261245          0.443421   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.594656        0.327778           0.273538             0.67578   \n",
      "\n",
      "    Average  \n",
      "0  0.429403  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 90/90 [00:12<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.519104684815868\n",
      "Results for K=5:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 3000, overlap 20%, K=5      0.740741          0.806737   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.395926        0.116667           0.793101            0.261457   \n",
      "\n",
      "    Average  \n",
      "0  0.519105  \n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}on the following question: Who wrote 'The Hanging Tree' song?\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-3749' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3750' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3751' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-3752' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:   1%|          | 1/90 [00:00<00:23,  3.79it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   3%|▎         | 3/90 [00:02<01:04,  1.36it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:   7%|▋         | 6/90 [00:04<01:11,  1.17it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  20%|██        | 18/90 [00:05<00:10,  6.89it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating:  72%|███████▏  | 65/90 [00:11<00:06,  4.03it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  81%|████████  | 73/90 [00:11<00:02,  5.79it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  98%|█████████▊| 88/90 [00:31<00:00,  6.99it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  99%|█████████▉| 89/90 [02:03<00:08,  8.44s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [02:14<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5558102541492808\n",
      "Results for K=6:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 3000, overlap 20%, K=6      0.447991          0.224892   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.821103        0.390395           0.701033            0.749447   \n",
      "\n",
      "   Average  \n",
      "0  0.55581  \n"
     ]
    }
   ],
   "source": [
    "k_values = [2, 3, 5, 6]\n",
    "\n",
    "# Iterate over different k values\n",
    "for k in k_values:\n",
    "    # Create retriever with k value\n",
    "    retriever = db_k.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    # Run and evaluate\n",
    "    result,results_df = run_and_evaluate(f\"Chunk size {chunk_size}, overlap {overlap_percentage}%, K={k}\", retriever, prompt, llm, results_df)\n",
    "    print(f\"Results for K={k}:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look for different retrievers\n",
    "\n",
    "7 chunks was the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Who arrives at the District 12 Peacekeeper base soon after Coriolanus in 'The Ballad of Songbirds and Snakes'?\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   1%|          | 1/90 [00:01<01:43,  1.17s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   2%|▏         | 2/90 [00:01<01:11,  1.23it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   8%|▊         | 7/90 [00:05<00:47,  1.76it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  82%|████████▏ | 74/90 [00:06<00:00, 30.38it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  96%|█████████▌| 86/90 [00:11<00:00,  5.42it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  98%|█████████▊| 88/90 [00:23<00:00,  4.70it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  99%|█████████▉| 89/90 [02:58<00:15, 15.08s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [02:59<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.572733279936672\n",
      "This is the new best value!\n",
      "                      System  Faithfulness  Answer Relevancy  \\\n",
      "0  Parent Retriever 1000-200      0.408467          0.668263   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.598895        0.679993            0.64895            0.431832   \n",
      "\n",
      "    Average  \n",
      "0  0.572733  \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap = 200)\n",
    "child_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap = 0)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents\",persist_directory = \"../ballad/vectordb-edit/parent\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "parent_document_retriever.add_documents(documents)\n",
    "result_parent, results_df = run_and_evaluate(f\"Parent Retriever 1000-200\", parent_document_retriever, prompt, llm, results_df)\n",
    "print(result_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  29%|██▉       | 26/90 [00:08<00:14,  4.40it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  62%|██████▏   | 56/90 [00:12<00:04,  8.34it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  68%|██████▊   | 61/90 [00:13<00:03,  9.39it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  94%|█████████▍| 85/90 [00:15<00:00,  8.85it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:18<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5350661417942891\n",
      "                     System  Faithfulness  Answer Relevancy  \\\n",
      "0  Parent Retriever 500-100      0.529343          0.496213   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.604886        0.529791           0.428873            0.621292   \n",
      "\n",
      "    Average  \n",
      "0  0.535066  \n"
     ]
    }
   ],
   "source": [
    "parent_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap = 50)\n",
    "child_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap = 0)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents_small\",persist_directory = \"../ballad/vectordb-edit/parent_small\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_small = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_small,\n",
    "    parent_splitter=parent_splitter_small,\n",
    ")\n",
    "parent_document_retriever_small.add_documents(documents)\n",
    "result_parent_small, results_df = run_and_evaluate(f\"Parent Retriever 500-100\", parent_document_retriever_small, prompt, llm, results_df)\n",
    "print(result_parent_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "ValueError: Azure has not provided the response due to a content filter being triggered\n",
      "Evaluating:   1%|          | 1/90 [00:01<02:31,  1.70s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   3%|▎         | 3/90 [00:05<02:36,  1.80s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  20%|██        | 18/90 [00:05<00:16,  4.50it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:11<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5444346678198149\n",
      "                      System  Faithfulness  Answer Relevancy  \\\n",
      "0  Parent Retriever 1500-200      0.668834          0.381221   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.648756        0.497418           0.475129             0.59525   \n",
      "\n",
      "    Average  \n",
      "0  0.544435  \n"
     ]
    }
   ],
   "source": [
    "parent_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1500, chunk_overlap = 150)\n",
    "child_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap = 0)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents_large\",persist_directory = \"../ballad/vectordb-edit/parent_large\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_large = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_large,\n",
    "    parent_splitter=parent_splitter_large,\n",
    ")\n",
    "parent_document_retriever_large.add_documents(documents)\n",
    "result_parent_large , results_df = run_and_evaluate(f\"Parent Retriever 1500-200\", parent_document_retriever_small, prompt, llm, results_df)\n",
    "print(result_parent_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest average value: 0.572733279936672\n"
     ]
    }
   ],
   "source": [
    "highest_average = results_df[\"Average\"].max()\n",
    "print(\"Highest average value:\", highest_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.810294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.793104</td>\n",
       "      <td>0.256609</td>\n",
       "      <td>0.390557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.887922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.801728</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.507236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.558016</td>\n",
       "      <td>0.659556</td>\n",
       "      <td>0.787525</td>\n",
       "      <td>0.514306</td>\n",
       "      <td>0.280777</td>\n",
       "      <td>0.529137</td>\n",
       "      <td>0.554886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0.744159</td>\n",
       "      <td>0.721819</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674370</td>\n",
       "      <td>0.531828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.088874</td>\n",
       "      <td>0.782310</td>\n",
       "      <td>0.277026</td>\n",
       "      <td>0.513533</td>\n",
       "      <td>0.580632</td>\n",
       "      <td>0.460039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.565476</td>\n",
       "      <td>0.218776</td>\n",
       "      <td>0.801542</td>\n",
       "      <td>0.346715</td>\n",
       "      <td>0.585408</td>\n",
       "      <td>0.657749</td>\n",
       "      <td>0.529278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.230681</td>\n",
       "      <td>0.793275</td>\n",
       "      <td>0.326791</td>\n",
       "      <td>0.746519</td>\n",
       "      <td>0.667383</td>\n",
       "      <td>0.569571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.841746</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.812384</td>\n",
       "      <td>0.341578</td>\n",
       "      <td>0.578127</td>\n",
       "      <td>0.536115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.636090</td>\n",
       "      <td>0.486717</td>\n",
       "      <td>0.237498</td>\n",
       "      <td>0.684846</td>\n",
       "      <td>0.471942</td>\n",
       "      <td>0.315999</td>\n",
       "      <td>0.472182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>0.364984</td>\n",
       "      <td>0.852743</td>\n",
       "      <td>0.731757</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671611</td>\n",
       "      <td>0.548152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>0.398970</td>\n",
       "      <td>0.648575</td>\n",
       "      <td>0.641253</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.383179</td>\n",
       "      <td>0.570392</td>\n",
       "      <td>0.506753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>0.362868</td>\n",
       "      <td>0.770050</td>\n",
       "      <td>0.729893</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671417</td>\n",
       "      <td>0.533674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>0.544835</td>\n",
       "      <td>0.650798</td>\n",
       "      <td>0.793733</td>\n",
       "      <td>0.519767</td>\n",
       "      <td>0.277074</td>\n",
       "      <td>0.531771</td>\n",
       "      <td>0.552996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.741209</td>\n",
       "      <td>0.412963</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.795649</td>\n",
       "      <td>0.266531</td>\n",
       "      <td>0.515424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.346149</td>\n",
       "      <td>0.829901</td>\n",
       "      <td>0.662909</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674888</td>\n",
       "      <td>0.530277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>0.686905</td>\n",
       "      <td>0.817278</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.790828</td>\n",
       "      <td>0.244493</td>\n",
       "      <td>0.541152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.665873</td>\n",
       "      <td>0.815407</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.787232</td>\n",
       "      <td>0.229308</td>\n",
       "      <td>0.528649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.739286</td>\n",
       "      <td>0.801774</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.791637</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.553154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.758210</td>\n",
       "      <td>0.526721</td>\n",
       "      <td>0.722255</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>0.456451</td>\n",
       "      <td>0.237525</td>\n",
       "      <td>0.571944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.336367</td>\n",
       "      <td>0.707624</td>\n",
       "      <td>0.672505</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674785</td>\n",
       "      <td>0.509849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.336423</td>\n",
       "      <td>0.794321</td>\n",
       "      <td>0.669045</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674978</td>\n",
       "      <td>0.520060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.387645</td>\n",
       "      <td>0.713186</td>\n",
       "      <td>0.734147</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.291758</td>\n",
       "      <td>0.674870</td>\n",
       "      <td>0.530206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.788360</td>\n",
       "      <td>0.741365</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.306730</td>\n",
       "      <td>0.550448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.807540</td>\n",
       "      <td>0.677429</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.793699</td>\n",
       "      <td>0.264258</td>\n",
       "      <td>0.527833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chunk size 3000, overlap 20%, K=2</td>\n",
       "      <td>0.411970</td>\n",
       "      <td>0.282099</td>\n",
       "      <td>0.579457</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.326902</td>\n",
       "      <td>0.593221</td>\n",
       "      <td>0.410053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 3000, overlap 20%, K=3</td>\n",
       "      <td>0.261245</td>\n",
       "      <td>0.443421</td>\n",
       "      <td>0.594656</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.273538</td>\n",
       "      <td>0.675780</td>\n",
       "      <td>0.429403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chunk size 3000, overlap 20%, K=5</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.806737</td>\n",
       "      <td>0.395926</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.793101</td>\n",
       "      <td>0.261457</td>\n",
       "      <td>0.519105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chunk size 3000, overlap 20%, K=6</td>\n",
       "      <td>0.447991</td>\n",
       "      <td>0.224892</td>\n",
       "      <td>0.821103</td>\n",
       "      <td>0.390395</td>\n",
       "      <td>0.701033</td>\n",
       "      <td>0.749447</td>\n",
       "      <td>0.555810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.408467</td>\n",
       "      <td>0.668263</td>\n",
       "      <td>0.598895</td>\n",
       "      <td>0.679993</td>\n",
       "      <td>0.648950</td>\n",
       "      <td>0.431832</td>\n",
       "      <td>0.572733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>0.529343</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>0.604886</td>\n",
       "      <td>0.529791</td>\n",
       "      <td>0.428873</td>\n",
       "      <td>0.621292</td>\n",
       "      <td>0.535066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Parent Retriever 1500-200</td>\n",
       "      <td>0.668834</td>\n",
       "      <td>0.381221</td>\n",
       "      <td>0.648756</td>\n",
       "      <td>0.497418</td>\n",
       "      <td>0.475129</td>\n",
       "      <td>0.595250</td>\n",
       "      <td>0.544435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               System  Faithfulness  Answer Relevancy  \\\n",
       "0                             GPT-3.5      0.083333          0.810294   \n",
       "1                               GPT-4      0.666667          0.887922   \n",
       "2                               Naive      0.558016          0.659556   \n",
       "3                           Recursive      0.379104          0.744159   \n",
       "4               Chunk 500, overlap 0%      0.517857          0.088874   \n",
       "5               Chunk 500, overlap 5%      0.565476          0.218776   \n",
       "6              Chunk 500, overlap 10%      0.652778          0.230681   \n",
       "7              Chunk 500, overlap 15%      0.841746          0.535714   \n",
       "8              Chunk 500, overlap 20%      0.636090          0.486717   \n",
       "9              Chunk 1000, overlap 0%      0.364984          0.852743   \n",
       "10             Chunk 1000, overlap 5%      0.398970          0.648575   \n",
       "11            Chunk 1000, overlap 10%      0.362868          0.770050   \n",
       "12            Chunk 1000, overlap 15%      0.544835          0.650798   \n",
       "13            Chunk 1000, overlap 20%      0.642857          0.741209   \n",
       "14             Chunk 2000, overlap 0%      0.346149          0.829901   \n",
       "15             Chunk 2000, overlap 5%      0.686905          0.817278   \n",
       "16            Chunk 2000, overlap 10%      0.665873          0.815407   \n",
       "17            Chunk 2000, overlap 15%      0.739286          0.801774   \n",
       "18            Chunk 2000, overlap 20%      0.758210          0.526721   \n",
       "19             Chunk 3000, overlap 0%      0.336367          0.707624   \n",
       "20             Chunk 3000, overlap 5%      0.336423          0.794321   \n",
       "21            Chunk 3000, overlap 10%      0.387645          0.713186   \n",
       "22            Chunk 3000, overlap 15%      0.788360          0.741365   \n",
       "23            Chunk 3000, overlap 20%      0.807540          0.677429   \n",
       "24  Chunk size 3000, overlap 20%, K=2      0.411970          0.282099   \n",
       "25  Chunk size 3000, overlap 20%, K=3      0.261245          0.443421   \n",
       "26  Chunk size 3000, overlap 20%, K=5      0.740741          0.806737   \n",
       "27  Chunk size 3000, overlap 20%, K=6      0.447991          0.224892   \n",
       "28          Parent Retriever 1000-200      0.408467          0.668263   \n",
       "29           Parent Retriever 500-100      0.529343          0.496213   \n",
       "30          Parent Retriever 1500-200      0.668834          0.381221   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.793104            0.256609   \n",
       "1                 NaN             NaN           0.801728            0.287099   \n",
       "2            0.787525        0.514306           0.280777            0.529137   \n",
       "3            0.721819        0.383333           0.288185            0.674370   \n",
       "4            0.782310        0.277026           0.513533            0.580632   \n",
       "5            0.801542        0.346715           0.585408            0.657749   \n",
       "6            0.793275        0.326791           0.746519            0.667383   \n",
       "7            0.107143        0.812384           0.341578            0.578127   \n",
       "8            0.237498        0.684846           0.471942            0.315999   \n",
       "9            0.731757        0.379630           0.288185            0.671611   \n",
       "10           0.641253        0.398148           0.383179            0.570392   \n",
       "11           0.729893        0.379630           0.288185            0.671417   \n",
       "12           0.793733        0.519767           0.277074            0.531771   \n",
       "13           0.412963        0.233333           0.795649            0.266531   \n",
       "14           0.662909        0.379630           0.288185            0.674888   \n",
       "15           0.407407        0.300000           0.790828            0.244493   \n",
       "16           0.374074        0.300000           0.787232            0.229308   \n",
       "17           0.379630        0.300000           0.791637            0.306600   \n",
       "18           0.722255        0.730503           0.456451            0.237525   \n",
       "19           0.672505        0.379630           0.288185            0.674785   \n",
       "20           0.669045        0.357407           0.288185            0.674978   \n",
       "21           0.734147        0.379630           0.291758            0.674870   \n",
       "22           0.374074        0.300000           0.792157            0.306730   \n",
       "23           0.357407        0.266667           0.793699            0.264258   \n",
       "24           0.579457        0.266667           0.326902            0.593221   \n",
       "25           0.594656        0.327778           0.273538            0.675780   \n",
       "26           0.395926        0.116667           0.793101            0.261457   \n",
       "27           0.821103        0.390395           0.701033            0.749447   \n",
       "28           0.598895        0.679993           0.648950            0.431832   \n",
       "29           0.604886        0.529791           0.428873            0.621292   \n",
       "30           0.648756        0.497418           0.475129            0.595250   \n",
       "\n",
       "     Average  \n",
       "0   0.390557  \n",
       "1   0.507236  \n",
       "2   0.554886  \n",
       "3   0.531828  \n",
       "4   0.460039  \n",
       "5   0.529278  \n",
       "6   0.569571  \n",
       "7   0.536115  \n",
       "8   0.472182  \n",
       "9   0.548152  \n",
       "10  0.506753  \n",
       "11  0.533674  \n",
       "12  0.552996  \n",
       "13  0.515424  \n",
       "14  0.530277  \n",
       "15  0.541152  \n",
       "16  0.528649  \n",
       "17  0.553154  \n",
       "18  0.571944  \n",
       "19  0.509849  \n",
       "20  0.520060  \n",
       "21  0.530206  \n",
       "22  0.550448  \n",
       "23  0.527833  \n",
       "24  0.410053  \n",
       "25  0.429403  \n",
       "26  0.519105  \n",
       "27  0.555810  \n",
       "28  0.572733  \n",
       "29  0.535066  \n",
       "30  0.544435  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_map = {\n",
    "    'Chunk size 1000, 0%, K=2': 'Chunk size 2000, 20%, K=2',\n",
    "'Chunk size 1000, 0%, K=3':'Chunk size 2000, 20%, K=3',\n",
    "     'Chunk size 1000, 0%, K=5':'Chunk size 2000, 20%, K=5',\n",
    "    'Chunk size 1000, 0%, K=6':'Chunk size 2000, 20%, K=6'\n",
    "}\n",
    "results_df['System'] = results_df['System'].replace(replacement_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.810294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.793104</td>\n",
       "      <td>0.256609</td>\n",
       "      <td>0.390557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.887922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.801728</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.507236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.558016</td>\n",
       "      <td>0.659556</td>\n",
       "      <td>0.787525</td>\n",
       "      <td>0.514306</td>\n",
       "      <td>0.280777</td>\n",
       "      <td>0.529137</td>\n",
       "      <td>0.554886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0.744159</td>\n",
       "      <td>0.721819</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674370</td>\n",
       "      <td>0.531828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.088874</td>\n",
       "      <td>0.782310</td>\n",
       "      <td>0.277026</td>\n",
       "      <td>0.513533</td>\n",
       "      <td>0.580632</td>\n",
       "      <td>0.460039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.565476</td>\n",
       "      <td>0.218776</td>\n",
       "      <td>0.801542</td>\n",
       "      <td>0.346715</td>\n",
       "      <td>0.585408</td>\n",
       "      <td>0.657749</td>\n",
       "      <td>0.529278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.230681</td>\n",
       "      <td>0.793275</td>\n",
       "      <td>0.326791</td>\n",
       "      <td>0.746519</td>\n",
       "      <td>0.667383</td>\n",
       "      <td>0.569571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.841746</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.812384</td>\n",
       "      <td>0.341578</td>\n",
       "      <td>0.578127</td>\n",
       "      <td>0.536115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.636090</td>\n",
       "      <td>0.486717</td>\n",
       "      <td>0.237498</td>\n",
       "      <td>0.684846</td>\n",
       "      <td>0.471942</td>\n",
       "      <td>0.315999</td>\n",
       "      <td>0.472182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>0.364984</td>\n",
       "      <td>0.852743</td>\n",
       "      <td>0.731757</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671611</td>\n",
       "      <td>0.548152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>0.398970</td>\n",
       "      <td>0.648575</td>\n",
       "      <td>0.641253</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.383179</td>\n",
       "      <td>0.570392</td>\n",
       "      <td>0.506753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>0.362868</td>\n",
       "      <td>0.770050</td>\n",
       "      <td>0.729893</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671417</td>\n",
       "      <td>0.533674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>0.544835</td>\n",
       "      <td>0.650798</td>\n",
       "      <td>0.793733</td>\n",
       "      <td>0.519767</td>\n",
       "      <td>0.277074</td>\n",
       "      <td>0.531771</td>\n",
       "      <td>0.552996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.741209</td>\n",
       "      <td>0.412963</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.795649</td>\n",
       "      <td>0.266531</td>\n",
       "      <td>0.515424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.346149</td>\n",
       "      <td>0.829901</td>\n",
       "      <td>0.662909</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674888</td>\n",
       "      <td>0.530277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>0.686905</td>\n",
       "      <td>0.817278</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.790828</td>\n",
       "      <td>0.244493</td>\n",
       "      <td>0.541152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.665873</td>\n",
       "      <td>0.815407</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.787232</td>\n",
       "      <td>0.229308</td>\n",
       "      <td>0.528649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.739286</td>\n",
       "      <td>0.801774</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.791637</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.553154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.758210</td>\n",
       "      <td>0.526721</td>\n",
       "      <td>0.722255</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>0.456451</td>\n",
       "      <td>0.237525</td>\n",
       "      <td>0.571944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.336367</td>\n",
       "      <td>0.707624</td>\n",
       "      <td>0.672505</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674785</td>\n",
       "      <td>0.509849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.336423</td>\n",
       "      <td>0.794321</td>\n",
       "      <td>0.669045</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674978</td>\n",
       "      <td>0.520060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.387645</td>\n",
       "      <td>0.713186</td>\n",
       "      <td>0.734147</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.291758</td>\n",
       "      <td>0.674870</td>\n",
       "      <td>0.530206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.788360</td>\n",
       "      <td>0.741365</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.306730</td>\n",
       "      <td>0.550448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.807540</td>\n",
       "      <td>0.677429</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.793699</td>\n",
       "      <td>0.264258</td>\n",
       "      <td>0.527833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chunk size 2000, 20%, K=2</td>\n",
       "      <td>0.411970</td>\n",
       "      <td>0.282099</td>\n",
       "      <td>0.579457</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.326902</td>\n",
       "      <td>0.593221</td>\n",
       "      <td>0.410053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 2000, 20%, K=3</td>\n",
       "      <td>0.261245</td>\n",
       "      <td>0.443421</td>\n",
       "      <td>0.594656</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.273538</td>\n",
       "      <td>0.675780</td>\n",
       "      <td>0.429403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chunk size 2000, 20%, K=5</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.806737</td>\n",
       "      <td>0.395926</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.793101</td>\n",
       "      <td>0.261457</td>\n",
       "      <td>0.519105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chunk size 2000, 20%, K=6</td>\n",
       "      <td>0.447991</td>\n",
       "      <td>0.224892</td>\n",
       "      <td>0.821103</td>\n",
       "      <td>0.390395</td>\n",
       "      <td>0.701033</td>\n",
       "      <td>0.749447</td>\n",
       "      <td>0.555810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.408467</td>\n",
       "      <td>0.668263</td>\n",
       "      <td>0.598895</td>\n",
       "      <td>0.679993</td>\n",
       "      <td>0.648950</td>\n",
       "      <td>0.431832</td>\n",
       "      <td>0.572733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>0.529343</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>0.604886</td>\n",
       "      <td>0.529791</td>\n",
       "      <td>0.428873</td>\n",
       "      <td>0.621292</td>\n",
       "      <td>0.535066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Parent Retriever 1500-200</td>\n",
       "      <td>0.668834</td>\n",
       "      <td>0.381221</td>\n",
       "      <td>0.648756</td>\n",
       "      <td>0.497418</td>\n",
       "      <td>0.475129</td>\n",
       "      <td>0.595250</td>\n",
       "      <td>0.544435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       System  Faithfulness  Answer Relevancy  \\\n",
       "0                     GPT-3.5      0.083333          0.810294   \n",
       "1                       GPT-4      0.666667          0.887922   \n",
       "2                       Naive      0.558016          0.659556   \n",
       "3                   Recursive      0.379104          0.744159   \n",
       "4       Chunk 500, overlap 0%      0.517857          0.088874   \n",
       "5       Chunk 500, overlap 5%      0.565476          0.218776   \n",
       "6      Chunk 500, overlap 10%      0.652778          0.230681   \n",
       "7      Chunk 500, overlap 15%      0.841746          0.535714   \n",
       "8      Chunk 500, overlap 20%      0.636090          0.486717   \n",
       "9      Chunk 1000, overlap 0%      0.364984          0.852743   \n",
       "10     Chunk 1000, overlap 5%      0.398970          0.648575   \n",
       "11    Chunk 1000, overlap 10%      0.362868          0.770050   \n",
       "12    Chunk 1000, overlap 15%      0.544835          0.650798   \n",
       "13    Chunk 1000, overlap 20%      0.642857          0.741209   \n",
       "14     Chunk 2000, overlap 0%      0.346149          0.829901   \n",
       "15     Chunk 2000, overlap 5%      0.686905          0.817278   \n",
       "16    Chunk 2000, overlap 10%      0.665873          0.815407   \n",
       "17    Chunk 2000, overlap 15%      0.739286          0.801774   \n",
       "18    Chunk 2000, overlap 20%      0.758210          0.526721   \n",
       "19     Chunk 3000, overlap 0%      0.336367          0.707624   \n",
       "20     Chunk 3000, overlap 5%      0.336423          0.794321   \n",
       "21    Chunk 3000, overlap 10%      0.387645          0.713186   \n",
       "22    Chunk 3000, overlap 15%      0.788360          0.741365   \n",
       "23    Chunk 3000, overlap 20%      0.807540          0.677429   \n",
       "24  Chunk size 2000, 20%, K=2      0.411970          0.282099   \n",
       "25  Chunk size 2000, 20%, K=3      0.261245          0.443421   \n",
       "26  Chunk size 2000, 20%, K=5      0.740741          0.806737   \n",
       "27  Chunk size 2000, 20%, K=6      0.447991          0.224892   \n",
       "28  Parent Retriever 1000-200      0.408467          0.668263   \n",
       "29   Parent Retriever 500-100      0.529343          0.496213   \n",
       "30  Parent Retriever 1500-200      0.668834          0.381221   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.793104            0.256609   \n",
       "1                 NaN             NaN           0.801728            0.287099   \n",
       "2            0.787525        0.514306           0.280777            0.529137   \n",
       "3            0.721819        0.383333           0.288185            0.674370   \n",
       "4            0.782310        0.277026           0.513533            0.580632   \n",
       "5            0.801542        0.346715           0.585408            0.657749   \n",
       "6            0.793275        0.326791           0.746519            0.667383   \n",
       "7            0.107143        0.812384           0.341578            0.578127   \n",
       "8            0.237498        0.684846           0.471942            0.315999   \n",
       "9            0.731757        0.379630           0.288185            0.671611   \n",
       "10           0.641253        0.398148           0.383179            0.570392   \n",
       "11           0.729893        0.379630           0.288185            0.671417   \n",
       "12           0.793733        0.519767           0.277074            0.531771   \n",
       "13           0.412963        0.233333           0.795649            0.266531   \n",
       "14           0.662909        0.379630           0.288185            0.674888   \n",
       "15           0.407407        0.300000           0.790828            0.244493   \n",
       "16           0.374074        0.300000           0.787232            0.229308   \n",
       "17           0.379630        0.300000           0.791637            0.306600   \n",
       "18           0.722255        0.730503           0.456451            0.237525   \n",
       "19           0.672505        0.379630           0.288185            0.674785   \n",
       "20           0.669045        0.357407           0.288185            0.674978   \n",
       "21           0.734147        0.379630           0.291758            0.674870   \n",
       "22           0.374074        0.300000           0.792157            0.306730   \n",
       "23           0.357407        0.266667           0.793699            0.264258   \n",
       "24           0.579457        0.266667           0.326902            0.593221   \n",
       "25           0.594656        0.327778           0.273538            0.675780   \n",
       "26           0.395926        0.116667           0.793101            0.261457   \n",
       "27           0.821103        0.390395           0.701033            0.749447   \n",
       "28           0.598895        0.679993           0.648950            0.431832   \n",
       "29           0.604886        0.529791           0.428873            0.621292   \n",
       "30           0.648756        0.497418           0.475129            0.595250   \n",
       "\n",
       "     Average  \n",
       "0   0.390557  \n",
       "1   0.507236  \n",
       "2   0.554886  \n",
       "3   0.531828  \n",
       "4   0.460039  \n",
       "5   0.529278  \n",
       "6   0.569571  \n",
       "7   0.536115  \n",
       "8   0.472182  \n",
       "9   0.548152  \n",
       "10  0.506753  \n",
       "11  0.533674  \n",
       "12  0.552996  \n",
       "13  0.515424  \n",
       "14  0.530277  \n",
       "15  0.541152  \n",
       "16  0.528649  \n",
       "17  0.553154  \n",
       "18  0.571944  \n",
       "19  0.509849  \n",
       "20  0.520060  \n",
       "21  0.530206  \n",
       "22  0.550448  \n",
       "23  0.527833  \n",
       "24  0.410053  \n",
       "25  0.429403  \n",
       "26  0.519105  \n",
       "27  0.555810  \n",
       "28  0.572733  \n",
       "29  0.535066  \n",
       "30  0.544435  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum marginal relevance retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  78%|███████▊  | 70/90 [00:06<00:00, 28.61it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': True, 'severity': 'medium'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  93%|█████████▎| 84/90 [00:11<00:00,  6.95it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 90/90 [00:12<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.47590768652696824\n",
      "Marginal relevance\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0    MMR      0.722222          0.611605           0.395734        0.123333   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.786041             0.21651  0.475908  \n"
     ]
    }
   ],
   "source": [
    "retriever_mmr = db_k.as_retriever(search_type=\"mmr\")\n",
    "result_mmr, results_df = run_and_evaluate(f\"MMR\", retriever_mmr, prompt, llm, results_df)\n",
    "print(\"Marginal relevance\")\n",
    "print(result_mmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest average value: 0.572733279936672\n"
     ]
    }
   ],
   "source": [
    "highest_average = results_df[\"Average\"].max()\n",
    "print(\"Highest average value:\", highest_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Who wrote 'The Hanging Tree' song?\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}on the following question: Where did the jabberjays originate in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   1%|          | 1/90 [00:00<01:02,  1.41it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:   2%|▏         | 2/90 [00:01<00:44,  1.97it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:   3%|▎         | 3/90 [00:01<00:46,  1.88it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating:   6%|▌         | 5/90 [00:04<01:28,  1.04s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  19%|█▉        | 17/90 [00:05<00:16,  4.29it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  99%|█████████▉| 89/90 [00:32<00:00,  3.19it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [02:55<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.41237378448922163\n",
      "BM25\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0   BM25      0.458274          0.461641           0.326519        0.312199   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.585142            0.330468  0.412374  \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 2000, chunk_overlap = 400)\n",
    "chunks_1000 = text_splitter.split_documents(documents)\n",
    "retriever_bm25 = BM25Retriever.from_documents(chunks_1000)\n",
    "result_bm25, results_df = run_and_evaluate(f\"BM25\", retriever_bm25, prompt, llm, results_df)\n",
    "print(\"BM25\")\n",
    "print(result_bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensambler - Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = db_k.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   4%|▍         | 4/90 [00:05<01:41,  1.18s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  16%|█▌        | 14/90 [00:06<00:22,  3.44it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  81%|████████  | 73/90 [00:07<00:00, 23.99it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:14<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5803926758896713\n",
      "This is the new best value!\n",
      "Ensambler\n",
      "        System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Ensambler 1      0.478271          0.703551           0.671269   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.673675           0.566497            0.389093  0.580393  \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever_1 = EnsembleRetriever(retrievers=[retriever_bm25, ret], weights=[0.75, 0.25])\n",
    "result_ensemble1, results_df = run_and_evaluate(f\"Ensambler 1\", ensemble_retriever_1, prompt, llm, results_df)\n",
    "print(\"Ensambler\")\n",
    "print(result_ensemble1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  16%|█▌        | 14/90 [00:06<00:20,  3.66it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  84%|████████▍ | 76/90 [00:07<00:00, 28.47it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  92%|█████████▏| 83/90 [00:12<00:01,  5.00it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  98%|█████████▊| 88/90 [00:25<00:00,  5.48it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [02:14<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5868242857901301\n",
      "This is the new best value!\n",
      "Ensambler\n",
      "        System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Ensambler 2      0.621946          0.429459           0.584322   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.682494           0.625598            0.577126  0.586824  \n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_2 = EnsembleRetriever(retrievers=[retriever_bm25, ret], weights=[0.5, 0.5])\n",
    "result_ensemble2, results_df = run_and_evaluate(f\"Ensambler 2\", ensemble_retriever_2, prompt, llm, results_df)\n",
    "print(\"Ensambler\")\n",
    "print(result_ensemble2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Who wrote 'The Hanging Tree' song?\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Who arrives at the District 12 Peacekeeper base soon after Coriolanus in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   2%|▏         | 2/90 [00:02<01:47,  1.22s/it]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating:   7%|▋         | 6/90 [00:05<01:07,  1.25it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  72%|███████▏  | 65/90 [00:06<00:00, 26.63it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  81%|████████  | 73/90 [00:12<00:03,  5.20it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  88%|████████▊ | 79/90 [00:13<00:02,  5.46it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  98%|█████████▊| 88/90 [00:35<00:03,  1.51s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  99%|█████████▉| 89/90 [02:44<00:11, 11.86s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [03:01<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.6292721453464117\n",
      "This is the new best value!\n",
      "Ensambler\n",
      "        System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Ensambler 3      0.614624           0.72338           0.801633   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.402776           0.611634            0.621585  0.629272  \n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_3 = EnsembleRetriever(retrievers=[retriever_bm25, ret], weights=[0.25,0.75])\n",
    "result_ensemble3, results_df = run_and_evaluate(f\"Ensambler 3\", ensemble_retriever_3, prompt, llm, results_df)\n",
    "print(\"Ensambler\")\n",
    "print(result_ensemble3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"../ballad/results/results_qa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-stage - reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Who wrote 'The Hanging Tree' song?\",\n",
    "    \"Which District was the source of the Snow family's wealth before the war?\",\n",
    "    \"Which district did the Plinth family call home before moving to the Capitol?\",\n",
    "    \"What is Lucy Gray wearing when she first appears in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"Where are the tributes sent immediately after they arrive in the Capitol in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"What does Coriolanus do to keep Lucy Gray safe when he realizes Dr. Gaul is putting her hybrid snakes in the arena in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"What does Coriolanus do that causes Sejanus's execution in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"What is one of the main reasons Coriolanus is chosen to sing the national anthem at Arachne's funeral in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"Who arrives at the District 12 Peacekeeper base soon after Coriolanus in 'The Ballad of Songbirds and Snakes'?\"\n",
    "]\n",
    "ground_truths = [\n",
    "    [\"Lucy Gray Baird from District 12.\"],\n",
    "    [\"District 13.\"],\n",
    "    [\"District 2.\"],\n",
    "    [\"A ruffled dress in rainbow colors.\"],\n",
    "    [\"The monkey house in the zoo.\"],\n",
    "    [\"He drops a handkerchief with Lucy Gray's scent into the snake tank.\"],\n",
    "    [\"He uses a jabberjay to record Sejanus talking about his part in a rebel plan.\"],\n",
    "    [\"He knows all the words.\"],\n",
    "    [\"Sejanus.\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Where did the jabberjays originate in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"Why does Coriolanus feel he needs to kill Lucy Gray?\",\n",
    "    \"What does Coriolanus do to keep Lucy Gray safe when he realizes Dr. Gaul is putting her hybrid snakes in the arena?\",\n",
    "    \"Coriolanus regularly receives a box from Mrs. Plinth in 'The Ballad of Songbirds and Snakes'. What is in the box?\",\n",
    "    \"What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\",\n",
    "    \"What does Coriolanus wear in his lapel at the interview to remind everyone that Lucy Gray 'belongs to' him in 'The Ballad of Songbirds and Snakes'?\"\n",
    "]\n",
    "ground_truths = [\n",
    "    [\"In Dr. Gaul's lab.\"],\n",
    "    [\"She can tie him to Mayfair's murder.\"],\n",
    "    [\"Drops a handkerchief with Lucy Gray's scent into the snake tank.\"],\n",
    "    [\"Baked goods.\"],\n",
    "    [\"Breadcrumbs so he will have food to eat during his journey.\"],\n",
    "    [\"A rose that matches the one in her hair.\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 54/54 [00:06<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker\n",
      "{'faithfulness': 0.8333, 'answer_relevancy': 0.8419, 'context_precision': 0.7593, 'context_recall': 0.5370, 'answer_similarity': 0.8132, 'answer_correctness': 0.3978}\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "compressor = CohereRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever_3\n",
    ")\n",
    "\n",
    "result_compression_1 = change_chunk_size(compression_retriever)\n",
    "print(\"Reranker\")\n",
    "print(result_compression_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: What is Sejanus sprinkling on Marcus's body in the arena in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/36 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 58, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 605, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 91, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 91, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 87, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\_answer_relevance.py\", line 136, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 544, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 408, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 398, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 577, in _generate_with_cache\n",
      "    return self._generate(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 462, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 663, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1200, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 889, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 980, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   6%|▌         | 2/36 [00:00<00:16,  2.08it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 58, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 605, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 91, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 91, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 87, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 192, in _ascore\n",
      "    nli_result = await self.llm.generate(p, callbacks=callbacks, is_async=is_async)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 544, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 408, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 398, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 577, in _generate_with_cache\n",
      "    return self._generate(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 462, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 663, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1200, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 889, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 980, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  61%|██████    | 22/36 [00:02<00:00, 17.31it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 58, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 605, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 91, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 91, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 87, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 129, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 110, in generate\n",
      "    return await loop.run_in_executor(None, generate_text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 139, in generate_text\n",
      "    return self.langchain_llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 544, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 408, in generate\n",
      "    raise e\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 398, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 577, in _generate_with_cache\n",
      "    return self._generate(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 462, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 663, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1200, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 889, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 980, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  92%|█████████▏| 33/36 [00:19<00:00, 18.50it/s]Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 58, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 605, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\executor.py\", line 91, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 91, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 87, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\metrics\\_context_recall.py\", line 113, in _ascore\n",
      "    result = await self.llm.generate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 92, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sigitalapina\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\ragas\\llms\\base.py\", line 169, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 554, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 514, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 617, in _agenerate_with_cache\n",
      "    return await self._agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 559, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1330, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1725, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1428, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1519, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 36/36 [02:02<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker\n",
      "{'faithfulness': 0.7000, 'answer_relevancy': 0.1810, 'context_precision': 0.8199, 'context_recall': 0.3550, 'answer_similarity': 0.6015, 'answer_correctness': 0.4136}\n"
     ]
    }
   ],
   "source": [
    "result_compression_2 = change_chunk_size(compression_retriever)\n",
    "print(\"Reranker\")\n",
    "print(result_compression_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Result.to_pandas of {'faithfulness': 0.8333, 'answer_relevancy': 0.8419, 'context_precision': 0.7593, 'context_recall': 0.5370, 'answer_similarity': 0.8132, 'answer_correctness': 0.3978}>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1=result_compression_1.to_pandas\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Result.to_pandas of {'faithfulness': 0.7000, 'answer_relevancy': 0.1810, 'context_precision': 0.8199, 'context_recall': 0.3550, 'answer_similarity': 0.6015, 'answer_correctness': 0.4136}>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2=result_compression_2.to_pandas\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.810294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.793104</td>\n",
       "      <td>0.256609</td>\n",
       "      <td>0.390557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.887922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.801728</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.507236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.558016</td>\n",
       "      <td>0.659556</td>\n",
       "      <td>0.787525</td>\n",
       "      <td>0.514306</td>\n",
       "      <td>0.280777</td>\n",
       "      <td>0.529137</td>\n",
       "      <td>0.554886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0.744159</td>\n",
       "      <td>0.721819</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674370</td>\n",
       "      <td>0.531828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.088874</td>\n",
       "      <td>0.782310</td>\n",
       "      <td>0.277026</td>\n",
       "      <td>0.513533</td>\n",
       "      <td>0.580632</td>\n",
       "      <td>0.460039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.565476</td>\n",
       "      <td>0.218776</td>\n",
       "      <td>0.801542</td>\n",
       "      <td>0.346715</td>\n",
       "      <td>0.585408</td>\n",
       "      <td>0.657749</td>\n",
       "      <td>0.529278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.230681</td>\n",
       "      <td>0.793275</td>\n",
       "      <td>0.326791</td>\n",
       "      <td>0.746519</td>\n",
       "      <td>0.667383</td>\n",
       "      <td>0.569571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.841746</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.812384</td>\n",
       "      <td>0.341578</td>\n",
       "      <td>0.578127</td>\n",
       "      <td>0.536115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.636090</td>\n",
       "      <td>0.486717</td>\n",
       "      <td>0.237498</td>\n",
       "      <td>0.684846</td>\n",
       "      <td>0.471942</td>\n",
       "      <td>0.315999</td>\n",
       "      <td>0.472182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>0.364984</td>\n",
       "      <td>0.852743</td>\n",
       "      <td>0.731757</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671611</td>\n",
       "      <td>0.548152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>0.398970</td>\n",
       "      <td>0.648575</td>\n",
       "      <td>0.641253</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.383179</td>\n",
       "      <td>0.570392</td>\n",
       "      <td>0.506753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>0.362868</td>\n",
       "      <td>0.770050</td>\n",
       "      <td>0.729893</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671417</td>\n",
       "      <td>0.533674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>0.544835</td>\n",
       "      <td>0.650798</td>\n",
       "      <td>0.793733</td>\n",
       "      <td>0.519767</td>\n",
       "      <td>0.277074</td>\n",
       "      <td>0.531771</td>\n",
       "      <td>0.552996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.741209</td>\n",
       "      <td>0.412963</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.795649</td>\n",
       "      <td>0.266531</td>\n",
       "      <td>0.515424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.346149</td>\n",
       "      <td>0.829901</td>\n",
       "      <td>0.662909</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674888</td>\n",
       "      <td>0.530277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>0.686905</td>\n",
       "      <td>0.817278</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.790828</td>\n",
       "      <td>0.244493</td>\n",
       "      <td>0.541152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.665873</td>\n",
       "      <td>0.815407</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.787232</td>\n",
       "      <td>0.229308</td>\n",
       "      <td>0.528649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.739286</td>\n",
       "      <td>0.801774</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.791637</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.553154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.758210</td>\n",
       "      <td>0.526721</td>\n",
       "      <td>0.722255</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>0.456451</td>\n",
       "      <td>0.237525</td>\n",
       "      <td>0.571944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.336367</td>\n",
       "      <td>0.707624</td>\n",
       "      <td>0.672505</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674785</td>\n",
       "      <td>0.509849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.336423</td>\n",
       "      <td>0.794321</td>\n",
       "      <td>0.669045</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674978</td>\n",
       "      <td>0.520060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.387645</td>\n",
       "      <td>0.713186</td>\n",
       "      <td>0.734147</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.291758</td>\n",
       "      <td>0.674870</td>\n",
       "      <td>0.530206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.788360</td>\n",
       "      <td>0.741365</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.306730</td>\n",
       "      <td>0.550448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.807540</td>\n",
       "      <td>0.677429</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.793699</td>\n",
       "      <td>0.264258</td>\n",
       "      <td>0.527833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chunk size 2000, 20%, K=2</td>\n",
       "      <td>0.411970</td>\n",
       "      <td>0.282099</td>\n",
       "      <td>0.579457</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.326902</td>\n",
       "      <td>0.593221</td>\n",
       "      <td>0.410053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 2000, 20%, K=3</td>\n",
       "      <td>0.261245</td>\n",
       "      <td>0.443421</td>\n",
       "      <td>0.594656</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.273538</td>\n",
       "      <td>0.675780</td>\n",
       "      <td>0.429403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chunk size 2000, 20%, K=5</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.806737</td>\n",
       "      <td>0.395926</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.793101</td>\n",
       "      <td>0.261457</td>\n",
       "      <td>0.519105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chunk size 2000, 20%, K=6</td>\n",
       "      <td>0.447991</td>\n",
       "      <td>0.224892</td>\n",
       "      <td>0.821103</td>\n",
       "      <td>0.390395</td>\n",
       "      <td>0.701033</td>\n",
       "      <td>0.749447</td>\n",
       "      <td>0.555810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.408467</td>\n",
       "      <td>0.668263</td>\n",
       "      <td>0.598895</td>\n",
       "      <td>0.679993</td>\n",
       "      <td>0.648950</td>\n",
       "      <td>0.431832</td>\n",
       "      <td>0.572733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>0.529343</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>0.604886</td>\n",
       "      <td>0.529791</td>\n",
       "      <td>0.428873</td>\n",
       "      <td>0.621292</td>\n",
       "      <td>0.535066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Parent Retriever 1500-200</td>\n",
       "      <td>0.668834</td>\n",
       "      <td>0.381221</td>\n",
       "      <td>0.648756</td>\n",
       "      <td>0.497418</td>\n",
       "      <td>0.475129</td>\n",
       "      <td>0.595250</td>\n",
       "      <td>0.544435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MMR</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.611605</td>\n",
       "      <td>0.395734</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.786041</td>\n",
       "      <td>0.216510</td>\n",
       "      <td>0.475908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.458274</td>\n",
       "      <td>0.461641</td>\n",
       "      <td>0.326519</td>\n",
       "      <td>0.312199</td>\n",
       "      <td>0.585142</td>\n",
       "      <td>0.330468</td>\n",
       "      <td>0.412374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ensambler 1</td>\n",
       "      <td>0.478271</td>\n",
       "      <td>0.703551</td>\n",
       "      <td>0.671269</td>\n",
       "      <td>0.673675</td>\n",
       "      <td>0.566497</td>\n",
       "      <td>0.389093</td>\n",
       "      <td>0.580393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ensambler 2</td>\n",
       "      <td>0.621946</td>\n",
       "      <td>0.429459</td>\n",
       "      <td>0.584322</td>\n",
       "      <td>0.682494</td>\n",
       "      <td>0.625598</td>\n",
       "      <td>0.577126</td>\n",
       "      <td>0.586824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ensambler 3</td>\n",
       "      <td>0.614624</td>\n",
       "      <td>0.723380</td>\n",
       "      <td>0.801633</td>\n",
       "      <td>0.402776</td>\n",
       "      <td>0.611634</td>\n",
       "      <td>0.621585</td>\n",
       "      <td>0.629272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       System  Faithfulness  Answer Relevancy  \\\n",
       "0                     GPT-3.5      0.083333          0.810294   \n",
       "1                       GPT-4      0.666667          0.887922   \n",
       "2                       Naive      0.558016          0.659556   \n",
       "3                   Recursive      0.379104          0.744159   \n",
       "4       Chunk 500, overlap 0%      0.517857          0.088874   \n",
       "5       Chunk 500, overlap 5%      0.565476          0.218776   \n",
       "6      Chunk 500, overlap 10%      0.652778          0.230681   \n",
       "7      Chunk 500, overlap 15%      0.841746          0.535714   \n",
       "8      Chunk 500, overlap 20%      0.636090          0.486717   \n",
       "9      Chunk 1000, overlap 0%      0.364984          0.852743   \n",
       "10     Chunk 1000, overlap 5%      0.398970          0.648575   \n",
       "11    Chunk 1000, overlap 10%      0.362868          0.770050   \n",
       "12    Chunk 1000, overlap 15%      0.544835          0.650798   \n",
       "13    Chunk 1000, overlap 20%      0.642857          0.741209   \n",
       "14     Chunk 2000, overlap 0%      0.346149          0.829901   \n",
       "15     Chunk 2000, overlap 5%      0.686905          0.817278   \n",
       "16    Chunk 2000, overlap 10%      0.665873          0.815407   \n",
       "17    Chunk 2000, overlap 15%      0.739286          0.801774   \n",
       "18    Chunk 2000, overlap 20%      0.758210          0.526721   \n",
       "19     Chunk 3000, overlap 0%      0.336367          0.707624   \n",
       "20     Chunk 3000, overlap 5%      0.336423          0.794321   \n",
       "21    Chunk 3000, overlap 10%      0.387645          0.713186   \n",
       "22    Chunk 3000, overlap 15%      0.788360          0.741365   \n",
       "23    Chunk 3000, overlap 20%      0.807540          0.677429   \n",
       "24  Chunk size 2000, 20%, K=2      0.411970          0.282099   \n",
       "25  Chunk size 2000, 20%, K=3      0.261245          0.443421   \n",
       "26  Chunk size 2000, 20%, K=5      0.740741          0.806737   \n",
       "27  Chunk size 2000, 20%, K=6      0.447991          0.224892   \n",
       "28  Parent Retriever 1000-200      0.408467          0.668263   \n",
       "29   Parent Retriever 500-100      0.529343          0.496213   \n",
       "30  Parent Retriever 1500-200      0.668834          0.381221   \n",
       "31                        MMR      0.722222          0.611605   \n",
       "32                       BM25      0.458274          0.461641   \n",
       "33                Ensambler 1      0.478271          0.703551   \n",
       "34                Ensambler 2      0.621946          0.429459   \n",
       "35                Ensambler 3      0.614624          0.723380   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.793104            0.256609   \n",
       "1                 NaN             NaN           0.801728            0.287099   \n",
       "2            0.787525        0.514306           0.280777            0.529137   \n",
       "3            0.721819        0.383333           0.288185            0.674370   \n",
       "4            0.782310        0.277026           0.513533            0.580632   \n",
       "5            0.801542        0.346715           0.585408            0.657749   \n",
       "6            0.793275        0.326791           0.746519            0.667383   \n",
       "7            0.107143        0.812384           0.341578            0.578127   \n",
       "8            0.237498        0.684846           0.471942            0.315999   \n",
       "9            0.731757        0.379630           0.288185            0.671611   \n",
       "10           0.641253        0.398148           0.383179            0.570392   \n",
       "11           0.729893        0.379630           0.288185            0.671417   \n",
       "12           0.793733        0.519767           0.277074            0.531771   \n",
       "13           0.412963        0.233333           0.795649            0.266531   \n",
       "14           0.662909        0.379630           0.288185            0.674888   \n",
       "15           0.407407        0.300000           0.790828            0.244493   \n",
       "16           0.374074        0.300000           0.787232            0.229308   \n",
       "17           0.379630        0.300000           0.791637            0.306600   \n",
       "18           0.722255        0.730503           0.456451            0.237525   \n",
       "19           0.672505        0.379630           0.288185            0.674785   \n",
       "20           0.669045        0.357407           0.288185            0.674978   \n",
       "21           0.734147        0.379630           0.291758            0.674870   \n",
       "22           0.374074        0.300000           0.792157            0.306730   \n",
       "23           0.357407        0.266667           0.793699            0.264258   \n",
       "24           0.579457        0.266667           0.326902            0.593221   \n",
       "25           0.594656        0.327778           0.273538            0.675780   \n",
       "26           0.395926        0.116667           0.793101            0.261457   \n",
       "27           0.821103        0.390395           0.701033            0.749447   \n",
       "28           0.598895        0.679993           0.648950            0.431832   \n",
       "29           0.604886        0.529791           0.428873            0.621292   \n",
       "30           0.648756        0.497418           0.475129            0.595250   \n",
       "31           0.395734        0.123333           0.786041            0.216510   \n",
       "32           0.326519        0.312199           0.585142            0.330468   \n",
       "33           0.671269        0.673675           0.566497            0.389093   \n",
       "34           0.584322        0.682494           0.625598            0.577126   \n",
       "35           0.801633        0.402776           0.611634            0.621585   \n",
       "\n",
       "     Average  \n",
       "0   0.390557  \n",
       "1   0.507236  \n",
       "2   0.554886  \n",
       "3   0.531828  \n",
       "4   0.460039  \n",
       "5   0.529278  \n",
       "6   0.569571  \n",
       "7   0.536115  \n",
       "8   0.472182  \n",
       "9   0.548152  \n",
       "10  0.506753  \n",
       "11  0.533674  \n",
       "12  0.552996  \n",
       "13  0.515424  \n",
       "14  0.530277  \n",
       "15  0.541152  \n",
       "16  0.528649  \n",
       "17  0.553154  \n",
       "18  0.571944  \n",
       "19  0.509849  \n",
       "20  0.520060  \n",
       "21  0.530206  \n",
       "22  0.550448  \n",
       "23  0.527833  \n",
       "24  0.410053  \n",
       "25  0.429403  \n",
       "26  0.519105  \n",
       "27  0.555810  \n",
       "28  0.572733  \n",
       "29  0.535066  \n",
       "30  0.544435  \n",
       "31  0.475908  \n",
       "32  0.412374  \n",
       "33  0.580393  \n",
       "34  0.586824  \n",
       "35  0.629272  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ensambler 3</td>\n",
       "      <td>0.614624</td>\n",
       "      <td>0.723380</td>\n",
       "      <td>0.801633</td>\n",
       "      <td>0.402776</td>\n",
       "      <td>0.611634</td>\n",
       "      <td>0.621585</td>\n",
       "      <td>0.629272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ensambler 2</td>\n",
       "      <td>0.621946</td>\n",
       "      <td>0.429459</td>\n",
       "      <td>0.584322</td>\n",
       "      <td>0.682494</td>\n",
       "      <td>0.625598</td>\n",
       "      <td>0.577126</td>\n",
       "      <td>0.586824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ensambler 1</td>\n",
       "      <td>0.478271</td>\n",
       "      <td>0.703551</td>\n",
       "      <td>0.671269</td>\n",
       "      <td>0.673675</td>\n",
       "      <td>0.566497</td>\n",
       "      <td>0.389093</td>\n",
       "      <td>0.580393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "35  Ensambler 3      0.614624          0.723380           0.801633   \n",
       "34  Ensambler 2      0.621946          0.429459           0.584322   \n",
       "33  Ensambler 1      0.478271          0.703551           0.671269   \n",
       "\n",
       "    Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "35        0.402776           0.611634            0.621585  0.629272  \n",
       "34        0.682494           0.625598            0.577126  0.586824  \n",
       "33        0.673675           0.566497            0.389093  0.580393  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_highest = results_df.nlargest(3, \"Average\")\n",
    "top_3_highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating context by remaking the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_context = \"Generate a search query to fetch the relevant documents using the user's {question}. Craft a query that specifically targets the keywords in the question. In the answer provide only the query.\"\n",
    "prompt_context = ChatPromptTemplate.from_template(template_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_final = []\n",
    "contexts_final = []\n",
    "llm_for_context =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt_context | llm}\n",
    ")\n",
    "for query in questions:\n",
    "    response_check = llm_for_context.invoke({\"question\": query})\n",
    "    search_query = response_check[\"response\"].content\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"context\"), \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "    docs = ensemble_retriever_3.get_relevant_documents(search_query)\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        resulting_doc = doc.page_content\n",
    "        formatted_docs.append(resulting_doc)\n",
    "    try:  \n",
    "            response = retrieval_augmented_qa_chain.invoke({\"context\": formatted_docs, \"question\": query})\n",
    "            # Access the response content\n",
    "            answers_final.append(response[\"response\"].content)\n",
    "            contexts_final.append(formatted_docs)  \n",
    "    except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers_final.append(\"No answer\")\n",
    "            contexts_final.append(formatted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   1%|          | 1/90 [00:00<01:05,  1.36it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   3%|▎         | 3/90 [00:05<03:11,  2.20s/it]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating:  79%|███████▉  | 71/90 [00:06<00:00, 26.96it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  98%|█████████▊| 88/90 [00:29<00:00,  4.22it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [02:19<00:00,  1.55s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.5932, 'answer_relevancy': 0.5870, 'context_precision': 0.4648, 'context_recall': 0.3916, 'answer_similarity': 0.7860, 'answer_correctness': 0.3464}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_search_query = evaluation_rag(questions, answers_final, contexts_final, ground_truths)\n",
    "result_search_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.528162961943606\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.810294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.793104</td>\n",
       "      <td>0.256609</td>\n",
       "      <td>0.390557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.887922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.801728</td>\n",
       "      <td>0.287099</td>\n",
       "      <td>0.507236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.558016</td>\n",
       "      <td>0.659556</td>\n",
       "      <td>0.787525</td>\n",
       "      <td>0.514306</td>\n",
       "      <td>0.280777</td>\n",
       "      <td>0.529137</td>\n",
       "      <td>0.554886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0.744159</td>\n",
       "      <td>0.721819</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674370</td>\n",
       "      <td>0.531828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.088874</td>\n",
       "      <td>0.782310</td>\n",
       "      <td>0.277026</td>\n",
       "      <td>0.513533</td>\n",
       "      <td>0.580632</td>\n",
       "      <td>0.460039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.565476</td>\n",
       "      <td>0.218776</td>\n",
       "      <td>0.801542</td>\n",
       "      <td>0.346715</td>\n",
       "      <td>0.585408</td>\n",
       "      <td>0.657749</td>\n",
       "      <td>0.529278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.230681</td>\n",
       "      <td>0.793275</td>\n",
       "      <td>0.326791</td>\n",
       "      <td>0.746519</td>\n",
       "      <td>0.667383</td>\n",
       "      <td>0.569571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.841746</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.812384</td>\n",
       "      <td>0.341578</td>\n",
       "      <td>0.578127</td>\n",
       "      <td>0.536115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.636090</td>\n",
       "      <td>0.486717</td>\n",
       "      <td>0.237498</td>\n",
       "      <td>0.684846</td>\n",
       "      <td>0.471942</td>\n",
       "      <td>0.315999</td>\n",
       "      <td>0.472182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>0.364984</td>\n",
       "      <td>0.852743</td>\n",
       "      <td>0.731757</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671611</td>\n",
       "      <td>0.548152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>0.398970</td>\n",
       "      <td>0.648575</td>\n",
       "      <td>0.641253</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.383179</td>\n",
       "      <td>0.570392</td>\n",
       "      <td>0.506753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>0.362868</td>\n",
       "      <td>0.770050</td>\n",
       "      <td>0.729893</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.671417</td>\n",
       "      <td>0.533674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>0.544835</td>\n",
       "      <td>0.650798</td>\n",
       "      <td>0.793733</td>\n",
       "      <td>0.519767</td>\n",
       "      <td>0.277074</td>\n",
       "      <td>0.531771</td>\n",
       "      <td>0.552996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.741209</td>\n",
       "      <td>0.412963</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.795649</td>\n",
       "      <td>0.266531</td>\n",
       "      <td>0.515424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.346149</td>\n",
       "      <td>0.829901</td>\n",
       "      <td>0.662909</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674888</td>\n",
       "      <td>0.530277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>0.686905</td>\n",
       "      <td>0.817278</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.790828</td>\n",
       "      <td>0.244493</td>\n",
       "      <td>0.541152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.665873</td>\n",
       "      <td>0.815407</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.787232</td>\n",
       "      <td>0.229308</td>\n",
       "      <td>0.528649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.739286</td>\n",
       "      <td>0.801774</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.791637</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.553154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.758210</td>\n",
       "      <td>0.526721</td>\n",
       "      <td>0.722255</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>0.456451</td>\n",
       "      <td>0.237525</td>\n",
       "      <td>0.571944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.336367</td>\n",
       "      <td>0.707624</td>\n",
       "      <td>0.672505</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674785</td>\n",
       "      <td>0.509849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.336423</td>\n",
       "      <td>0.794321</td>\n",
       "      <td>0.669045</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.288185</td>\n",
       "      <td>0.674978</td>\n",
       "      <td>0.520060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.387645</td>\n",
       "      <td>0.713186</td>\n",
       "      <td>0.734147</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.291758</td>\n",
       "      <td>0.674870</td>\n",
       "      <td>0.530206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.788360</td>\n",
       "      <td>0.741365</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.306730</td>\n",
       "      <td>0.550448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.807540</td>\n",
       "      <td>0.677429</td>\n",
       "      <td>0.357407</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.793699</td>\n",
       "      <td>0.264258</td>\n",
       "      <td>0.527833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chunk size 2000, 20%, K=2</td>\n",
       "      <td>0.411970</td>\n",
       "      <td>0.282099</td>\n",
       "      <td>0.579457</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.326902</td>\n",
       "      <td>0.593221</td>\n",
       "      <td>0.410053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 2000, 20%, K=3</td>\n",
       "      <td>0.261245</td>\n",
       "      <td>0.443421</td>\n",
       "      <td>0.594656</td>\n",
       "      <td>0.327778</td>\n",
       "      <td>0.273538</td>\n",
       "      <td>0.675780</td>\n",
       "      <td>0.429403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chunk size 2000, 20%, K=5</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.806737</td>\n",
       "      <td>0.395926</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.793101</td>\n",
       "      <td>0.261457</td>\n",
       "      <td>0.519105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chunk size 2000, 20%, K=6</td>\n",
       "      <td>0.447991</td>\n",
       "      <td>0.224892</td>\n",
       "      <td>0.821103</td>\n",
       "      <td>0.390395</td>\n",
       "      <td>0.701033</td>\n",
       "      <td>0.749447</td>\n",
       "      <td>0.555810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.408467</td>\n",
       "      <td>0.668263</td>\n",
       "      <td>0.598895</td>\n",
       "      <td>0.679993</td>\n",
       "      <td>0.648950</td>\n",
       "      <td>0.431832</td>\n",
       "      <td>0.572733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>0.529343</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>0.604886</td>\n",
       "      <td>0.529791</td>\n",
       "      <td>0.428873</td>\n",
       "      <td>0.621292</td>\n",
       "      <td>0.535066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Parent Retriever 1500-200</td>\n",
       "      <td>0.668834</td>\n",
       "      <td>0.381221</td>\n",
       "      <td>0.648756</td>\n",
       "      <td>0.497418</td>\n",
       "      <td>0.475129</td>\n",
       "      <td>0.595250</td>\n",
       "      <td>0.544435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MMR</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.611605</td>\n",
       "      <td>0.395734</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.786041</td>\n",
       "      <td>0.216510</td>\n",
       "      <td>0.475908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.458274</td>\n",
       "      <td>0.461641</td>\n",
       "      <td>0.326519</td>\n",
       "      <td>0.312199</td>\n",
       "      <td>0.585142</td>\n",
       "      <td>0.330468</td>\n",
       "      <td>0.412374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ensambler 1</td>\n",
       "      <td>0.478271</td>\n",
       "      <td>0.703551</td>\n",
       "      <td>0.671269</td>\n",
       "      <td>0.673675</td>\n",
       "      <td>0.566497</td>\n",
       "      <td>0.389093</td>\n",
       "      <td>0.580393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ensambler 2</td>\n",
       "      <td>0.621946</td>\n",
       "      <td>0.429459</td>\n",
       "      <td>0.584322</td>\n",
       "      <td>0.682494</td>\n",
       "      <td>0.625598</td>\n",
       "      <td>0.577126</td>\n",
       "      <td>0.586824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ensambler 3</td>\n",
       "      <td>0.614624</td>\n",
       "      <td>0.723380</td>\n",
       "      <td>0.801633</td>\n",
       "      <td>0.402776</td>\n",
       "      <td>0.611634</td>\n",
       "      <td>0.621585</td>\n",
       "      <td>0.629272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Search query</td>\n",
       "      <td>0.593194</td>\n",
       "      <td>0.586986</td>\n",
       "      <td>0.464791</td>\n",
       "      <td>0.391598</td>\n",
       "      <td>0.786024</td>\n",
       "      <td>0.346385</td>\n",
       "      <td>0.528163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       System  Faithfulness  Answer Relevancy  \\\n",
       "0                     GPT-3.5      0.083333          0.810294   \n",
       "1                       GPT-4      0.666667          0.887922   \n",
       "2                       Naive      0.558016          0.659556   \n",
       "3                   Recursive      0.379104          0.744159   \n",
       "4       Chunk 500, overlap 0%      0.517857          0.088874   \n",
       "5       Chunk 500, overlap 5%      0.565476          0.218776   \n",
       "6      Chunk 500, overlap 10%      0.652778          0.230681   \n",
       "7      Chunk 500, overlap 15%      0.841746          0.535714   \n",
       "8      Chunk 500, overlap 20%      0.636090          0.486717   \n",
       "9      Chunk 1000, overlap 0%      0.364984          0.852743   \n",
       "10     Chunk 1000, overlap 5%      0.398970          0.648575   \n",
       "11    Chunk 1000, overlap 10%      0.362868          0.770050   \n",
       "12    Chunk 1000, overlap 15%      0.544835          0.650798   \n",
       "13    Chunk 1000, overlap 20%      0.642857          0.741209   \n",
       "14     Chunk 2000, overlap 0%      0.346149          0.829901   \n",
       "15     Chunk 2000, overlap 5%      0.686905          0.817278   \n",
       "16    Chunk 2000, overlap 10%      0.665873          0.815407   \n",
       "17    Chunk 2000, overlap 15%      0.739286          0.801774   \n",
       "18    Chunk 2000, overlap 20%      0.758210          0.526721   \n",
       "19     Chunk 3000, overlap 0%      0.336367          0.707624   \n",
       "20     Chunk 3000, overlap 5%      0.336423          0.794321   \n",
       "21    Chunk 3000, overlap 10%      0.387645          0.713186   \n",
       "22    Chunk 3000, overlap 15%      0.788360          0.741365   \n",
       "23    Chunk 3000, overlap 20%      0.807540          0.677429   \n",
       "24  Chunk size 2000, 20%, K=2      0.411970          0.282099   \n",
       "25  Chunk size 2000, 20%, K=3      0.261245          0.443421   \n",
       "26  Chunk size 2000, 20%, K=5      0.740741          0.806737   \n",
       "27  Chunk size 2000, 20%, K=6      0.447991          0.224892   \n",
       "28  Parent Retriever 1000-200      0.408467          0.668263   \n",
       "29   Parent Retriever 500-100      0.529343          0.496213   \n",
       "30  Parent Retriever 1500-200      0.668834          0.381221   \n",
       "31                        MMR      0.722222          0.611605   \n",
       "32                       BM25      0.458274          0.461641   \n",
       "33                Ensambler 1      0.478271          0.703551   \n",
       "34                Ensambler 2      0.621946          0.429459   \n",
       "35                Ensambler 3      0.614624          0.723380   \n",
       "36               Search query      0.593194          0.586986   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.793104            0.256609   \n",
       "1                 NaN             NaN           0.801728            0.287099   \n",
       "2            0.787525        0.514306           0.280777            0.529137   \n",
       "3            0.721819        0.383333           0.288185            0.674370   \n",
       "4            0.782310        0.277026           0.513533            0.580632   \n",
       "5            0.801542        0.346715           0.585408            0.657749   \n",
       "6            0.793275        0.326791           0.746519            0.667383   \n",
       "7            0.107143        0.812384           0.341578            0.578127   \n",
       "8            0.237498        0.684846           0.471942            0.315999   \n",
       "9            0.731757        0.379630           0.288185            0.671611   \n",
       "10           0.641253        0.398148           0.383179            0.570392   \n",
       "11           0.729893        0.379630           0.288185            0.671417   \n",
       "12           0.793733        0.519767           0.277074            0.531771   \n",
       "13           0.412963        0.233333           0.795649            0.266531   \n",
       "14           0.662909        0.379630           0.288185            0.674888   \n",
       "15           0.407407        0.300000           0.790828            0.244493   \n",
       "16           0.374074        0.300000           0.787232            0.229308   \n",
       "17           0.379630        0.300000           0.791637            0.306600   \n",
       "18           0.722255        0.730503           0.456451            0.237525   \n",
       "19           0.672505        0.379630           0.288185            0.674785   \n",
       "20           0.669045        0.357407           0.288185            0.674978   \n",
       "21           0.734147        0.379630           0.291758            0.674870   \n",
       "22           0.374074        0.300000           0.792157            0.306730   \n",
       "23           0.357407        0.266667           0.793699            0.264258   \n",
       "24           0.579457        0.266667           0.326902            0.593221   \n",
       "25           0.594656        0.327778           0.273538            0.675780   \n",
       "26           0.395926        0.116667           0.793101            0.261457   \n",
       "27           0.821103        0.390395           0.701033            0.749447   \n",
       "28           0.598895        0.679993           0.648950            0.431832   \n",
       "29           0.604886        0.529791           0.428873            0.621292   \n",
       "30           0.648756        0.497418           0.475129            0.595250   \n",
       "31           0.395734        0.123333           0.786041            0.216510   \n",
       "32           0.326519        0.312199           0.585142            0.330468   \n",
       "33           0.671269        0.673675           0.566497            0.389093   \n",
       "34           0.584322        0.682494           0.625598            0.577126   \n",
       "35           0.801633        0.402776           0.611634            0.621585   \n",
       "36           0.464791        0.391598           0.786024            0.346385   \n",
       "\n",
       "     Average  \n",
       "0   0.390557  \n",
       "1   0.507236  \n",
       "2   0.554886  \n",
       "3   0.531828  \n",
       "4   0.460039  \n",
       "5   0.529278  \n",
       "6   0.569571  \n",
       "7   0.536115  \n",
       "8   0.472182  \n",
       "9   0.548152  \n",
       "10  0.506753  \n",
       "11  0.533674  \n",
       "12  0.552996  \n",
       "13  0.515424  \n",
       "14  0.530277  \n",
       "15  0.541152  \n",
       "16  0.528649  \n",
       "17  0.553154  \n",
       "18  0.571944  \n",
       "19  0.509849  \n",
       "20  0.520060  \n",
       "21  0.530206  \n",
       "22  0.550448  \n",
       "23  0.527833  \n",
       "24  0.410053  \n",
       "25  0.429403  \n",
       "26  0.519105  \n",
       "27  0.555810  \n",
       "28  0.572733  \n",
       "29  0.535066  \n",
       "30  0.544435  \n",
       "31  0.475908  \n",
       "32  0.412374  \n",
       "33  0.580393  \n",
       "34  0.586824  \n",
       "35  0.629272  \n",
       "36  0.528163  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average = dictionary(result_search_query)\n",
    "    # Create a dictionary to store the results\n",
    "system_results = {\n",
    "        \"System\": \"Search query\",\n",
    "        \"Faithfulness\": result_search_query[\"faithfulness\"],\n",
    "        \"Answer Relevancy\": result_search_query[\"answer_relevancy\"],\n",
    "        \"Context Precision\": result_search_query[\"context_precision\"],\n",
    "        \"Context Recall\": result_search_query[\"context_recall\"],\n",
    "        \"Answer Similarity\": result_search_query[\"answer_similarity\"],\n",
    "        \"Answer Correctness\": result_search_query[\"answer_correctness\"],\n",
    "        \"Average\": average\n",
    "    }\n",
    "df_result_search_query = pd.DataFrame([system_results])\n",
    "results_df = pd.concat([results_df, df_result_search_query], ignore_index=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change model to GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ensambler 3</td>\n",
       "      <td>0.614624</td>\n",
       "      <td>0.723380</td>\n",
       "      <td>0.801633</td>\n",
       "      <td>0.402776</td>\n",
       "      <td>0.611634</td>\n",
       "      <td>0.621585</td>\n",
       "      <td>0.629272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ensambler 2</td>\n",
       "      <td>0.621946</td>\n",
       "      <td>0.429459</td>\n",
       "      <td>0.584322</td>\n",
       "      <td>0.682494</td>\n",
       "      <td>0.625598</td>\n",
       "      <td>0.577126</td>\n",
       "      <td>0.586824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ensambler 1</td>\n",
       "      <td>0.478271</td>\n",
       "      <td>0.703551</td>\n",
       "      <td>0.671269</td>\n",
       "      <td>0.673675</td>\n",
       "      <td>0.566497</td>\n",
       "      <td>0.389093</td>\n",
       "      <td>0.580393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "35  Ensambler 3      0.614624          0.723380           0.801633   \n",
       "34  Ensambler 2      0.621946          0.429459           0.584322   \n",
       "33  Ensambler 1      0.478271          0.703551           0.671269   \n",
       "\n",
       "    Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "35        0.402776           0.611634            0.621585  0.629272  \n",
       "34        0.682494           0.625598            0.577126  0.586824  \n",
       "33        0.673675           0.566497            0.389093  0.580393  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_highest = results_df.nlargest(3, \"Average\")\n",
    "top_3_highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Who wrote 'The Hanging Tree' song?\n",
      "Warning: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}on the following question: Who arrives at the District 12 Peacekeeper base soon after Coriolanus in 'The Ballad of Songbirds and Snakes'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   1%|          | 1/90 [00:01<01:39,  1.12s/it]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating:   2%|▏         | 2/90 [00:04<03:47,  2.59s/it]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  21%|██        | 19/90 [00:05<00:14,  4.87it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  73%|███████▎  | 66/90 [00:06<00:00, 27.66it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  83%|████████▎ | 75/90 [00:11<00:02,  5.32it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  98%|█████████▊| 88/90 [00:39<00:02,  1.44s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  99%|█████████▉| 89/90 [02:39<00:09,  9.33s/it]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [03:13<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.6301538266175434\n",
      "This is the new best value!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensambler 3, GPT-4</td>\n",
       "      <td>0.703828</td>\n",
       "      <td>0.777438</td>\n",
       "      <td>0.664803</td>\n",
       "      <td>0.405762</td>\n",
       "      <td>0.583367</td>\n",
       "      <td>0.645725</td>\n",
       "      <td>0.630154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "0  Ensambler 3, GPT-4      0.703828          0.777438           0.664803   \n",
       "\n",
       "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "0        0.405762           0.583367            0.645725  0.630154  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ensemble3_gpt4, results_df = run_and_evaluate(f\"Ensambler 3, GPT-4\", ensemble_retriever_3, prompt, llm_gpt4, results_df)\n",
    "result_ensemble3_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   1%|          | 1/90 [00:00<01:11,  1.25it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:   2%|▏         | 2/90 [00:01<00:56,  1.56it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:   3%|▎         | 3/90 [00:01<00:52,  1.67it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'low'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  11%|█         | 10/90 [00:05<00:35,  2.26it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  84%|████████▍ | 76/90 [00:07<00:00, 29.27it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  91%|█████████ | 82/90 [00:11<00:01,  5.28it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating:  99%|█████████▉| 89/90 [00:33<00:00,  4.85it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': True, 'severity': 'medium'}, 'violence': {'filtered': False, 'severity': 'safe'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [02:43<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5801561033439621\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensambler 2, GPT-4</td>\n",
       "      <td>0.611953</td>\n",
       "      <td>0.700445</td>\n",
       "      <td>0.549128</td>\n",
       "      <td>0.66728</td>\n",
       "      <td>0.484443</td>\n",
       "      <td>0.467687</td>\n",
       "      <td>0.580156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "0  Ensambler 2, GPT-4      0.611953          0.700445           0.549128   \n",
       "\n",
       "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "0         0.66728           0.484443            0.467687  0.580156  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ensemble2_gpt4, results_df = run_and_evaluate(f\"Ensambler 2, GPT-4\", ensemble_retriever_2, prompt, llm_gpt4, results_df)\n",
    "result_ensemble2_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/90 [00:00<?, ?it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'low'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:   1%|          | 1/90 [00:00<01:07,  1.32it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'low'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  14%|█▍        | 13/90 [00:06<00:27,  2.85it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating:  84%|████████▍ | 76/90 [00:07<00:00, 25.88it/s]Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Runner in Executor raised an exception\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766\", 'type': None, 'param': 'prompt', 'code': 'content_filter', 'status': 400, 'innererror': {'code': 'ResponsibleAIPolicyViolation', 'content_filter_result': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': True, 'severity': 'medium'}}}}}\n",
      "Evaluating: 100%|██████████| 90/90 [00:13<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.5811690432381572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ensambler 1, GPT-4</td>\n",
       "      <td>0.541574</td>\n",
       "      <td>0.33571</td>\n",
       "      <td>0.596699</td>\n",
       "      <td>0.712009</td>\n",
       "      <td>0.716593</td>\n",
       "      <td>0.58443</td>\n",
       "      <td>0.581169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "0  Ensambler 1, GPT-4      0.541574           0.33571           0.596699   \n",
       "\n",
       "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "0        0.712009           0.716593             0.58443  0.581169  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ensemble1_gpt4, results_df = run_and_evaluate(f\"Ensambler 1, GPT-4\", ensemble_retriever_1, prompt, llm_gpt4, results_df)\n",
    "result_ensemble1_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"../ballad/results/results_qa.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
