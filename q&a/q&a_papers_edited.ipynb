{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain ragas datasets cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "dotenv.load_dotenv()\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.tracebacklimit = 0\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_VERSION = os.environ.get(\"OPENAI_API_VERSION\")\n",
    "AZURE_OPENAI_ENDPOINT = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\")\n",
    "OPENAI_DEPLOYMENT = os.environ.get(\"OPENAI_DEPLOYMENT\")\n",
    "EMBEDDING_MODEL = os.environ.get(\"EMBEDDING_MODEL\")\n",
    "EMBEDDING_DEPLOYMENT = os.environ.get(\"EMBEDDING_DEPLOYMENT\")\n",
    "OPENAI_MODEL_GPT4 = os.environ.get(\"OPENAI_MODEL_GPT4\")\n",
    "OPENAI_DEPLOYMENT_GPT4 = os.environ.get(\"OPENAI_DEPLOYMENT_GPT4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"In the framework for implementing generative AI services using the Retrieval-Augmented Generation (RAG) model, what specific open-source products are chosen for vectorized databases?\",\n",
    "    \"What does CRUD stand for when it comes to RAG?\",\n",
    "    \"What evaluation metrics can be used to evaluate RAG systems?\",\n",
    "    \"What are the four fundamental abilities required for Retrieval-Augmented Generation (RAG) according to Chen et al., and how are they evaluated in the Retrieval-Augmented Generation Benchmark (RGB)?\",\n",
    "    \"What method does the Thulke et al. propose to address the computational complexity issue in the knowledge selection task, and how does it achieve a significant reduction in computation time?\",\n",
    "    \"How does the proposed system by Matei Bucur address the limitations related to the availability of relevant external documents for form filling in the broader context?\",\n",
    "    \"What embedding methods can be used for RAG systems?\",\n",
    "    \"According to the experimental results by Wu et al., how much did the second strategy, based on the absence of detected hallucination spans, reduce the hallucination rate for the GPT-3.5-Turbo and GPT-4 models in comparison to random selection?\",\n",
    "    \"What are some of the best retrieval strategies in RAG systems?\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    [\"RAG stands for Retrieval Augmented Generation. It is a technique developed to address the challenge of requiring external data for generating more cohesive answers. The approach involves fetching and incorporating information into the prompt, allowing the model to generate responses on subjects and data not encountered during training. This aims to reduce the occurrence of hallucinations in the model's outputs.\"],\n",
    "    [\"Chroma DB and FAISS.\"],\n",
    "    [\"In 'CREATE', the system improves the input text by adding relevant information from external sources, making creative outputs such as poetry, stories, or code. In 'READ', the system uses external knowledge retrieval to answer questions, solving problems in question-answering, dialogue, and reasoning, and increasing understanding of the input text. In 'UPDATE', the system fixes errors in the input text using retrieved content, correcting spelling, grammar or factual errors to make the text better. In 'DELETE', the system simplifies the input by improving retrieval results, removing unnecessary details, and doing tasks like text summarization or simplification\"],\n",
    "    [\"Accuracy, Rejection rate, Error detection rate, Baseline model Reflexion, Precision of reasoning, Multi-hop reasoning accuracy, Mean Recriprocal Rank (MRR@k), Recall@k, Relative Maximum Score, Mean Average Precision at K (MAP@K), Mean Reciprocal Rank at K (MRR@K),  Hit Rate at K (Hit@K), Exact Match (EM), F1 score, Precision,  Recall, perplexity (PPL), GLEU, BLEU, ROUGE, METEOR,  BERTScore, EM score (Exact Match score), RAGQuestEval, Faithfulness, Recall@1 (R@1), BLEU-1, METEOR, ROUGE-L\"],\n",
    "    [\"The four fundamental abilities required for RAG, as mentioned in the paper, are Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness. These abilities are evaluated in RGB through four separate testbeds, each focusing on one of these abilities. The testbeds are constructed based on the common challenges in RAG and include instances that assess the performance of large language models in terms of noise robustness, negative rejection, information integration, and counterfactual robustness.\"],\n",
    "    [\"They propose two methods to address the computational complexity issue in the knowledge selection task. The first method involves hierarchical selection, which splits the task into stages, reducing the computational cost significantly. The second method employs Dense Knowledge Retrieval, utilizing siamese sequence embedding networks for efficient document retrieval. This method achieves a more than 100x reduction in computation time compared to the baseline, albeit with a 5-6% degradation in R@1 compared to the hierarchical selection method.\"],\n",
    "    [\"The proposed system addresses the limitations related to the availability of relevant external documents for form filling in the broader context by encouraging users to upload all documents relevant to the specific form-filling task, such as FAQs, policies, guidelines, and rules. These documents are used to create a search index, and the retrieval mechanism leverages this index to generate form completions. Additionally, the system allows users to provide a brief description of the intent and purpose of the form, adding extra short-form documents to enhance the understanding of the task. This approach aims to ensure that the system has access to the necessary information, making it more adaptable to different form-filling scenarios.\"],\n",
    "    [\"Custom ADA-002, BM25, Customization techniques involving adaptations like Multiple Negative Ranking Loss and Mean Squared Error (MSE) Loss, text-embedding-ada-002, text-search-ada-query-001, llm-embedder, bge-large-en-v1.5, jina-embeddings-v2-base-en, e5-base-v2,  instructor-large, voyage-02.\"],\n",
    "    [\"The second strategy, based on the absence of detected hallucination spans, achieved a reduction in hallucination rates of 52.9% for GPT-3.5-Turbo and 41% for GPT-4 models, compared to random selection\"],\n",
    "    [\"Hybrid search, reranker, multi-time retrieval approach, vector search, BM25, hierarchical selection, prompt engineering.\"]\n",
    "]\n",
    "answers_llm = []\n",
    "contexts_llm = [[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"],[\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_client = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=EMBEDDING_DEPLOYMENT,\n",
    "    openai_api_version=OPENAI_API_VERSION)\n",
    "llm = AzureChatOpenAI(model_name=OPENAI_MODEL, azure_deployment=OPENAI_DEPLOYMENT,temperature=0)\n",
    "llm_gpt4 = AzureChatOpenAI(model_name=OPENAI_MODEL_GPT4, azure_deployment=OPENAI_DEPLOYMENT_GPT4,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_llm(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  \n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_rag(questions, answers, contexts, ground_truths):\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truths\": ground_truths\n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    azure_configs = {\n",
    "        \"base_url\": AZURE_OPENAI_ENDPOINT,\n",
    "        \"model_deployment\": OPENAI_DEPLOYMENT,\n",
    "        \"model_name\": OPENAI_MODEL,\n",
    "        \"embedding_deployment\": EMBEDDING_DEPLOYMENT,\n",
    "        \"embedding_name\": EMBEDDING_MODEL,  # most likely\n",
    "    }\n",
    "\n",
    "    azure_model = AzureChatOpenAI(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"model_deployment\"],\n",
    "        model=azure_configs[\"model_name\"],\n",
    "        validate_base_url=False,\n",
    "    )\n",
    "\n",
    "    azure_embeddings = AzureOpenAIEmbeddings(\n",
    "        openai_api_version=OPENAI_API_VERSION,\n",
    "        azure_endpoint=azure_configs[\"base_url\"],\n",
    "        azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "        model=azure_configs[\"embedding_name\"],\n",
    "    )\n",
    "    result = evaluate(\n",
    "        dataset = dataset, \n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            answer_similarity,\n",
    "            answer_correctness,\n",
    "        ], \n",
    "        llm=azure_model, \n",
    "        embeddings=azure_embeddings,\n",
    "        raise_exceptions=False,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"System\", \"Faithfulness\", \"Answer Relevancy\", \"Context Precision\", \"Context Recall\", \"Answer Similarity\", \"Answer Correctness\"]\n",
    "results_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_average = 0\n",
    "def find_highest(average_score):\n",
    "    global max_average\n",
    "    if average_score > max_average:\n",
    "        max_average = average_score\n",
    "        print(\"This is the new best value!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary(result):\n",
    "    dict_result = dict(result)\n",
    "    average_score = sum(dict_result.values()) / len(dict_result)\n",
    "    print(f\"The average score is: {average_score}\")\n",
    "    find_highest(average_score)\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(system_name, questions, answers, contexts, ground_truths):\n",
    "    result = evaluation_rag(questions, answers, contexts, ground_truths)\n",
    "    average = dictionary(result)\n",
    "    # Create a dictionary to store the results\n",
    "    system_results = {\n",
    "        \"System\": system_name,\n",
    "        \"Faithfulness\": result[\"faithfulness\"],\n",
    "        \"Answer Relevancy\": result[\"answer_relevancy\"],\n",
    "        \"Context Precision\": result[\"context_precision\"],\n",
    "        \"Context Recall\": result[\"context_recall\"],\n",
    "        \"Answer Similarity\": result[\"answer_similarity\"],\n",
    "        \"Answer Correctness\": result[\"answer_correctness\"],\n",
    "        \"Average\": average\n",
    "    }\n",
    "    df_system_results = pd.DataFrame([system_results])\n",
    "    return df_system_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LLM(system_name, questions, answers, contexts, ground_truths):\n",
    "    result = evaluation_rag(questions, answers, contexts, ground_truths)\n",
    "    average = dictionary(result)\n",
    "    # Create a dictionary to store the results\n",
    "    system_results = {\n",
    "        \"System\": system_name,\n",
    "        \"Faithfulness\": result[\"faithfulness\"],\n",
    "        \"Answer Relevancy\": result[\"answer_relevancy\"],\n",
    "        \"Context Precision\": np.nan,\n",
    "        \"Context Recall\": np.nan,\n",
    "        \"Answer Similarity\": result[\"answer_similarity\"],\n",
    "        \"Answer Correctness\": result[\"answer_correctness\"],\n",
    "        \"Average\": average\n",
    "    }\n",
    "    df_llm_results = pd.DataFrame([system_results])\n",
    "    return df_llm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General answers by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm}\n",
    ")\n",
    "llm_chain_gpt4 =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt | llm_gpt4}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in questions:\n",
    "    response = llm_chain.invoke({\"question\": query})\n",
    "    answers_llm.append(response[\"response\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:15<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.6381804570446418\n",
      "This is the new best value!\n",
      "    System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0  GPT-3.5           1.0          0.581812                NaN             NaN   \n",
      "\n",
      "   Answer Similarity  Answer Correctness  Average  \n",
      "0           0.863942            0.483329  0.63818  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sigitalapina\\AppData\\Local\\Temp\\ipykernel_23856\\800954320.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, llm_results], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "llm_results = evaluate_LLM(\"GPT-3.5\", questions, answers_llm, contexts_llm, ground_truths)\n",
    "results_df = pd.concat([results_df, llm_results], ignore_index=True)\n",
    "print(llm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:16<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.6302111460020389\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0  GPT-4           1.0          0.567018                NaN             NaN   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0            0.86454            0.449709  0.630211  \n"
     ]
    }
   ],
   "source": [
    "answers_llm_gpt4 = []\n",
    "for query in questions:\n",
    "    response = llm_chain_gpt4.invoke({\"question\": query})\n",
    "    answers_llm_gpt4.append(response[\"response\"].content)\n",
    "llm_results_gpt4 = evaluate_LLM(\"GPT-4\",questions, answers_llm_gpt4, contexts_llm, ground_truths)\n",
    "results_df = pd.concat([results_df, llm_results_gpt4], ignore_index=True)\n",
    "print(llm_results_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.581812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.863942</td>\n",
       "      <td>0.483329</td>\n",
       "      <td>0.638180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.567018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.864540</td>\n",
       "      <td>0.449709</td>\n",
       "      <td>0.630211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
       "0  GPT-3.5           1.0          0.581812                NaN             NaN   \n",
       "1    GPT-4           1.0          0.567018                NaN             NaN   \n",
       "\n",
       "   Answer Similarity  Answer Correctness   Average  \n",
       "0           0.863942            0.483329  0.638180  \n",
       "1           0.864540            0.449709  0.630211  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"User input {question}. \n",
    "Context {context}.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_chain(prompt, retriever, llm):\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    return retrieval_augmented_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../papers', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "db_naive = Chroma.from_documents(chunks, embeddings_client, persist_directory = \"../papers/vectordb-edit/naive\")\n",
    "db_naive.persist()\n",
    "retriever_naive = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_naive = Chroma(persist_directory = \"../papers/vectordb-edit/naive\", embedding_function=embeddings_client)\n",
    "retriever_naive = db_naive.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_naive = []\n",
    "contexts_naive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_naive, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_naive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_naive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_naive.append(\"No answer\")\n",
    "        context_full = retriever_naive.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_naive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['111:2 Lyu, et al.\\nof various components of the RAG system, such as the retriever, context length, knowledge base construction,\\nand LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios1.\\nCCS Concepts: •Computing methodologies →Natural language generation ;•Information systems\\n→Information retrieval .\\nAdditional Key Words and Phrases: Retrieval-Augmented Generation, Large Language Models, Evaluation\\nACM Reference Format:\\nYuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu,\\nEnhong Chen, Yi Luo, Peng Cheng, Haiying Deng, Zhonghao Wang, and Zijia Lu. 2018. CRUD-RAG: A\\nComprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models. J. ACM\\n37, 4, Article 111 (August 2018), 28 pages. https://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nRetrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge\\nsources to enhance the text generation capabilities of large language models(LLMs). It retrieves\\nrelevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the\\ninput. With the help of external knowledge, LLMs can generate more accurate and credible responses\\nand effectively address challenges such as outdated knowledge [ 17], hallucinations [ 3,8,31,54],\\nand lack of domain expertise [ 26,40]. Therefore, RAG technology is attracting increasing attention.\\nAlthough the effectiveness of retrieval-augmented strategies has been proven through extensive\\npractice, their implementation still requires a significant amount of tuning. The overall performance\\nof the RAG system is affected by multiple factors, such as the retrieval model, construction of the\\nexternal knowledge base, and language model. Therefore, automatic evaluation of RAG systems is\\ncrucial. Currently, there are only a few existing benchmarks for evaluating RAG performance, as\\ncreating high-quality datasets and experimenting on them entail significant costs. These benchmarks\\ncan be classified into two types: reference-required and reference-free evaluation. Reference-free\\nevaluation frameworks, such as RAGAS [ 12] and ARES [ 38], use LLM-generated data to evaluate\\nRAG systems on contextual relevance, faithfulness, and informativeness. These frameworks do\\nnot depend on ground truth references, but only assess the coherence of the generated text with\\nthe retrieved context. This approach may be unreliable if the retrieved external information is\\nlow-quality.\\nConsequently, reference-required evaluations remain the predominant method for assessing RAG\\nsystems. Existing benchmarks for reference-required evaluations, such as RGB [ 7] and NQ [ 23]. do\\nhave their limitations. First, they all rely on question answering tasks to measure the performance\\nof RAG systems. Question answering is not the only RAG application scenario, and an optimization\\nstrategy that works well for question answering may not be generalized to other scenarios. Thus,\\nthese benchmarks may not capture the full potential of RAG systems. Second, in the experiments,\\ncurrent evaluations usually focus on evaluating the LLM part of the RAG pipeline, while ignoring\\nthe retrieval model and external knowledge base construction. These components are also crucial\\nfor RAG systems, as they determine the quality and relevance of retrieved passages and external\\nknowledge. Therefore, a comprehensive evaluation of the RAG system may not be obtained using\\nany existing benchmarks.\\nTo evaluate the performance of RAG in different application scenarios, we need a comprehensive\\nbenchmark that covers more than just the question answering task. Lewis et al . [25] argue that the\\ncore of RAG systems is their interactive way of combining LLMs with external knowledge sources.\\nAnd following [ 22], we can group any interaction with external knowledge sources into four basic\\nactions: create, read, update, and delete, which are also known as CRUD actions [ 42]. Therefore,\\n1The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.', 'certain. Initial studies [Wang et al. , 2023b ]have begun to ad-\\ndress this, yet the parameter count in RAG models still lags\\nbehind that of LLMs. The possibility of an Inverse Scaling\\nLaw9, where smaller models outperform larger ones, is par-\\nticularly intriguing and merits further investigation.\\nProduction-Ready RAG . RAG’s practicality and alignment\\nwith engineering requirements have facilitated its adoption.\\nHowever, enhancing retrieval efficiency, improving document\\nrecall in large knowledge bases, and ensuring data secu-\\nrity—such as preventing inadvertent disclosure of document\\nsources or metadata by LLMs—are critical engineering chal-\\nlenges that remain to be addressed [Alon et al. , 2022 ].\\nModality Extension of RAG\\nRAG has transcended its initial text-based question-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage . RA-CM3 [Yasunaga et al. , 2022 ]stands as a pio-\\nneering multimodal model of both retrieving and generating\\ntext and images. BLIP-2 [Liet al. , 2023a ]leverages frozen\\nimage encoders alongside LLMs for efficient visual language\\npre-training, enabling zero-shot image-to-text conversions.\\nThe “Visualize Before You Write” method [Zhuet al. , 2022 ]\\nemploys image generation to steer the LM’s text generation,\\nshowing promise in open-ended text generation tasks.\\nAudio and Video . The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [Zhao et al. , 2022 ]. UEOP marks\\na significant advancement in end-to-end automatic speech\\nrecognition by incorporating external, offline strategies for\\nvoice-to-text conversion [Chan et al. , 2023 ]. Additionally,\\nKNN-based attention fusion leverages audio embeddings and\\nsemantically related text embeddings to refine ASR, thereby\\naccelerating domain adaptation. Vid2Seq augments language\\nmodels with specialized temporal markers, facilitating the\\nprediction of event boundaries and textual descriptions within\\na unified output sequence [Yang et al. , 2023a ].\\nCode . RBPS [Nashid et al. , 2023 ]excels in small-scale\\nlearning tasks by retrieving code examples that align with de-\\nvelopers’ objectives through encoding and frequency analy-\\nsis. This approach has demonstrated efficacy in tasks such as\\ntest assertion generation and program repair. For structured\\nknowledge, the CoK method [Liet al. , 2023c ]first extracts\\nfacts pertinent to the input query from a knowledge graph,\\nthen integrates these facts as hints within the input, enhancing\\nperformance in knowledge graph question-answering tasks.\\n8.2 Ecosystem of RAG\\nDownstream Tasks and Evaluation\\nRAG has shown considerable promise in enriching language\\nmodels with the capacity to handle intricate queries and pro-\\nduce detailed responses by leveraging extensive knowledge\\nbases. Empirical evidence suggests that RAG excels in a\\nvariety of downstream tasks, including open-ended question\\nanswering and fact verification. The integration of RAG not\\nonly bolsters the precision and relevance of responses but also\\ntheir diversity and depth.\\n9https://github.com/inverse-scaling/prizeThe scalability and versatility of RAG across multiple do-\\nmains warrant further investigation, particularly in special-\\nized fields such as medicine, law, and education. In these ar-\\neas, RAG could potentially reduce training costs and enhance\\nperformance compared to traditional fine-tuning approaches\\nin professional domain knowledge question answering.\\nConcurrently, refining the evaluation framework for RAG\\nis essential to maximize its efficacy and utility across different\\ntasks. This entails the development of nuanced metrics and\\nassessment tools that can gauge aspects such as contextual\\nrelevance, creativity of content, and non-maleficence.\\nFurthermore, improving the interpretability of RAG-driven\\nmodels continues to be a key goal. Doing so would allow\\nusers to understand the reasoning behind the responses gener-\\nated by the model, thereby promoting trust and transparency\\nin the use of RAG applications.\\nTechnical Stack\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.\\nEmerging technical stacks, while not as feature-rich as\\nLangChain and LLamaIndex, distinguish themselves with\\nspecialized offerings. For instance, Flowise AI10prioritizes a\\nlow-code approach, enabling users to deploy AI applications,\\nincluding RAG, through a user-friendly drag-and-drop inter-\\nface. Other technologies like HayStack, Meltano11, and Co-\\nhere Coral12are also gaining attention for their unique con-\\ntributions to the field.\\nIn addition to AI-focused providers, traditional software\\nand cloud service providers are expanding their offerings to\\ninclude RAG-centric services. Verba13from Weaviate is de-\\nsigned for personal assistant applications, while Amazon’s\\nKendra14provides an intelligent enterprise search service, al-\\nlowing users to navigate through various content repositories\\nusing built-in connectors. During the evolution of the RAG\\ntechnology landscape, there has been a clear divergence to-\\nwards different specializations, such as: 1) Customization.\\nTailoring RAG to meet a specific requirements. 2) Simpli-\\nfication. Making RAG easier to use, thereby reducing the ini-\\ntial learning curve. 3) Specialization. Refining RAG to serve\\nproduction environments more effectively.\\nThe mutual growth of RAG models and their technical\\nstack is evident; technological advancements consistently es-\\ntablish new standards for the existing infrastructure. In turn,\\nenhancements to the technical stack drive the evolution of\\nRAG capabilities. The RAG toolkit is converging into a foun-\\ndational technical stack, laying the groundwork for advanced\\nenterprise applications. However, the concept of a fully in-\\ntegrated, comprehensive platform remains on the horizon,\\npending further innovation and development.\\n10https://flowiseai.com\\n11https://meltano.com\\n12https://cohere.com/coral\\n13https://github.com/weaviate/Verba\\n14https://aws.amazon.com/cn/kendra/', 'CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:3\\nFig. 1. We have classified the application scenarios of RAG into four primary aspects: Create, Read, Update,\\nand Delete. The figure provides an illustrative example for each category, showcasing the wide-ranging\\npotential of RAG technology.\\nwe can use the CRUD framework to classify the RAG systems’ application scenarios. As shown in\\nFigure 1, each CRUD category demonstrates different capabilities of the RAG system:\\n•In \"CREATE \", the system improves the input text by adding relevant information from\\nexternal sources, making creative outputs such as poetry, stories, or code.\\n•In \"READ \", the system uses external knowledge retrieval to answer questions, solving\\nproblems in question-answering, dialogue, and reasoning, and increasing understanding of\\nthe input text.\\n•In \"UPDATE \", the system fixes errors in the input text using retrieved content, correcting\\nspelling, grammar or factual errors to make the text better.\\n•In \"DELETE \", the system simplifies the input by improving retrieval results, removing\\nunnecessary details, and doing tasks like text summarization or simplification.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.', 'Figure 6: RAG compared with other model optimization methods\\nand avoiding contradictions.\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively address-\\ning the core inquiry.\\nRequired Abilities\\nRAG evaluation also encompasses four abilities indicative of\\nits adaptability and efficiency: noise robustness, negative re-\\njection, information integration, and counterfactual robust-\\nness [Chen et al. , 2023b, Liu et al. , 2023b ]. These abilities\\nare critical for the model’s performance under various chal-\\nlenges and complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in re-\\nfraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\nInformation Integration evaluates the model’s proficiency\\nin synthesizing information from multiple documents to ad-\\ndress complex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.\\nContext relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating thequality of generation.\\nThe specific metrics for each evaluation aspect are summa-\\nrized in Table 2. It is essential to recognize that these metrics,\\nderived from related work, are traditional measures and do\\nnot yet represent a mature or standardized approach for quan-\\ntifying RAG evaluation aspects. Custom metrics tailored to\\nthe nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\n7.3 Evaluation Benchmarks and Tools\\nThis section delineates the evaluation framework for RAG\\nmodels, comprising benchmark tests and automated eval-\\nuation tools. These instruments furnish quantitative met-\\nrics that not only gauge RAG model performance but also\\nenhance comprehension of the model’s capabilities across\\nvarious evaluation aspects. Prominent benchmarks such as\\nRGB and RECALL [Chen et al. , 2023b, Liu et al. , 2023b ]\\nfocus on appraising the essential abilities of RAG mod-\\nels. Concurrently, state-of-the-art automated tools like RA-\\nGAS [Eset al. , 2023 ], ARES [Saad-Falcon et al. , 2023 ], and\\nTruLens8employ LLMs to adjudicate the quality scores.\\nThese tools and benchmarks collectively form a robust frame-\\nwork for the systematic evaluation of RAG models, as sum-\\nmarized in Table 3.\\n8https://www.trulens.org/trulens eval/core concepts ragtriad/'], [\"13 \\n In this section, we delve into the details of the RAG model, which is employed as an implementation method in this \\npaper. The RAG model stores inf ormation, leverages databases, and enhances the generation process through the \\ninclusion of relevant reference materials, ultimately leading to improved answer quality and reliability.  \\n \\n2.3.1  Architecture of the RAG Model   \\nThe RAG (Retrieval -Augmented Generation ) model is designed for text generation tasks, performing a process \\nthat involves retrieving information from given source data and utilizing that information to generate desired text. \\nFig 6 illustrates the data processing pipeline for RAG usage, involving  breaking down the original data into \\nsmaller chunks and converting text data into numerical vectors through embedding, which are then stored in a \\nvector repository  [13]. \\n \\n• Source Data Collection and Preparation : Relevant source data is required for the model's training and \\nutilization. This data can include documents, web pages, news articles, etc. It forms the foundation for the \\nmodel to search for and generate content.  \\n• Chunking of Searchable Units: Source da ta is divided into smaller units known as chunks. Chunks are typically \\nsmall text fragments, such as sentences or paragraphs, making it easier to search for and utilize information at \\nthis granular level.  \\n• Embedding: Generated chunks undergo embedding, a process of converting text into meaningful vector \\nrepresentations. Pre -trained language models are often used to transform text into dense vectors, capturing the \\nmeaning and related information in vector form.  \\n• Construction of Vector Database: A vector da tabase is built based on the embedded chunks. This database \\nrepresents the positions of each chunk within the vector space, enabling efficient retrieval and similarity \\ncalculations.  \\n• Search and Information Integration: To retrieve information relevant to the context of the text to be generated, \\nappropriate chunks are searched within the vector database. The retrieved chunks are decoded back into \\noriginal text data to extract information, which is then utilized during the generation process.  \\n• Text Generati on: Using the retrieved information as a basis, text is generated. Users can specify the type, \\nlength, and linguistic style of the text to be generated. The RAG model is designed to seamlessly integrate \\ninformation retrieval and generation processes, aimin g to produce more accurate and meaningful text outputs.  \\n \\n \\n \\nFig 6 RAG Model  Diagram (Microsoft, 2023)\", \"19 \\n III. Methods  \\nIn this chapter, we outline a comprehensive framework for implementing generative AI services by effectively \\ncombining and orchestrating various technologies within the previously discussed generative AI technology stack. We \\nwill describe how each component o f the framework is applied to achieve efficient and harmonious integration of the \\ndiverse generative AI technologies. The following sections detail the implementation process for each technology \\ncomponent within the framework:  \\n \\n3.1 Framework for Implementi ng Generative AI Services using RAG Model   \\nBased on previous research, we have designed a comprehensive framework for implementing generative AI \\nservices using the Retrieval -Augmented Generation (RAG) model. The framework encompasses a series of \\nprocedures  and core functionalities, as depicted in Fig 10. The diagram conceptually illustrates the process of utilizing \\nLLMs to retrieve information from documents and outlines the key steps involved. The major components of the \\nframework are detailed below.  \\n \\nFig 10 Framework for Implementing Generative AI Services using RAG Model  \\n \\n3.2 RAG model and LangChain integration implementation process  \\nWithin the framework for implementing generative AI services, various solutions exist for each step of the process. \\nConsider ing the awareness and cost aspects of these solutions, this study has proposed a framework that primarily \\nrelies on open -source products in alignment with the findings from Chapter 2. In this context, the framework takes \\nadvantage of both OpenAI's propriet ary models and open -source models to trigger the generative AI capabilities of \\nLLMs. The composition of the framework is illustrated in Fig 10. \\nFor the overall orchestration framework, LangChain is adopted, while specific tasks like chunking and embedding \\nare accomplished using a combination of OpenAI models and GPT4All. The vector repository is facilitated through \\nChroma DB, chosen for its ease of implementation. Regarding the LLM component, OpenAI's GPT -3.5-turbo model \\nand GPT4All are integrated, allowing  developers to harness diverse choices to achieve optimal development \\noutcomes.\", \"20 \\n 3.2.1  RAG based implementation procedure  \\nThe RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search -augmented \\ngenerative model used to retrieve and generate responses based on information relevant to given questions or topics. \\nEach step follows the procedure outlined in Fig 9.  \\n1) Source Data Collection and Extraction: During the data collection phase, both structured and unstructured \\ndata are gathered. Structured data is stored in standardized formats such as CSV, JSON, or XML, while \\nunstructured data is stored in formats like PDF, TXT, HTML, images, and videos. Preparatory materials \\nrelated to the task, such as regulations, user manuals, and terms and c onditions, are loaded into \\nLangChain using the LangChain module.  \\n2) Chunk Generation: Source data is processed to split it into smaller units known as chunks. These chunks \\ntypically consist of sentences or paragraphs, serving as smaller text fragments that ca n be used to search \\nand retrieve information from LLM. LangChain's module is utilized to split data into chunks that are suitable \\nfor retrieval.  \\n3) Embedding: The generated chunk -level text data is transformed into numerical vector representations. This \\nstep involves mapping words or sentences to vectors, and libraries provided by OpenAI or GPT4All can be \\nemployed for this purpose.  \\n4) Building the Vector Database: Based on the embedded chunks, the vector database is constructed. This \\ndatabase represents the posit ions of each chunk in the vector space, facilitating efficient search and \\nsimilarity calculations. Typically, content from each document is included, embeddings and documents are \\nstored in the vector repository, and documents are indexed using embeddings. Tools like Chroma or FAISS \\nfor vector indexing can be used.  \\n5) Integration of Prompt and Search Results: This step involves searching for information based on the \\nprompted question and integrating relevant information. To search for contextually relevant info rmation \\nbased on the prompt, appropriate chunks are retrieved from the vector database. These retrieved chunks \\nare then sent to LLM to aid in the response generation process. Various search engines available within \\nLangChain for vector store similarity sea rch are utilized.  \\n6) Answer Generation: Using the retrieved information as a basis, the response text is generated. At this stage, \\nthe type, length, and linguistic style of the generated text can be specified. LLM, such as OpenAI's \\nGPT-3.5-turbo model or GPT4 All, uses the similarity search module in LangChain to retrieve relevant \\ndocuments and generate responses.  \\nThe RAG -based implementation procedure outlined above illustrates how the RAG model, in combination with \\nLangChain, can be effectively integrated int o the generative AI service framework. This comprehensive \\napproach covers data collection, processing, embedding, search, and response generation, ensuring that the \\nservice provides accurate and contextually relevant answers to user queries.  \\nIV. Experiment  \\nIn this chapter, the generative AI service implementation framework introduced in Chapter 3 is utilized to implement \\nvarious scenarios based on enterprise internal data using the integrated RAG model and LangChain according to the \\nimplementation procedure . This provides a series of practical examples for each implementation step. Through these \\nexamples, we explore the methods of implementation and consider the factors to be taken into account during the \\nconstruction process.\", 'THECHRONICLES OF RAG: T HERETRIEVER ,THE CHUNK\\nAND THE GENERATOR\\nPREPRINT\\n∗Paulo Finardi∗Leonardo Avila Rodrigo Castaldoni Pedro Gengo Celio Larcher\\nMarcos Piau Pablo Costa Vinicius Caridá\\n22h, Brazil\\nemail: {pfinardi, leonardo.bernardi.avila, castaldoniro, pedro.gengo.lourenco,\\nceliolarcher, marcos.piau.vieira, pablo.botton.costa, vfcarida}@gmail.com\\n∗Both authors contributed equally to this research.\\nABSTRACT\\nRetrieval Augmented Generation (RAG) has become one of the most popular paradigms for enabling\\nLLMs to access external data, and also as a mechanism for grounding to mitigate against halluci-\\nnations. When implementing RAG you can face several challenges like effective integration of\\nretrieval models, efficient representation learning, data diversity, computational efficiency optimiza-\\ntion, evaluation, and quality of text generation. Given all these challenges, every day a new technique\\nto improve RAG appears, making it unfeasible to experiment with all combinations for your prob-\\nlem. In this context, this paper presents good practices to implement, optimize, and evaluate RAG\\nfor the Brazilian Portuguese language, focusing on the establishment of a simple pipeline for in-\\nference and experiments. We explored a diverse set of methods to answer questions about the first\\nHarry Potter book. To generate the answers we used the OpenAI’s gpt-4 ,gpt-4-1106-preview ,\\ngpt-3.5-turbo-1106 , and Google’s Gemini Pro . Focusing on the quality of the retriever, our\\napproach achieved an improvement of MRR@10 by 35.4%compared to the baseline. When opti-\\nmizing the input size in the application, we observed that it is possible to further enhance it by 2.4%.\\nFinally, we present the complete architecture of the RAG with our recommendations. As result, we\\nmoved from a baseline of 57.88% to a maximum relative score of 98.61%.\\n1 Introduction\\nThe rise of Large Language Models (LLMs) has changed the way we approach Artificial Intelligence (AI) applications.\\nTheir ability to answer different user queries in different domains allow these models to show a notable performance\\nin a wide range of tasks like translation, summarizing, question answering, and many others [1]. However, there are\\na lot of open challenges when it comes to problems that require answers based on updated information, and external\\ndata, that were not available in the training data.\\nIn order to overcome this challenge, a technique called Retrieval Augmented Generation (RAG) [2] was developed.\\nThis approach aims to solve the limitation of the need for external data, by fetching and incorporating this information\\nin the prompt. With this, the model can generate more cohesive answers about subjects and data not seen during the\\ntraining, decreasing the occurrence of hallucinations [3]. Nevertheless, this approach adds a new layer of challenges\\nsince it requires the development of a trustworthy retriever pipeline, given that the quality of the final answer can be\\nhighly affected if the retrieved text is not relevant to the user query [4].\\nThe landscape of RAG is rapidly expanding, with a constant influx of new papers introducing diverse implementations\\n[5]. Each of these variants proposes technical modifications or enhancements, such as different retrieval mechanisms,\\naugmentation techniques, or fine-tuning methodologies. This proliferation, while a testament to the field’s dynamism,arXiv:2401.07883v1  [cs.LG]  15 Jan 2024'], ['CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:3\\nFig. 1. We have classified the application scenarios of RAG into four primary aspects: Create, Read, Update,\\nand Delete. The figure provides an illustrative example for each category, showcasing the wide-ranging\\npotential of RAG technology.\\nwe can use the CRUD framework to classify the RAG systems’ application scenarios. As shown in\\nFigure 1, each CRUD category demonstrates different capabilities of the RAG system:\\n•In \"CREATE \", the system improves the input text by adding relevant information from\\nexternal sources, making creative outputs such as poetry, stories, or code.\\n•In \"READ \", the system uses external knowledge retrieval to answer questions, solving\\nproblems in question-answering, dialogue, and reasoning, and increasing understanding of\\nthe input text.\\n•In \"UPDATE \", the system fixes errors in the input text using retrieved content, correcting\\nspelling, grammar or factual errors to make the text better.\\n•In \"DELETE \", the system simplifies the input by improving retrieval results, removing\\nunnecessary details, and doing tasks like text summarization or simplification.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.', 'CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:7\\nRAG-based SystemSearchDatabase\\nContextUser Query\\nRecall and RankingPre-RankFilterRerankVector DatabaseGraph DatabaseText Database\\nBase LLMNo-RAGRetrieval-Augmented Generation （RAG）Embedding ModelRetrieval StrategyChunk SizeTop-kOverlapping12345\\n6\\n7Task8Evaluation PointsTextContinuationQuestionAnsweringHallucinationModiﬁcationMulti-DocSummarizationCreateReadUpdateDeleteCRUD\\nFig. 2. Illustration of CRUD-RAG, our comprehensive Chinese benchmark for RAG. It classifies the RAG\\napplication scenarios into four categories: create, read, update, and delete. For each category, we create\\nappropriate evaluation tasks and datasets. In the experiments, we evaluate various components of the RAG\\nsystem using our benchmarks.\\n3 CRUD-RAG: A COMPREHENSIVE CHINESE BENCHMARK FOR RAG\\nAs we discussed earlier, implementing RAG effectively requires careful tuning of multiple com-\\nponents, such as the retrieval model, the knowledge corpus, the language model, and the query\\nformulation. Therefore, we need a framework that can evaluate the RAG system automatically. This\\nframework would enable us to examine how these components affect the system’s performance,\\nand provide us with useful insights for improving and innovating the system.\\nHowever, The current RAG benchmarks have several drawbacks: they only evaluate question\\nanswering tasks [ 1,36,50], ignoring other diverse application of RAG. The optimization strategy\\nfor question answering tasks may not suit other tasks; And in the evaluation experiment, current\\nRAG benchmarks only account for the large language model component in the RAG pipeline,\\ndisregarding the vital roles of retrieval database construction and retrieval.\\nTo address the shortcomings of previous benchmarks, we introduce CRUD-RAG, a comprehensive\\nChinese benchmark for RAG. Figure 2 illustrates the features of our CRUD-RAG benchmark. It\\nclassifies the RAG application scenarios into four categories: Create, Read, Update, Delete, and\\nconstructs appropriate evaluation tasks and datasets for each category. Besides, in the experiments,\\nwe will assess the impact of various components of RAG, such as chunk size, retrieval strategy,\\ntop-k, LLM, etc., on all tasks.\\nIn the following section, we will describe the evaluation tasks and the datasets that we design\\nfor each RAG application scenario type. Table 2 presents the size and composition of our datasets,\\nand Figure 3 illustrates an example of our datasets.\\n3.1 News Collection\\nAs mentioned above, the existing benchmarks for evaluating RAG systems are mainly constructed\\nfor question answering tasks. Therefore, the datasets, such as NQ [ 23] and RGB [ 7], are also tailored\\nfor this type of task. Hence, we need to construct new datasets.\\nWe argue that the latest news data is the most suitable choice to create an RAG evaluation\\ndataset. Unlike other types of data, such as encyclopedias, questions, or conversations, the latest\\nnews data prevent the model from generating answers directly from its own knowledge base.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.', '111:4 Lyu, et al.\\nTo evaluate the RAG system in these four scenarios, we introduce CRUD-RAG, a comprehensive,\\nlarge-scale Chinese RAG benchmark. CRUD-RAG consists of four evaluation tasks: text continu-\\nation, question answering (with single-document and multi-document questions), hallucination\\nmodification, and open-domain multi-document summarization, which respectively correspond to\\nthe CRUD-RAG classification of RAG application scenarios. We construct CRUD-RAG by crawling\\nthe latest high-quality news data from major news websites in China, which were not exposed\\nto the LLMs during training. We then automatically create datasets using GPT-4 based on these\\nnews data. For the multi-document summarization task, we apply a reverse construction strategy.\\nWe first generate news events and their summaries using GPT-4. Then, we use these events as\\nkeywords to search for 10 related and non-duplicate reports from the web, which we add to our\\nretrieval database. During evaluation, the RAG system will use the retrieval database to generate\\nsummaries for the events. For the text continuation task, we split the news text into a beginning\\nand a continuation paragraph. We then use each sentence in the continuation paragraph as a\\nkeyword to search for 10 related reports on the Web. We remove any duplicate content and add the\\nreports to the retrieval database. For the single-document QA task, we use the RGB [ 7] construction\\nmethod. For the multi-document QA task, we use the Chain-of-Thought technology to help the\\nmodel identify common and different aspects among documents, and then generate questions based\\non these aspects with increasing difficulty. For the hallucination modification task, we use the\\nannotations in the UHGEval dataset and correct hallucinations with GPT-4. We also include the\\nreal news in UHGEval in the retrieval database.\\nIn the experiments, we systematically evaluate the RAG system’s performance on our CRUD-RAG\\nbenchmark. We also investigate various factors that affect the RAG system, such as the context\\nlength, the chunk size, the embedding model, the retrieval algorithms, and the LLM. Based on our\\nexperimental results, we provide some valuable suggestions for building effective RAG systems.\\nThe contributions of this paper are:\\n•A comprehensive evaluation benchmark : Our benchmark covers not only question\\nanswering, but also create, read, update, and delete (CRUD) of RAG applications.\\n•High-quality evaluation datasets : We constructed diverse datasets for different evaluation\\ntasks, based on the application scenarios of RAG. These tasks include text continuation,\\nmulti-document summarization, question answering and hallucination modification.\\n•Extensive experiments : we performed extensive experiments on our own benchmark, using\\nvarious metrics to measure the performance of RAG systems. Based on our experiments, we\\noffered useful guidance for future researchers and RAG system developers.\\n2 RELATED WORK\\n2.1 RAG: A Neural Text Generation Framework that Combines Retrieval and\\nGeneration\\nLLMs excel in text generation but also confront challenges such as outdated knowledge and the\\ngeneration of hallucinatory content [ 6,17,37]. In response to these challenges, RAG, also referred\\nto as RALM (Retrieval-Augmented Language Models), incorporate external knowledge to generate\\nresponses characterized by enhanced accuracy and realism [ 41]. This is particularly critical in\\ndomains that heavily depend on precision and reliability, including but not limited to the legal,\\nmedical, and financial sectors. Retrieval models have been promoting the development of language\\nmodels [14, 29, 52].\\nConventional RAG systems adhere to a standardized workflow encompassing indexing, retrieval,\\nand generation phases [ 25,32]. The indexing phase encompasses data cleansing, extraction, trans-\\nformation into plain text, segmentation, and indexing, utilizing embedding models to transform text\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.', 'CRUD-RAG: A Comprehensive Chinese Benchmark for\\nRetrieval-Augmented Generation of Large Language Models\\nYUANJIE LYU∗,University of Science and Technology of China, China\\nZHIYU LI∗,Institute for Advanced Algorithms Research (Shanghai), China\\nSIMIN NIU, Renmin University of China, China\\nFEIYU XIONG and BO TANG, Institute for Advanced Algorithms Research (Shanghai), China\\nWENJIN WANG and HAO WU, Institute for Advanced Algorithms Research (Shanghai), China\\nHUANYONG LIU, 360 AI Research Institute, China\\nTONG XU†,University of Science and Technology of China, China\\nENHONG CHEN, University of Science and Technology of China, China\\nYI LUO, PENG CHENG, HAIYING DENG, ZHONGHAO WANG, and ZIJIA LU, State Key\\nLaboratory of Media Convergence Production Technology and Systems, Beijing, China\\nRetrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models\\n(LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations,\\nincluding outdated information and the tendency to produce inaccurate \"hallucinated\" content. However,\\nevaluating RAG systems is a challenge. Most current benchmarks mainly focus on question-answering\\napplications, and neglect the wider range of scenarios where RAG could be beneficial. Moreover, they only\\nevaluate the performance of LLM components of RAG pipeline in the experiments, and neglect the influence\\nof retrieval components and external knowledge database construction. To address these issues, this paper\\nconstructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems\\nin various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions\\nbetween users and knowledge bases, and also categorize the range of RAG applications into four distinct\\ntypes—Create, Read, Update, and Delete (CRUD). \"Create\" refers to scenarios requiring the generation of\\noriginal, varied content. \"Read\" involves responding to intricate questions in knowledge-intensive situations.\\n\"Update\" focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \"Delete\"\\npertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories,\\nwe have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects\\n∗Both authors contributed equally to this research.\\n†Corresponding author.\\nAuthors’ addresses: Yuanjie Lyu, University of Science and Technology of China, Hefei, China, s1583050085@gmail.com;\\nZhiyu Li, Institute for Advanced Algorithms Research (Shanghai), China, lizy@iaar.ac.cn; Simin Niu, niusimi@ruc.edu.cn,\\nRenmin University of China, Beijing, China; Feiyu Xiong, xiongfy@iaar.ac.cn; Bo Tang, tangb@iaar.ac.cn, Institute for\\nAdvanced Algorithms Research (Shanghai), China; Wenjin Wang, wangwj@iaar.ac.cn; Hao Wu, wuh@iaar.ac.cn, Institute\\nfor Advanced Algorithms Research (Shanghai), China; Huanyong Liu, liuhuanyong@360.cn, 360 AI Research Institute,\\nBeijing, China; Tong Xu, University of Science and Technology of China, Hefei, China, tongxu@ustc.edu.cn; Enhong Chen,\\nUniversity of Science and Technology of China, Hefei, China, cheneh@ustc.edu.cn; Yi Luo, luoyi@xinhua.org; Peng Cheng,\\nchengpeng@xinhua.org; Haiying Deng, denghaiying@xinhuaskl.com; Zhonghao Wang, wangzhonghao@xinhua.org; Zijia\\nLu, luzijia@xinhua.org, State Key Laboratory of Media Convergence Production Technology and Systems, Beijing, China.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\n©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM 0004-5411/2018/8-ART111\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.arXiv:2401.17043v2  [cs.CL]  19 Feb 2024'], ['Figure 6: RAG compared with other model optimization methods\\nand avoiding contradictions.\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively address-\\ning the core inquiry.\\nRequired Abilities\\nRAG evaluation also encompasses four abilities indicative of\\nits adaptability and efficiency: noise robustness, negative re-\\njection, information integration, and counterfactual robust-\\nness [Chen et al. , 2023b, Liu et al. , 2023b ]. These abilities\\nare critical for the model’s performance under various chal-\\nlenges and complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in re-\\nfraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\nInformation Integration evaluates the model’s proficiency\\nin synthesizing information from multiple documents to ad-\\ndress complex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.\\nContext relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating thequality of generation.\\nThe specific metrics for each evaluation aspect are summa-\\nrized in Table 2. It is essential to recognize that these metrics,\\nderived from related work, are traditional measures and do\\nnot yet represent a mature or standardized approach for quan-\\ntifying RAG evaluation aspects. Custom metrics tailored to\\nthe nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\n7.3 Evaluation Benchmarks and Tools\\nThis section delineates the evaluation framework for RAG\\nmodels, comprising benchmark tests and automated eval-\\nuation tools. These instruments furnish quantitative met-\\nrics that not only gauge RAG model performance but also\\nenhance comprehension of the model’s capabilities across\\nvarious evaluation aspects. Prominent benchmarks such as\\nRGB and RECALL [Chen et al. , 2023b, Liu et al. , 2023b ]\\nfocus on appraising the essential abilities of RAG mod-\\nels. Concurrently, state-of-the-art automated tools like RA-\\nGAS [Eset al. , 2023 ], ARES [Saad-Falcon et al. , 2023 ], and\\nTruLens8employ LLMs to adjudicate the quality scores.\\nThese tools and benchmarks collectively form a robust frame-\\nwork for the systematic evaluation of RAG models, as sum-\\nmarized in Table 3.\\n8https://www.trulens.org/trulens eval/core concepts ragtriad/', 'Table 2: Summary of metrics applicable for evaluation aspects of RAG\\nContext\\nRelevanceFaithfulnessAnswer\\nRelevanceNoise\\nRobustnessNegative\\nRejectionInformation\\nIntegrationCounterfactual\\nRobustness\\nAccuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nEM ✓\\nRecall ✓\\nPrecision ✓ ✓\\nR-Rate ✓\\nCosine Similarity ✓\\nHit Rate ✓\\nMRR ✓\\nNDCG ✓\\nTable 3: Summary of evaluation frameworks\\nEvaluation Framework Evaluation Targets Evaluation Aspects Quantitative Metrics\\nRGB† Retrieval Quality\\nGeneration QualityNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual RobustnessAccuracy\\nEM\\nAccuracy\\nAccuracy\\nRECALL†Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\\nRAGAS‡ Retrieval Quality\\nGeneration QualityContext Relevance\\nFaithfulness\\nAnswer Relevance*\\n*\\nCosine Similarity\\nARES‡ Retrieval Quality\\nGeneration QualityContext Relevance\\nFaithfulness\\nAnswer RelevanceAccuracy\\nAccuracy\\nAccuracy\\nTruLens‡ Retrieval Quality\\nGeneration QualityContext Relevance\\nFaithfulness\\nAnswer Relevance*\\n*\\n*\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\n8 Future Prospects\\nThis section explores three future prospects for RAG: future\\nchallenges, modality expansion, and the RAG ecosystem.\\n8.1 Future Challenges of RAG\\nDespite the considerable progress in RAG technology, several\\nchallenges persist that warrant in-depth research:\\nContext Length . RAG’s efficacy is limited by the context\\nwindow size of Large Language Models (LLMs). Balancing\\nthe trade-off between a window that is too short, risking insuf-\\nficient information, and one that is too long, risking informa-\\ntion dilution, is crucial. With ongoing efforts to expand LLM\\ncontext windows to virtually unlimited sizes, the adaptation\\nof RAG to these changes presents a significant research ques-\\ntion[Xuet al. , 2023c, Packer et al. , 2023, Xiao et al. , 2023 ].\\nRobustness . The presence of noise or contradictory infor-\\nmation during retrieval can detrimentally affect RAG’s out-put quality. This situation is figuratively referred to as “Mis-\\ninformation can be worse than no information at all”. Im-\\nproving RAG’s resistance to such adversarial or counterfac-\\ntual inputs is gaining research momentum and has become a\\nkey performance metric [Yuet al. , 2023a, Glass et al. , 2021,\\nBaek et al. , 2023 ].\\nHybrid Approaches (RAG+FT) . Combining RAG with\\nfine-tuning is emerging as a leading strategy. Determining the\\noptimal integration of RAG and fine-tuning whether sequen-\\ntial, alternating, or through end-to-end joint training—and\\nhow to harness both parameterized and non-parameterized\\nadvantages are areas ripe for exploration [Linet al. , 2023 ].\\nExpanding LLM Roles . Beyond generating final answers,\\nLLMs are leveraged for retrieval and evaluation within RAG\\nframeworks. Identifying ways to further unlock LLMs poten-\\ntial in RAG systems is a growing research direction.\\nScaling Laws . While scaling laws [Kaplan et al. , 2020 ]are\\nestablished for LLMs, their applicability to RAG remains un-', '111:6 Lyu, et al.\\n2.2 RAG Benchmark: Evaluation Datasets and Methods for Retrieval-Augmented\\nGeneration\\nWhen investigating the development and optimization of RAG, the effective evaluation of their\\nperformance becomes a fundamental concern. LangChain provides benchmark tasks, such as\\nLangChain Docs Q&A and Semi-structured Reports [ 24], designed to assess various RAG architec-\\ntures. These datasets are constructed from snapshots of Python documentation and PDFs containing\\ntables and charts. They emphasize the model’s capability to handle structured and semi-structured\\ndata. Evaluation standards encompass the accuracy of answers and the faithfulness of model re-\\nsponses. Utilizing large models for question-answering generation has emerged as a prevalent\\napproach in building evaluation datasets. For instance, RGB [ 7] creates its evaluation dataset by\\ngathering recent news reports and employing LLM to generate relevant events, questions, and\\nanswers. Conversely, ARES [ 38]. relies on generating synthetic queries and answers, leveraging\\nthe FLAN-T5 XXL model. These methods not only showcase the RAG system’s proficiency in\\nhandling real-time data but also illustrate the utility of automation and synthetic data in the evalu-\\nation process. For evaluating the capabilities of models across various professional domains, the\\nInstruct-Benchmark-Tester dataset encompasses a range of question types, with a particular focus\\non financial services, legal, and intricate business scenarios [35].\\nDepending on whether the evaluation phase incorporates ground truth, metrics of existing\\nevaluation methods can be categorized into those necessitating reference and those not requiring\\nit. Reference-required evaluation methods gauge the accuracy and robustness of the RAG by\\ncontrasting model-generated answers with factual benchmarks. As an example, RAG-Instruct-\\nBenchmark-Tester [ 35] employs accuracy score as an evaluation metric, a widely acknowledged\\nmeasure of model performance that assesses the extent to which model-generated answers align\\nwith reference answers. The primary objective of RGB [ 7] is to evaluate whether large models can\\neffectively utilize external documents to acquire knowledge and generate accurate answers. Its\\nevaluation metrics encompass accuracy, rejection rate, error detection rate, and correction rate.\\nReference-free evaluation methods, including TruLens-Eval [ 13], RAGAS [ 12], and ARES [ 38],\\nprovide distinct viewpoints for evaluating the performance of RAG systems, particularly concerning\\ncontext relevance, answer faithfulness, and answer relevance. TruLens-Eval [ 13] introduces the\\nRAG Triad as an innovative approach to evaluate hallucination issues within the RAG architecture,\\nencompassing context relevance, groundedness, and answer relevance. RAGAS [ 12], serving as\\na reference-free evaluation framework, concentrates on assessing the retrieval system’s capacity\\nto identify pertinent and concentrated context passages, along with the LLMs’ proficiency in\\nfaithfully and accurately leveraging these passages. In contrast to RAGAS, which depends on a\\npredefined set of heuristically crafted prompts, ARES generates tailored LLMs judges for each\\naspect of a RAG pipeline, leading to a substantial enhancement in evaluation precision and accuracy\\nwhen compared to existing methods such as RAGAS. Furthermore, ARES [ 38] employs prediction-\\npowered inference to offer statistical assurances for its scoring, generating confidence intervals.\\nARES emphasizes three evaluation scores: context relevance, answer faithfulness, and answer\\nrelevance, highlighting the importance of a proficient RAG system in identifying relevant contexts\\nand producing both faithful and relevant answers. Regarding evaluation methods, [ 28] places an\\nemphasis on assessing the credibility and accuracy of responses generated by generative search\\nengines through manual inspection. Nonetheless, manual evaluation possesses drawbacks, including\\nhigh costs and challenges in scalability. Hence, rule-based evaluation metrics such as accuracy,\\nexact match, rouge, or self-devised metrics like rejection rate, error detection rate, and correction\\nrate continue to be widely adopted in the field. Furthermore, employing large language models for\\nevaluation closely approximates manual evaluation outcomes.\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.', 'CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models 111:13\\nFig. 6. Overview of RAGQuestEval. A set of questions is generated based on the ground truth references.\\nThe questions are then answered using both the ground truth and the response. For the recall score of\\nRAGQuestEval, we calculate the ratio of answerable questions to all questions(in this case, recall = 2/3). For\\nthe precision score of RAGQuestEval, corresponding answers are compared using a similarity function and\\naveraged across questions(in this case, precision = (0.5 + 1) / 2 = 0.75). The recall metric of RAGQuestEval\\nindicates how much of the key information in the ground truth reference is included in the generated text,\\nwhile the precision metric of RAGQuestEval indicates how correct the recalled key information is.\\na similar answer if the summary realistically matches the original document. They evaluate the\\naccuracy of each local piece of key information in the summary.\\nWe also consider question answering-based metrics to evaluate the factual accuracy of generation.\\nIn this paper, we examine QuestEval [39], a metric that improves the correlation with human\\njudgments over previous metrics in their extensive experiments. QuestEval evaluates the factual\\nconsistency between the generated text and the source document, which is mainly used for text\\nsummarization tasks. Therefore, it does not require any ground truth reference. However, for\\nRAG systems, the retrieved texts may be irrelevant or incorrect, so consistency with them is not a\\nvalid criterion. Instead, we use this metric to measure how well the generated text matches the\\nground-truth reference. We call this metric RAGQuestEval . We will explain this metric in detail.\\nLet𝐺𝑇and𝐺𝑀 be two sequences of tokens, where 𝐺𝑇denotes the ground truth references and\\n𝐺𝑀 the corresponding evaluated generations. First, we generate a series of questions from the\\nground truth references 𝐺𝑇using the QuestEval method, which extracts entities and noun phrases\\nfrom the text. The goal of RAGQuestEval is to check if the generated text includes and conveys\\ncorrectly all the key information from the ground truth reference.\\nNext, we answer these questions using both real references and model-generated text. If the\\nquestion is unanswerable, the model returns \"<Unanswerable>\".\\nFinally, we calculate two scores to evaluate the quality of the generated text: recall and precision.\\nRecall .Recall is the ratio of answerable questions to all questions. This score shows how much\\ninformation in the ground truth reference is captured by the text generated by the RAG system. A\\nhigher recall means that the generated text covers more information from the reference.\\nRecall(𝐺𝑇,𝐺𝑀)=1\\n|𝑄𝐺(𝐺𝑇)|∑︁\\n(𝑞,𝑟)∈𝑄𝐺(𝐺𝑇)I[𝑄𝐴(𝐺𝑀,𝑞)≠<Unanswerable >] (1)\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.'], ['Benchmarking Large Language Models in Retrieval-Augmented Generation\\nJiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2\\n1Chinese Information Processing Laboratory2State Key Laboratory of Computer Science\\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\\n3University of Chinese Academy of Sciences, Beijing, China\\n{jiawei2020,hongyu,xianpei,sunle }@iscas.ac.cn\\nAbstract\\nRetrieval-Augmented Generation (RAG) is a promising ap-\\nproach for mitigating the hallucination of large language\\nmodels (LLMs). However, existing research lacks rigorous\\nevaluation of the impact of retrieval-augmented generation\\non different large language models, which make it challeng-\\ning to identify the potential bottlenecks in the capabilities\\nof RAG for different LLMs. In this paper, we systemati-\\ncally investigate the impact of Retrieval-Augmented Gener-\\nation on large language models. We analyze the performance\\nof different large language models in 4 fundamental abili-\\nties required for RAG, including noise robustness, negative\\nrejection, information integration, and counterfactual robust-\\nness. To this end, we establish Retrieval-Augmented Genera-\\ntion Benchmark (RGB), a new corpus for RAG evaluation in\\nboth English and Chinese. RGB divides the instances within\\nthe benchmark into 4 separate testbeds based on the afore-\\nmentioned fundamental abilities required to resolve the case.\\nThen we evaluate 6 representative LLMs on RGB to diag-\\nnose the challenges of current LLMs when applying RAG.\\nEvaluation reveals that while LLMs exhibit a certain degree\\nof noise robustness, they still struggle significantly in terms of\\nnegative rejection, information integration, and dealing with\\nfalse information. The aforementioned assessment outcomes\\nindicate that there is still a considerable journey ahead to ef-\\nfectively apply RAG to LLMs.\\nIntroduction\\nRecently, there have been impressive advancements in large\\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\\nChatGLM (THUDM 2023a). Although these models have\\nshown remarkable general abilities (Bang et al. 2023; Guo\\net al. 2023), they still suffer severely from challenges includ-\\ning factual hallucination (Cao et al. 2020; Raunak, Menezes,\\nand Junczys-Dowmunt 2021; Ji et al. 2023), knowledge out-\\ndating (He, Zhang, and Roth 2022), and the lack of domain-\\nspecific expertise (Li et al. 2023c; Shen et al. 2023).\\nIncorporating external knowledge via information re-\\ntrieval, i.e., Retrieval-Augmented Generation (RAG), has\\nbeen regarded as a promising way to resolve the above chal-\\nlenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\\n*Corresponding authors.\\nCopyright © 2024, Association for the Advancement of Artificial\\nIntelligence (www.aaai.org). All rights reserved.\\nNoise Robustness Negative Rejection\\nWho was awarded the 2022 Nobel prize in \\nliterature?\\nThe Nobel Prize in Literature for 2022 is \\nawarded to the French author Annie Ernaux, \\n“for the courage and clinical acuity …\\nThe Nobel Prize in Literature for 2021 is \\nawarded to the novelist Abdulrazak Gurnah, \\nborn in Zanzibar and active in …\\nAnnie ErnauxQuestion\\nExternal documents contain noises\\nRetrieval Augmented \\nGenerationWho was awarded the 2022 Nobel prize in \\nliterature?\\nThe Nobel Prize in Literature for 2021 is \\nawarded to the novelist Abdulrazak Gurnah, \\nborn in Zanzibar and active in …\\nThe 2020 Nobel Laureate in Literature, \\npoet Louise Glück, has written both poetry \\nand essays about poetry. Since her…\\nI can not answer the question because of the \\ninsufficient information in documentsQuestion\\nExternal documents are all noises\\nInformation Integration\\nWhen were the ChatGPT app for iOS and \\nChatGPT apilaunched?\\nOn May 18th, 2023, OpenAI introduced its \\nown ChatGPT app for iOS…\\nThat changed on March 1, when OpenAI \\nannounced the release of API access to \\nChatGPT and Whisper,…\\nMay 18 and March 1 .Question\\nExternal documents contain all answers\\nRetrieval Augmented \\nGenerationCounterfactual Robustness\\nWhich city hosted the Olympic games in \\n2004?\\nThe 2004 Olympic Games returned home to \\nNew York , birthplace of the … \\nAfter leading all voting rounds, New York\\neasily defeated Rome in the fifth and \\nfinal vote …\\nThere are factual errors in the provided \\ndocuments. The answer should be Athens . Question\\nCounterfactual external documents\\nRetrieval Augmented \\nGenerationRetrieval Augmented \\nGenerationFigure 1: Illustration of 4 kinds of abilities required for\\nretrieval-augmented generation of LLMs.\\n2022; Izacard et al. 2022). With the help of external knowl-\\nedge, LLMs can generate more accurate and reliable re-\\nsponses. The most common method is to use a search engine\\nas a retriever such as New Bing. Due to the vast amount of\\ninformation available on the Internet, using a search engine\\ncan provide more real-time information.\\nHowever, Retrieval-Augmented Generation brings not\\nonly positive effects to LLMs (Liu, Zhang, and Liang 2023;\\nMaynez et al. 2020). On one hand, there is a significant\\namount of noise information even fake news in the content\\navailable on the Internet, which poses challenges for search\\nengines in accurately retrieving desirable knowledge. On the\\nother hand, LLMs suffer from unreliable generation chal-\\nlenge. LLMs can be misled by incorrect information con-\\ntained in the context (Bian et al. 2023) and also suffer from\\nhallucination during the generation (Adlakha et al. 2023),\\nresulting in generating content that goes beyond external in-arXiv:2309.01431v2  [cs.CL]  20 Dec 2023', 'formation. These challenges result in LLMs being unable to\\nconsistently generate reliable and accurate responses. Un-\\nfortunately, currently there lacks of comprehensive under-\\nstanding on how these factors can influence RAG, and how\\ncould each model survives from these drawbacks and im-\\nprovement their performance via information retrieval. As a\\nresult, there is a pressing need for a comprehensive evalua-\\ntion of LLMs on their ability to effectively utilize retrieved\\ninformation, as well as their ability to withstand the various\\ndrawbacks present in information retrieval.\\nTo this end, this paper conducts a comprehensive evalua-\\ntion of RAG for current LLMs. Specifically, we create a new\\nRetrieval-Augmented Generation Benchmark, namely RGB,\\nin both English and Chinese. In order to ensure that the in-\\nternal knowledge of LLMs does not introduce bias into the\\nevaluation results, RGB chooses to aggregate the latest news\\ninformation and constructs queries based on the news infor-\\nmation. Then, based on these queries, we use Search API to\\nfetch relevant documents and select most relevant snippets\\nfrom the content as external retrieved documents. Finally,\\nbased on different compositions of query and document-set\\npairs, we expand the corpus and divided it into 4 testbeds to\\nevaluate the following basic abilities of LLMs according to\\nthe common challenges in RAG, as shown in Figure 1:\\n•Noise Robustness , which means a LLM can extract use-\\nful information from noisy documents. In this paper, we\\ndefine noisy documents as those that are relevant to the\\nquestion but do not contain any information of the an-\\nswer. For the instance in Figure 1, the noisy documents\\nrelated to the question “Who was awarded the 2022 No-\\nbel Prize in Literature” include reports about the 2021\\nNobel Prize in Literature. To this end, the testbed for\\nnoise robustness contains instances whose external doc-\\numents contain a certain number of noisy documents\\nbased on the desired noise ratio.\\n•Negative Rejection , which means that a LLM should re-\\nject to answer the question when the required knowledge\\nis not present in any retrieved document. The testbed for\\nnegative rejection contains instances whose external doc-\\numents are only with noisy documents. LLMs are ex-\\npected to indicate “insufficient information” or other re-\\njection signals.\\n•Information Integration , which evaluates whether\\nLLMs can answer complex questions that require inte-\\ngrating information from multiple documents. For the in-\\nstance in Figure 1, for the question “When were the Chat-\\nGPT app for iOS and ChatGPT api launched?”, LLMs\\nare expected to provide information of the launch dates\\nfor both the ChatGPT iOS app and ChatGPT API. The\\ntestbed for information integration contains instances\\nthat can only be answered using multiple documents.\\n•Counterfactual Robustness , which evaluates whether\\nLLMs can identify risks of known factual errors in the\\nretrieved documents when the LLMs are given warnings\\nabout potential risks in the retrieved information through\\ninstruction. The testbed for counterfactual robustness in-\\ncludes instances that can be answered directly by the\\nLLMs, but the external documents contain factual errors.Based on RGB, we conduct evaluation on 6 state-of-\\nthe-art large language models including ChatGPT (Ope-\\nnAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-\\n6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\\nQwen-7B-Chat (QwenLM 2023), BELLE-7B (Yunjie Ji\\n2023). We found that even though RAG can improve the re-\\nsponse accuracy of LLMs, they still suffer from the above-\\nmentioned challenges significantly. Specifically, we found\\nthat even though LLMs demonstrate some level of noise ro-\\nbustness, they tend to confuse similar information and fre-\\nquently generate inaccurate answers when relevant informa-\\ntion exists. For example, when faced with a question about\\nthe 2022 Nobel Prize in Literature, if there are noisy docu-\\nments about the 2021 Nobel Prize in Literature in external\\ndocuments, LLMs may become confused and provide inac-\\ncurate answers. Besides, LLMs frequently fail to reject an-\\nswering and generate incorrect answers when none of the\\nexternal documents contain relevant information. Further-\\nmore, LLMs lack the ability to summarize from multiple\\ndocuments, and therefore if multiple documents are needed\\nto answer a question, LLMs often fail to provide accurate\\nanswer. Finally, we found that even when the LLMs contain\\nthe required knowledge and are given warnings about po-\\ntential risks in the retrieved information through instruction,\\nthey still tend to trust and prioritize the retrieved information\\nover their own existing knowledge. The experimental results\\nmentioned above highlight the need for further resolution of\\nimportant issues in the existing RAG method. Therefore, it\\nis crucial to exercise caution and carefully design its usage.\\nGenerally speaking, the contributions of this paper are1:\\n• We proposed to evaluate four capabilities for retrieval-\\naugmented generation of LLMs and created the\\nRetrieval-Augmented Generation Benchmark in both En-\\nglish and Chinese. To best of our knowledge, it is the first\\nbenchmark designed to assess these four capabilities for\\nretrieval-augmented generation of LLMs.\\n• We evaluated the existing LLMs using RGB and found\\nthe limitations of them in the four different abilities.\\n• We analyzed the responses of LLMs in RGB and identi-\\nfied their current shortcomings as well as suggested di-\\nrections for improvement.\\nRelated work\\nRetrieval-augmented models The knowledge stored in\\nlarge language models is commonly out-of-date (He, Zhang,\\nand Roth 2022) and they also sometimes generate hallu-\\ncination (Cao et al. 2020; Raunak, Menezes, and Junczys-\\nDowmunt 2021; Ji et al. 2023) i.e., they may generate ir-\\nrelevant or factually incorrect contents. By using external\\nknowledge as guidance, retrieval-augmented models can\\ngenerate more accurate and reliable responses (Guu et al.\\n2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\\net al. 2022; Shi et al. 2023; Ren et al. 2023). Retrieval-\\naugmented models have achieved remarkable results in var-\\nious tasks such as open-domain QA (Izacard and Grave\\n2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\\n1Our code&data: https://github.com/chen700564/RGB.', 'THECHRONICLES OF RAG: T HERETRIEVER ,THE CHUNK\\nAND THE GENERATOR\\nPREPRINT\\n∗Paulo Finardi∗Leonardo Avila Rodrigo Castaldoni Pedro Gengo Celio Larcher\\nMarcos Piau Pablo Costa Vinicius Caridá\\n22h, Brazil\\nemail: {pfinardi, leonardo.bernardi.avila, castaldoniro, pedro.gengo.lourenco,\\nceliolarcher, marcos.piau.vieira, pablo.botton.costa, vfcarida}@gmail.com\\n∗Both authors contributed equally to this research.\\nABSTRACT\\nRetrieval Augmented Generation (RAG) has become one of the most popular paradigms for enabling\\nLLMs to access external data, and also as a mechanism for grounding to mitigate against halluci-\\nnations. When implementing RAG you can face several challenges like effective integration of\\nretrieval models, efficient representation learning, data diversity, computational efficiency optimiza-\\ntion, evaluation, and quality of text generation. Given all these challenges, every day a new technique\\nto improve RAG appears, making it unfeasible to experiment with all combinations for your prob-\\nlem. In this context, this paper presents good practices to implement, optimize, and evaluate RAG\\nfor the Brazilian Portuguese language, focusing on the establishment of a simple pipeline for in-\\nference and experiments. We explored a diverse set of methods to answer questions about the first\\nHarry Potter book. To generate the answers we used the OpenAI’s gpt-4 ,gpt-4-1106-preview ,\\ngpt-3.5-turbo-1106 , and Google’s Gemini Pro . Focusing on the quality of the retriever, our\\napproach achieved an improvement of MRR@10 by 35.4%compared to the baseline. When opti-\\nmizing the input size in the application, we observed that it is possible to further enhance it by 2.4%.\\nFinally, we present the complete architecture of the RAG with our recommendations. As result, we\\nmoved from a baseline of 57.88% to a maximum relative score of 98.61%.\\n1 Introduction\\nThe rise of Large Language Models (LLMs) has changed the way we approach Artificial Intelligence (AI) applications.\\nTheir ability to answer different user queries in different domains allow these models to show a notable performance\\nin a wide range of tasks like translation, summarizing, question answering, and many others [1]. However, there are\\na lot of open challenges when it comes to problems that require answers based on updated information, and external\\ndata, that were not available in the training data.\\nIn order to overcome this challenge, a technique called Retrieval Augmented Generation (RAG) [2] was developed.\\nThis approach aims to solve the limitation of the need for external data, by fetching and incorporating this information\\nin the prompt. With this, the model can generate more cohesive answers about subjects and data not seen during the\\ntraining, decreasing the occurrence of hallucinations [3]. Nevertheless, this approach adds a new layer of challenges\\nsince it requires the development of a trustworthy retriever pipeline, given that the quality of the final answer can be\\nhighly affected if the retrieved text is not relevant to the user query [4].\\nThe landscape of RAG is rapidly expanding, with a constant influx of new papers introducing diverse implementations\\n[5]. Each of these variants proposes technical modifications or enhancements, such as different retrieval mechanisms,\\naugmentation techniques, or fine-tuning methodologies. This proliferation, while a testament to the field’s dynamism,arXiv:2401.07883v1  [cs.LG]  15 Jan 2024', '111:2 Lyu, et al.\\nof various components of the RAG system, such as the retriever, context length, knowledge base construction,\\nand LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios1.\\nCCS Concepts: •Computing methodologies →Natural language generation ;•Information systems\\n→Information retrieval .\\nAdditional Key Words and Phrases: Retrieval-Augmented Generation, Large Language Models, Evaluation\\nACM Reference Format:\\nYuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu,\\nEnhong Chen, Yi Luo, Peng Cheng, Haiying Deng, Zhonghao Wang, and Zijia Lu. 2018. CRUD-RAG: A\\nComprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models. J. ACM\\n37, 4, Article 111 (August 2018), 28 pages. https://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nRetrieval-augmented generation (RAG) is an advanced technique that leverages external knowledge\\nsources to enhance the text generation capabilities of large language models(LLMs). It retrieves\\nrelevant paragraphs from a corpus based on the input, and feeds them to the LLMs along with the\\ninput. With the help of external knowledge, LLMs can generate more accurate and credible responses\\nand effectively address challenges such as outdated knowledge [ 17], hallucinations [ 3,8,31,54],\\nand lack of domain expertise [ 26,40]. Therefore, RAG technology is attracting increasing attention.\\nAlthough the effectiveness of retrieval-augmented strategies has been proven through extensive\\npractice, their implementation still requires a significant amount of tuning. The overall performance\\nof the RAG system is affected by multiple factors, such as the retrieval model, construction of the\\nexternal knowledge base, and language model. Therefore, automatic evaluation of RAG systems is\\ncrucial. Currently, there are only a few existing benchmarks for evaluating RAG performance, as\\ncreating high-quality datasets and experimenting on them entail significant costs. These benchmarks\\ncan be classified into two types: reference-required and reference-free evaluation. Reference-free\\nevaluation frameworks, such as RAGAS [ 12] and ARES [ 38], use LLM-generated data to evaluate\\nRAG systems on contextual relevance, faithfulness, and informativeness. These frameworks do\\nnot depend on ground truth references, but only assess the coherence of the generated text with\\nthe retrieved context. This approach may be unreliable if the retrieved external information is\\nlow-quality.\\nConsequently, reference-required evaluations remain the predominant method for assessing RAG\\nsystems. Existing benchmarks for reference-required evaluations, such as RGB [ 7] and NQ [ 23]. do\\nhave their limitations. First, they all rely on question answering tasks to measure the performance\\nof RAG systems. Question answering is not the only RAG application scenario, and an optimization\\nstrategy that works well for question answering may not be generalized to other scenarios. Thus,\\nthese benchmarks may not capture the full potential of RAG systems. Second, in the experiments,\\ncurrent evaluations usually focus on evaluating the LLM part of the RAG pipeline, while ignoring\\nthe retrieval model and external knowledge base construction. These components are also crucial\\nfor RAG systems, as they determine the quality and relevance of retrieved passages and external\\nknowledge. Therefore, a comprehensive evaluation of the RAG system may not be obtained using\\nany existing benchmarks.\\nTo evaluate the performance of RAG in different application scenarios, we need a comprehensive\\nbenchmark that covers more than just the question answering task. Lewis et al . [25] argue that the\\ncore of RAG systems is their interactive way of combining LLMs with external knowledge sources.\\nAnd following [ 22], we can group any interaction with external knowledge sources into four basic\\nactions: create, read, update, and delete, which are also known as CRUD actions [ 42]. Therefore,\\n1The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.'], ['Sampling (Holtzman et al. 2020) for decoding, we use beam\\nsearch with a beam size of 4. Additionally, we set a rep-\\netition penalty of 1.2 (Keskar et al. 2019) and a maximum\\ndecode length of 60 tokens.\\n4.2 Hierarchical Selection\\nOne of the main issues with the baseline approach for knowl-\\nedge selection is its computational complexity. In the fol-\\nlowing, we refer to |D|,|E|and|K|as the total number\\nof domains, entities and knowledge snippets. To calculate a\\nrelevance score for each knowledge snippet, |K|inference\\ncomputations using the whole model are required. In a real-\\ntime dialog application, this may become prohibitive.\\nAn approach to solve this issue is to make use of the ad-\\nditional metadata available for each knowledge snippet. In -\\nstead of directly selecting the relevant knowledge snippet\\nfrom all available documents, we can divide the problem\\nby ﬁrst identifying the relevant domain and entity and then\\nselecting the relevant snippet among the documents of this\\nentity. Therefore we use the same relevance classiﬁcation\\nmodel as in the baseline. We try two variants of this ap-\\nproach. In the ﬁrst one, we train three separate models to\\nretrieve the relevant domain, entity, and document. In the\\nsecond one, we train one model to jointly retrieve the rele-\\nvant domain and entity and one model to retrieve the relevant\\ndocument.\\nIn total the approach reduces the complexity of the task\\nfrom\\nO(|K|) =O(\\n|D|·|E|\\n|D|·|K|\\n|E|)\\nto\\nO(\\n|D|+|E|\\n|D|+|K|\\n|E|)\\nandO(\\n|E|+|K|\\n|E|)\\nfor the ﬁrst and second variants.\\n4.3 Dense Knowledge Retrieval\\nWhile this approach decreases computational complexity, i t\\nmay still be infeasible if the number of knowledge snippets\\ngets too large. Kim et al. showed that classical information\\nretrieval approaches like TF-IDF and BM25 are signiﬁcantly\\noutperformed by their relevance classiﬁcation model. Sim-\\nple bag-of-words models do not seem to be able to capture\\nthe relevant information of a dialog context necessary to re -\\ntrieve the correct document.\\nRecent approaches like Sentence-BERT\\n(Reimers and Gurevych 2019) or Dense Passage Re-\\ntrieval (Karpukhin et al. 2020) showed that sentence\\nrepresentations based on pre-trained transformer models\\ncan be used effectively in information retrieval tasks when\\nthey are ﬁne tuned on a task. Similar to them we propose to\\nuse a siamese network structure made of a dialog context\\nencoder and a knowledge snippets encoder so that the\\ndistance between their representation builds a suitable\\nranking function. Thus, this essentially becomes a metric\\nlearning problem. For the encoders, we use pre-trained\\nRoBERTa (Liu et al. 2019b) models. We directly use the\\nlast hidden state of the <s>token. We experiment withtwo different distance functions: the euclidian distance a nd\\nthe dot product.\\nFor the former, we use the triplet loss\\n(Weinberger and Saul 2009), to train both encoders.\\nGiven an anchor, in our case the encoded dialog context,\\nand a positive and negative sample, in our case the relevant\\nand a random irrelevant knowledge snippet, it trains the\\nencoders so that the distance between the anchor and\\npositive sample is lower than the distance to a negative\\nsnippet by a margin ǫ.\\nFor the second method, we use the dot product between\\nthe embeddings created by encoder E1andE2as a similarity\\nmeasure. We train the model, given an anchor a, to correctly\\nclassify a positive sample given the positive sample pand\\na set of negative samples N. Mathematically the loss is the\\nnegative log-likelihood of the correct positive sample:\\nL=−logexp(E1(a)·E2(p))∑\\ns∈N∪{p}exp(E1(a)·E2(s))\\nThe anchor can either be a dialog context and the other sam-\\nples are relevant and irrelevant knowledge snippets or the\\nother way around. To select negative examples we experi-\\nment with two different batching strategies. The ﬁrst is to\\nrandomly sample among the full set of negative samples.\\nAlternatively, instead of randomly sampling negative ex-\\namples, one approach in metric learning tasks is to use hard\\nnegatives, i.e. the samples from the negative class whose\\nembeddings are closest to the anchor. To implement this ef-\\nﬁciently, either only samples in the current batch are con-\\nsidered or the hardest samples are selected from a random\\nsubset. For simplicity, we use the second approach in this\\nwork.\\nSince the embeddings of the knowledge snippets can be\\npre-computed only the embedding of the current dialog con-\\ntext has to be computed during runtime. If the total num-\\nber is rather small, i.e. in the thousands as in our case, the\\nk nearest neighbor search to ﬁnd the closest embedding is\\nnegligible compared to the inference time of the transforme r\\nmodel. Thus, effectively this method is in O(1). Even for a\\nvery large number of knowledge snippets there are efﬁcient\\nmeans of search (Johnson, Douze, and Jegou 2019).\\n4.4 Retrieval Augmented Generation\\nThe baseline approach for response generation only con-\\nsiders the single best selected knowledge snippet. In some\\ncases, multiple snippets might contain relevant informati on\\nto generate a response. Further, by making a hard decision\\nfor a single knowledge snippet in the selection step, we in-\\ntroduce errors that are propagated to the response genera-\\ntion. This motivates us to reformulate our selection and gen -\\neration task into a single task, i.e. to generate a response\\nbased on all of our knowledge snippets. The approach is\\nsimilar to what Lewis et al. (2020b) propose and to other re-\\ntrieval augmented models like REALM (Guu et al. 2020).\\nMathematically, we can formulate this as a marginalization\\nover the selected knowledge snippet kwhich we introduce\\nas a latent variable:\\np(uT+1|uT\\n1;K) =∑\\nk∈Kp(uT+1,k|uT\\n1;K)', 'arXiv:2102.04643v1  [cs.CL]  9 Feb 2021Efﬁcient Retrieval Augmented Generation\\nfrom Unstructured Knowledge for Task-Oriented Dialog\\nDavid Thulke,1,2Nico Daheim,1Christian Dugast,1,2Hermann Ney1,2\\n1Human Language Technology and Pattern Recognition Group, R WTH Aachen University, Germany\\n2AppTek GmbH, Aachen, Germany\\n{thulke, daheim, dugast, ney }@i6.informatik.rwth-aachen.de\\nAbstract\\nThis paper summarizes our work on the ﬁrst track of the ninth\\nDialog System Technology Challenge (DSTC 9), “Beyond\\nDomain APIs: Task-oriented Conversational Modeling with\\nUnstructured Knowledge Access”. The goal of the task is to\\ngenerate responses to user turns in a task-oriented dialog t hat\\nrequire knowledge from unstructured documents. The task is\\ndivided into three subtasks: detection, selection and gene ra-\\ntion. In order to be compute efﬁcient, we formulate the se-\\nlection problem in terms of hierarchical classiﬁcation ste ps.\\nWe achieve our best results with this model. Alternatively, we\\nemploy siamese sequence embedding models, referred to as\\nDense Knowledge Retrieval, to retrieve relevant documents .\\nThis method further reduces the computation time by a factor\\nof more than 100x at the cost of degradation in R@1 of 5-6%\\ncompared to the ﬁrst model. Then for either approach, we use\\nRetrieval Augmented Generation to generate responses base d\\non multiple selected snippets and we show how the method\\ncan be used to ﬁne-tune trained embeddings.\\n1 Introduction\\nTask-oriented dialog systems allow users to achieve cer-\\ntain goals, such as restaurant or hotel reservations, by in-\\nteracting with a system using natural language. Typically,\\nthese systems are restricted to information provided by an\\napplication-speciﬁc interface (API) which accesses a stru c-\\ntured database. However, the breadth of structured informa -\\ntion available is often limited to a set of ﬁxed ﬁelds and may\\nnot satisfy all information needs users may have. The neces-\\nsary information is in many cases already available but only\\nfound in unstructured documents, such as descriptions on\\nwebsites, FAQ documents, or customer reviews. The aim of\\nTrack 1 of the ninth Dialog System Technology Challenge\\n(DSTC 9) (Gunasekara et al. 2020) is to make use of such\\ninformation to provide users with an answer relevant to thei r\\nquestion. To do so, the task at hand is split up into three sub-\\ntasks, namely Knowledge-seeking Turn Detection to iden-\\ntify those questions that can not be answered by an existing\\nAPI, Knowledge Selection to retrieve relevant documents,\\nandResponse Generation to generate a suitable system re-\\nsponse.\\nCopyright © 2021, Association for the Advancement of Artiﬁc ial\\nIntelligence (www.aaai.org). All rights reserved.In a real-world scenario, the amount of time that can be\\nspent on generating a response is limited. Since the num-\\nber of relevant documents can potentially grow large, we\\nnote that efﬁcient means of retrieval are crucial. In the bas e-\\nline approach, this is identiﬁed as the limiting factor. Thu s,\\nour work focuses on the task of Knowledge Selection and,\\nspeciﬁcally, efﬁcient document retrieval methods. For thi s,\\nwe propose two different methods. Our ﬁrst method splits\\nthe task of classifying a relevant document into subsequent\\nstages. Thereby we are not only able to signiﬁcantly reduce\\nthe computational cost, but also to outperform the single-\\nstage classiﬁcation approach by a signiﬁcant margin on both\\ntest sets. Secondly, we employ siamese embedding networks\\nto learn dense document and dialog embeddings, which are\\nthen used to retrieve relevant documents based on vector\\nsimilarity. Additionally, we ﬁne-tune these models jointl y\\nwith a retrieval augmented generation model, achieving re-\\nsults comparable to the baseline. This model can then also be\\nused during generation, to condition the model on multiple\\ndocuments.\\n2 Related Work\\nRecently, the use of pre-trained transformer language mod-\\nels like GPT-2 has shown to be successful for task-oriented\\ndialog systems. It is used either as a model in one of\\nthe subtasks, like response generation, or as an end-to-end\\nmodel for the whole task (Budzianowski and Vuli´ c 2019;\\nHam et al. 2020). Other transformer models which have\\nbeen pre-trained on other tasks such as BERT or RoBERTa\\nhave been shown to outperform GPT-2 on a range of natural\\nlanguage understanding tasks (Devlin et al. 2019; Liu et al.\\n2019b). Recently, Lewis et al. (2020a) proposed BART, an\\nencoder-decoder model pre-trained as a denoising autoen-\\ncoder. On natural language understanding tasks like GLUE\\nand SQuAD, it achieves similar performance to RoBERTa\\nand can be effectively ﬁne-tuned to sequence generation\\ntasks, for example summarization or dialog response gen-\\neration.\\nMost previous works considered the problem of inte-\\ngrating unstructured knowledge into conversations on open\\ndomain dialogs. Existing benchmarks for example include\\nconversations on topics grounded on Wikipedia articles\\n(Dinan et al. 2018) or chats on movies (Moghe et al. 2018).\\nGhazvininejad et al. (2018) propose an encoder-decoder', 'which can then be further split into a selection probability ,\\ni.e. the probability of a knowledge snippet given a dialog\\ncontext, and a generation probability which corresponds to\\nthe baseline model for generation:\\np(uT+1,k|uT\\n1;K) =p(\\nk|uT\\n1;K)\\n\\ued19\\ued18\\ued17\\ued1a\\nselection·generation\\ued17\\ued1a\\ued19 \\ued18\\np(\\nuT+1|uT\\n1,k;K)\\nThe same decomposition can also be applied on the token\\nlevel which allows us to use this as a drop-in replacement\\nfor our current generation probability. To be able to calcul ate\\nthis efﬁciently during training and testing, we approximat e\\nthe sum over all knowledge snippets Kby a sum over the\\ntopnsnippets. To ensure that the model is still normalized,\\nwe renormalize the selection probabilities over this subse t.\\nIn our experiments, we typically use n= 5 and ensure that\\nthe correct knowledge snippet is always included in the top\\nnsnippets during training. For the generation probability,\\nwe use the same model as in the baseline. For the selection\\nprobability, we try all three models discussed so far. In the -\\nory, this model allows us to train the selection and generati on\\nmodels jointly. However, calculating the selection probab ili-\\nties using the relevance classiﬁcation models during train ing\\non the ﬂy is not feasible, even when using the Hierarchical\\nSelection models. Therefore we calculate these probabilit ies\\nin a previous step and keep them ﬁxed during training.\\nFortunately, using the Dense Knowledge Retrieval model,\\ntraining both models jointly becomes feasible. Therefore, we\\nkeep the knowledge snippet encoder ﬁxed and only ﬁne-tune\\nthe dialog context encoder. The top nknowledge snippets\\ncan then be effectively retrieved during training.\\n4.5 Multi-Task Learning\\nMotivated by the recent success of multi-task learning for\\nvarious NLP tasks (Liu et al. 2019a; Raffel et al. 2020), we\\nexplored two approaches to apply the method in this chal-\\nlenge. In the ﬁrst approach, we apply it to the Hierarchical\\nSelection method. Instead of training three separate model s\\n(or two if we classify the domain and entity jointly), we trai n\\na single model on all three relevance classiﬁcation tasks. F or\\nthe second approach, we train a single model on all three\\nsubtasks of the challenge. In both scenarios, we employ hard\\nparameter sharing where the hidden layers are shared among\\nall tasks (Ruder 2017). On top of the shared layers, we add\\nseparate feed-forward layers for each task on top of the ﬁnal\\nlayer output of the [CLS] token, to obtain separate outputs\\nfor each task. For each relevance classiﬁcation task we only\\ninclude the relevant parts of the knowledge snippet into the\\ninput, i.e. to predict the relevant entity we only include th e\\ndomain and the name of the entity but not the content of the\\ndocument and calculate a loss based on the relevant output.\\nWe used the same strategies to sample positive and negative\\nsamples as for the baseline models.\\n5 Data\\nThe training data provided as part of the challenge\\nfor this task is based on the MultiWOZ 2.1 dataset(Budzianowski et al. 2018; Eric et al. 2020). MultiWOZ is\\na task-oriented dialog dataset consisting of 10,438 dialog s\\nspanning 7 different domains. Each of these domains is de-\\nﬁned by an ontology and has a database of corresponding\\nentities. For this challenge, Kim et al. (2020) extended the\\ncorpus by adding user turns requesting information not cov-\\nered by the database, corresponding system responses, and\\nknowledge snippets for each entity. The latter were collect ed\\nfrom the FAQ websites of the corresponding entities occur-\\nring in the corpus. Each snippet consists of a domain, an\\nentity, a question, and an answer. The additional turns were\\ncollected with the help of crowd workers. In total, 21.857\\nnew pairs of turns and 2.900 knowledge snippets were added\\nto the corpus. The training and validation datasets are re-\\nstricted to four domains, namely hotel, restaurant, taxi, a nd\\ntrain. The latter two domains do not contain any entities and\\ncorresponding knowledge snippets are relevant for the whol e\\ndomain.\\nThe organizers of the challenge announced that the ﬁ-\\nnal test data will include additional new domains, entities ,\\nand knowledge snippets. To simulate these conditions when\\nevaluating our approaches, we created an additional traini ng\\ndataset in which we removed all dialogs corresponding to\\nthe train and restaurant domain.\\nThe ﬁnal test data introduced one additional domain: at-\\ntraction. It contains 4.181 additional dialogs of which 1.9 81\\nhave knowledge-seeking turns and 12.039 knowledge snip-\\npets. Around half of these dialogs are from the MultiWOZ\\ndataset augmented as described above. The other half are\\nhuman-to-human conversations about touristic informatio n\\nfor San Francisco. Of these, 90% are written conversations\\nand 10% transcriptions of spoken conversations.\\n6 Experiments\\nWe implemented our proposed methods on top of the base-\\nline implementation provided as part of the challenge1. For\\nthe pre-trained models, we used the implementation and\\ncheckpoints provided by Huggingface’s Transformer librar y\\n(Wolf et al. 2019). We use RoBERTa (Liu et al. 2019b) as a\\npre-trained language model for the encoders of the embed-\\nding models. In all other cases, if not otherwise stated, we\\nuse BART large (Lewis et al. 2020a). For the selection, dia-\\nlog contexts are truncated to maximum length of 384 tokens.\\nTo organize our experiments we used the workﬂow man-\\nager Sisyphus (Peter, Beck, and Ney 2018). All models are\\ntrained on Nvidia GTX 1080 Ti or RTX 2080 Ti GPUs.\\n6.1 Results\\nTables 1 and 2 show our main results on the validation and\\ntest data for all three tasks. The results of the selection an d\\ngeneration are based on the results of the previous subtasks .\\nIn all other tables, these results are based on the ground tru th\\nof the previous task to increase comparability. The ﬁrst two\\nlines in the table for each evaluation dataset compare the re -\\nsults of the baseline method of Kim et al. (2020) and using\\nour general modiﬁcations discussed in Section 4.1. Rows\\nwith entry IDs correspond to the systems we submitted to\\n1Code is available at https://github.com/dthulke/dstc9-t rack1', 'WWW ’24, May 13–17, 2024, Singapore, Singapore Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, & Zhicheng Dou\\nAPPENDIX\\nA IMPLEMENTATION DETAILS\\nA.1 Model Selection\\n•gpt-35-turbo-16k API: https://api.openai.com/v1/chat/completions\\n•Expert monitoring model : https://huggingface.co/gaussalgo/\\nT5-LM-Large_Canard-Fullwiki-HotpotQA.\\n•Similarity model: https://huggingface.co/sentence-transformers/\\nall-MiniLM-L6-v2.\\n•NLI model: https://huggingface.co/google/t5_xxl_true_nli_mixture.\\nA.2 Categorize the Question\\nDuring the task exploration phase, we employed a human annota-\\ntion approach to categorize the questions. We recruited 10 graduate\\nstudents specializing in information retrieval. For each question,\\nwe provided five retrieved documents and one closebook answer\\ngenerated by an LLM. The annotators were tasked with assessing:\\n•Quality of Closebook Answers from the LLM: They deter-\\nmined whether the LLM’s closebook answer could satisfactorily\\naddress the question (can/cannot).\\n•Knowledge Completeness of Retrieved Documents: They\\nevaluated whether the retrieved documents contained sufficient\\ninformation to answer the question (can/cannot).\\nBased on these assessments, we used a voting mechanism among\\nthe annotators to categorize each question into one of the four\\ntypes outlined in Section 3.2. In the actual reasoning phase, the\\ncategorization is supported by the evaluator LLM and NLI model.\\nThe evaluator LLM assesses the internal knowledge (i.e., the LLM’s\\nown understanding and the quality of its closebook answer), while\\nthe NLI model evaluates the external knowledge. This dual evalua-\\ntion helps determine whether the LLM has the necessary internal\\nknowledge and whether the external information is adequate to\\nformulate a correct response.\\nB THE ROLE OF EXPERT MODEL\\n•Role of the Expert Model: The expert model in our system\\nis not used as a provider of ground-truth answers but rather\\nas a benchmark for monitoring the consistency and potential\\ncorrectness of answers. In our experiments, we found that no\\nsingle model, including the expert model, can consistently deliver\\nperfect answers. However, we observed that when the responses\\nof ChatGPT and the fine-tuned expert model align closely, the\\nlikelihood of the answer being correct increases. This alignment\\nserves as a crucial indicator to assess whether an answer is likely\\nto be accurate or requires further refinement.\\n•MetaRAG’s Performance Relative to the Expert Model: In\\nSection 6.2, we conducted tests to evaluate the impact of using\\nexpert models of varying quality. Our findings indicate that the\\nperformance of MetaRAG is indeed influenced by the quality of\\nthe expert model. A higher-quality expert model tends to enhance\\nthe effectiveness of MetaRAG. However, it’s important to note\\nthat MetaRAG’s functionality extends beyond merely replicating\\nthe expert model’s answers. Instead, MetaRAG leverages the\\nexpert model as part of its metacognitive process to evaluate\\nand potentially correct its responses, thereby adding a layer of\\nself-awareness and adaptability to the answer generation process.C PROMPT DESIGNING\\nIn designing prompts for our study, we adhered to the following\\nthree principles to ensure both effectiveness and fairness:\\n•Fairness: We maintained consistency in prompts for similar\\ncomponents across baselines. This approach ensures that any ob-\\nserved differences in performance are attributable to the models\\nthemselves rather than the prompts.\\n•Robustness: We implemented demonstrations and a Chain of\\nThought (CoT) voting strategy. Demonstrations ensure that the\\noutput format aligns with our expectations, while the CoT voting\\nstrategy enhances the stability and consistency of model outputs.\\n•Simplicity: In designing prompts for the Evaluator-critic LLM,\\nwe aimed for brevity to avoid biases introduced by intricate\\nprompt engineering. Excluding demonstrations, our prompts\\nhave an average length of 23 words. This focus on simplicity is\\nto highlight the effectiveness of the metacognitive mechanism\\nindependently of detailed prompt design.\\nD COMPLEXITY AND COST ANALYSIS\\nWe have conducted an in-depth analysis focusing on two critical\\nhyperparameters that influence the balance between performance\\nand computational cost in our proposed MetaRAG method: the\\nThreshold for Metacognitive Evaluation , which determines whether\\nto initiate metacognitive reasoning based on a certain confidence\\nlevel, and the Maximum Iteration Rounds , which controls the maxi-\\nmum number of metacognitive reasoning iterations the model can\\nperform. We conducted experiments on the 2WikiMultiHop dataset\\nto assess the tradeoffs. The results are as follows:\\nTable 4: Results Based on Threshold for Metacognition.\\nModels EM F1 Prec. Rec. Threshold Time(s)\\nReAct 21.0 28.0 27.6 30.0 - 5.36\\nSelf-Ask 28.6 37.5 36.5 42.8 - 6.34\\nMetaRAG 38.0 45.0 45.2 45.8 0.2 5.37\\nMetaRAG 42.8 50.8 50.7 52.2 0.4 8.06\\nMetaRAG 42.0 50.0 49.9 51.6 0.6 8.65\\nMetaRAG 41.4 49.3 49.4 50.6 0.8 11.81\\nTable 5: Results Based on Maximum Iteration Rounds.\\nModels EM F1 Prec. Rec. Max_iter Time(s)\\nReAct 21.0 28.0 27.6 30.0 - 5.36\\nSelf-Ask 28.6 37.5 36.5 42.8 - 6.34\\nMetaRAG 42.8 50.8 50.7 52.2 2 8.06\\nMetaRAG 42.6 50.3 50.2 51.4 3 8.73\\nMetaRAG 43.2 50.7 50.7 51.6 4 11.27\\nMetaRAG 43.4 51.7 51.8 53.0 5 12.92\\nThe results demonstrate that MetaRAG achieves superior per-\\nformance within a comparable inference time. When higher per-\\nformance is required, adjusting the threshold and the maximum\\niteration rounds can effectively enhance the response quality.'], ['Exploring Large Language Models and Retrieval Augmented Generation\\nfor Automated Form Filling\\nMATEI BUCUR, University of Twente, Netherlands\\nLarge language models (LLMs) such as the GPT family have shown re-\\nmarkable natural language processing capabilities across a variety of tasks\\nwithout requiring retraining or fine-tuning. However, leveraging their poten-\\ntial for use cases beyond the traditional chatbot paradigm remains an open\\nchallenge. One potential application is automated form completion, which\\nenables users to fill out online forms using natural language and leverages\\navailable data about the user and the form completion guidelines. This can\\nbenefit a broad range of processes, such as applying for a loan or grant,\\nfiling a tax statement, or requesting a service. However, automated form\\nfilling faces challenges such as understanding form layout, guidelines, and\\nuser intent, as well as reasoning over data, in order to generate accurate and\\ncoherent text. In this paper, I propose a general method for adapting LLMs to\\ndifferent form-filling domains and tasks. The method consists of three steps:\\n(1) creating a knowledge base that contains facts and rules related to the\\nform-filling task; (2) augmenting the LLM with the knowledge base using\\nretrieval-augmented generation; and (3) using prompt engineering tech-\\nniques to improve the outputs. I evaluated the effectiveness of the method\\nand the impact of the techniques on the task of completing request forms\\nfor various incentives and services.\\nAdditional Key Words and Phrases: Large language models, gpt3.5, retrieval\\naugmented generation, form filling\\n1 INTRODUCTION\\n1.1 Motivation\\nForms are an essential way to extract information from people and\\nhave been used for a long time in various domains and contexts.\\nThey are a tool used to collect information in a structured and\\nstandardised way by means of a printed document with spaces in\\nwhich answers to questions can be written. [ 3] Forms are needed\\nfor interacting with different institutions. Several industries, like\\nhealthcare, finance, government, law, and education, rely heavily on\\ntheir usage. Every person faces a high likelihood of having to deal\\nwith form filling throughout their life, as it is a pervasive activity\\nneeded in common endeavours such as reporting taxes, applying for\\na credit card, or registering insurance. I have identified five types\\nof forms based on their purpose and format: request, registration,\\nconsent, evaluation, application, report, and declaration. Form fill-\\ning is typically not a creative task but a functional one in which\\naccuracy, compliance with guidelines, and attention to detail are\\nvalued. However, it is common for people to make mistakes while\\ncompleting them. According to an expert in grant applications, com-\\nmon mistakes include not following the instructions, not providing\\nenough detail, and not explaining the significance of the project\\nTScIT 37, July 7, 2023, Enschede, The Netherlands\\n©2023 University of Twente, Faculty of Electrical Engineering, Mathematics and\\nComputer Science.\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. To copy otherwise, or republish, to post on servers or to redistribute\\nto lists, requires prior specific permission and/or a fee.[9,10]. Therefore, the task of form filling is considered to be time-\\nconsuming, bureaucratic, and tedious[ 4,31].This could cause users\\nto abandon the form or provide inaccurate or incomplete informa-\\ntion. A survey found that 81% of people discontinued filling out an\\nonline form midway through the process, and out of those, 67% of\\nthem chose to terminate the process completely if they encountered\\nany difficulties[2].\\nIn addition to filling out forms for personal use, this process is\\ncommon and crucial in the business world, 59% of US workers hav-\\ning to use forms in their jobs[ 35]. Forms are frequently employed as\\na tool to organise and standardise the data that organisations must\\ngather, handle, and analyse from multiple sources and stakeholders.\\nBusinesses deal with a variety of unstructured, dynamic data sources\\nthat must be accessed and comprehended. Even though searching\\nis now commonplace, the problem has not yet been resolved. An\\nexcellent illustration of this is enterprise search, which is, roughly\\nspeaking, the use of information retrieval technology to find infor-\\nmation within organisations. Despite having great financial value,\\nthe topic has received little academic attention. [ 18] In order to be\\nable to fill in forms accurately, users must obtain and grasp informa-\\ntion from numerous sources, such as documents, databases, or web\\nsites, about the form guidelines and requirements. This requires\\nstrong enterprise search capabilities. Enterprise search faces unique\\nchallenges and problems that make it notoriously difficult to achieve\\nuser satisfaction and have significant economic importance[ 13]. One\\nof the challenges identified is that context-aware search is a chal-\\nlenging problem for enterprise search systems, where users often\\nhave diverse and dynamic information goals that depend on various\\naspects of their work context.\\nIn the digital age, online forms are the most common method\\nof data collection since they allow for inexpensive and effective\\nstorage, processing, and analysis. However, online forms also pose\\nsome challenges and limitations for users. To help improve the\\nefficiency of form filling, web browsers introduced the autofill and\\nautocomplete functions[ 1]. The autofill functionality fills in the\\nfields of a web form based on previously entered data, while the\\nautocomplete functions suggest or complete the fields based on\\nwhat the user has typed. While those are successful in easing the\\ncompletion of simple fields, they do not consider the broader context\\nof the form and the user and are not able to assist in more complex\\nones.\\nTo overcome the limitations of existing solutions, we require a\\nflexible system that is able to understand the context, requirements\\nand generate text. In this context, Natural Language Generation\\n(NLG), a sub-field of natural language processing, is relevant, as it\\naims to generate text using input data (prompts, tables, images, etc.).\\n[11]. The transformer is a neural network architecture that lever-\\nages self-attention to encode and decode sequences without using\\nrecurrent or convolutional layers. It was first presented in [ 33] and\\nhas since been widely applied to NLP tasks. This innovative design\\n1', 'Exploring Large Language Models and Retrieval Augmented Generation for Automated Form Filling TScIT 37, July 7, 2023, Enschede, The Netherlands\\nTable 1. Evaluation results for different variants of prompts\\nVariant f1 score BERTscore P BERTscore R BERTscore F1 G-Eval\\n0 (vanilla) 0.30 0.43 0.44 0.43 2.37\\n1 (CoT + cite sources + optimized prompt) 0.31 0.71 0.72 0.71 2.41\\n2 (CoT only) 0.34 0.53 0.54 0.54 2.38\\nTable 2. Evaluation results for different variants of contexts\\nVariant Evaluation f1 BERTscore ROUGE-1 ROUGE-2 ROUGE-L\\n0 Form guidelines 0.09 0.65 0.11 0.01 0.1\\n1 Project description 0.23 0.74 0.29 0.07 0.28\\n2 No supporting docs 0.13 0.72 0.14 0 0.14\\nthat is still in the early stages of development and has shown a\\nbias towards LLM-generated texts. One alternative is BERTScore,\\nan embedding-based metric that compares the contextual embed-\\ndings of the generated text and the ground truth[ 40]. I evaluated the\\nForms Copilot on the proprietary dataset with the metrics described\\nabove.\\n6 DISCUSSION\\nIn 1, it can be seen the impact of various prompt engineering tech-\\nniques. The results of the experiment show that the version of the\\nForms Copilot that leverages all data sources and responses, uses\\nchain of thought prompting, asks the model to cite the sources,\\nand provides a description of the form has achieved an average of\\n2.41 / 5 G-EVAL score and 0.71 F1 BERTscore, indicating that the\\nsuggestions are similar to the ground truth. This indicates that an\\nLLM based approach may be able to generate suitable responses\\nfor the form-filling task that can later be reviewed and edited, thus\\nindicating a high probability of being more time-efficient. Further-\\nmore, this shows that the model can benefit from a more detailed\\ndescription of the task and from breaking it down into sub-tasks.\\nFinally, the model can generate more accurate and relevant infor-\\nmation by citing the sources, which also greatly benefits the users\\nby increasing the transparency and explainability of the AI system.\\nThe results in2 show that the documents directly related to the\\nform-filling task have the greatest impact on the quality of the\\ngenerated result. Also, the form guidelines and rules decrease the\\nperformance of the model if they are not accompanied by specific\\ndata that needs to be in the form. A qualitative analysis of the results\\nshows that in that case, the model tends to generate more guidelines\\ninstead of values, which is not desirable and explains the low score.\\nThis shows the importance of providing the right data for the model.\\n6.1 Limitations\\nThis research presents an architecture and conducts a preliminary\\nexperiment with LLMs for the task of form filling. However, I ac-\\nknowledge that the evaluation is not comprehensive and only covers\\none domain-specific use case, which may affect the applicability of\\nthis approach to other scenarios. Further experiments are needed to\\nensure the generalizability of the method in other contexts. Also, the\\ngeneration of accurate results relies on the presence of relevant ex-\\nternal documents, which may not be available in the broader contextof form filling. The frontend’s functionality of extraction of labels\\nand autofilling is contingent on the webpage’s implementation and\\nfaces many obstacles, such as complex and inconsistent HTML code.\\nTherefore, a more robust frontend component would be desirable for\\na production-level application. Finally, the system does not address\\nthe ethical and social implications of using an AI system for form\\nfilling. Further research is needed in order to provide guidelines\\nregarding appropriate use cases as well as potential ethical risks.\\n6.2 Future work\\nIn the future, I would envision form-filling software that would be\\nadaptable to any form-filling scenario with no code changes. Further\\nresearch would need to be conducted on a user testing study in order\\nto test if this approach improves productivity, form filling accuracy,\\nand efficiency. Finally, more thorough experimentation is needed\\nto test the impact of different configurations of the retrieval and\\ngeneration parts of the system. Despite the encouraging results of\\nthis research, there is a lot of room for development. Therefore, it\\nis important that academia continue to be involved in the field of\\ngenerative AI and ensure that LLM applications are developed in a\\nconscious and responsible manner.\\nRQ: How can we design a large language model based system that\\nleverages and adapts the model’s pre-trained knowledge with\\nretrieval-augmented generation of form completions in the context\\nof enterprise search?\\nAnswer: We can design a large language model based system\\nthat leverages and adapts the model’s pre-trained knowledge with\\nretrieval-augmented generation of form completions in the context\\nof enterprise search by following these steps:\\n(1)Augment the LMM by providing documents, elaborate de-\\nscriptions, instructions and user input related to the specific\\nform-filling task directly in the prompt\\n(2)Use prompt engineering techniques to improve the outputs,\\nsuch as chain-of-thought prompting\\n(3)Create a knowledge base that contains facts and rules related\\nto the form-filling task, such as guidelines, policies, FAQs,\\nand user-specific documents.\\n(4)Augment the LLM with the knowledge base using retrieval-\\naugmented generation\\n5', 'TScIT 37, July 7, 2023, Enschede, The Netherlands Author\\n7 CONCLUSION\\nIn this paper, I have proposed an architecture and a preliminary\\nexperiment for a form filling application that uses LLMs to generate\\nsuggestions for online forms. The software is composed of two parts:\\na frontend component that reads form labels and autofills forms, and\\na backend component that creates the form completions using an\\nLLM and external data sources. The preliminary experiment reveals\\nthat the LLM-based approach may create appropriate responses for\\none form filling activity, which can then be reviewed and edited,\\nshowing a high probability of increasing the user’s productivity.\\nREFERENCES\\n[1]2012. Making form-filling faster, easier and smarter | Google Search Central\\nBlog. https://developers.google.com/search/blog/2012/01/making-form-filling-\\nfaster-easier-and\\n[2]2018. 6 Steps for Avoiding Online Form Abandonment. https://themanifest.com/\\nweb-design/blog/6-steps-avoid-online-form-abandonment\\n[3] 2023. form. https://dictionary.cambridge.org/dictionary/english/form\\n[4]Hichem Belgacem, Xiaochen Li, Domenico Bianculli, and Lionel C. Briand. 2023.\\nA Machine Learning Approach for Automated Filling of Categorical Fields in Data\\nEntry Forms. ACM Transactions on Software Engineering and Methodology 32, 2\\n(April 2023), 1–40. https://doi.org/10.1145/3533021 arXiv:2202.08572 [cs].\\n[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learn-\\ners. In Advances in Neural Information Processing Systems , Vol. 33. Curran As-\\nsociates, Inc., 1877–1901. https://papers.nips.cc/paper_files/paper/2020/hash/\\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\\n[6]Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\\nGeneral Intelligence: Early experiments with GPT-4. http://arxiv.org/abs/2303.\\n12712 arXiv:2303.12712 [cs].\\n[7]Hai Dang, Lukas Mecke, Florian Lehmann, Sven Goller, and Daniel Buschek.\\n2022. How to Prompt? Opportunities and Challenges of Zero- and Few-Shot\\nLearning for Human-AI Interaction in Creative Applications of Generative Models.\\nhttps://doi.org/10.48550/arXiv.2209.01390 arXiv:2209.01390 [cs].\\n[8]Nouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and Siva Reddy. 2022. On\\nthe Origin of Hallucinations in Conversational Models: Is it the Datasets or the\\nModels? http://arxiv.org/abs/2204.07931 arXiv:2204.07931 [cs].\\n[9]John Ellery. 2013. Avoiding common mistakes on grant applications. Headteacher\\nUpdate 2013, 6 (June 2013), htup.2013.13.6.98917. https://doi.org/10.12968/htup.\\n2013.13.6.98917\\n[10] John Ellery. 2013. Get that funding – avoiding common grant application mistakes.\\nInSecEd , Vol. 2013. sece.2013.6.1783. https://doi.org/10.12968/sece.2013.6.1783\\nISSN: 1479-7704 Issue: 6 Journal Abbreviation: SecEd.\\n[11] Giacomo Frisoni, A. Carbonaro, G. Moro, Andrea Zammarchi, and Marco\\nAvagnano. 2022. NLG-Metricverse: An End-to-End Library for Evaluating\\nNatural Language Generation. https://www.semanticscholar.org/paper/NLG-\\nMetricverse%3A-An-End-to-End-Library-for-Natural-Frisoni-Carbonaro/\\n9f158e69eb2ccf5dce78cd50f4be3cff99b25ca8\\n[12] ggailey777. 2023. Getting started with Azure Functions. https://learn.microsoft.\\ncom/en-us/azure/azure-functions/functions-get-started\\n[13] David Hawking. 2004. Challenges in Enterprise Search. (2004).\\n[14] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo\\nSchick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.\\n2022. Atlas: Few-shot Learning with Retrieval Augmented Language Models.\\nhttp://arxiv.org/abs/2208.03299 arXiv:2208.03299 [cs].\\n[15] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu,\\nYiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Augmented\\nGeneration. http://arxiv.org/abs/2305.06983 arXiv:2305.06983 [cs].\\n[16] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017. Billion-scale similarity\\nsearch with GPUs. http://arxiv.org/abs/1702.08734 arXiv:1702.08734 [cs].\\n[17] Jeff Hervé Jegou Johnson, Matthijs Douze. 2017. Faiss: A library for efficient\\nsimilarity search. https://engineering.fb.com/2017/03/29/data-infrastructure/\\nfaiss-a-library-for-efficient-similarity-search/\\n[18] Udo Kruschwitz and Charlie Hull. 2017. Searching the Enterprise. Foundations\\nand Trends ®in Information Retrieval 11, 1 (July 2017), 1–142. https://doi.org/10.1561/1500000053 Publisher: Now Publishers, Inc..\\n[19] langchain. 2022. LangChain | LangChain. https://docs.langchain.com/docs/\\n[20] Jieh-Sheng Lee and Jieh Hsiang. 2020. Patent claim generation by fine-tuning\\nOpenAI GPT-2. World Patent Information 62 (Sept. 2020), 101983. https://doi.org/\\n10.1016/j.wpi.2020.101983\\n[21] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\\nNaman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,\\nSebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks. https://arxiv.org/abs/2005.11401v4\\n[22] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\\nInText Summarization Branches Out . Association for Computational Linguistics,\\nBarcelona, Spain, 74–81. https://aclanthology.org/W04-1013\\n[23] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian,\\nHao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Dajiang Zhu, Xiang\\nLi, Ning Qiang, Dingang Shen, Tianming Liu, and Bao Ge. 2023. Summary of\\nChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language\\nModels. http://arxiv.org/abs/2304.01852 arXiv:2304.01852 [cs].\\n[24] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang\\nZhu. 2023. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment.\\nhttp://arxiv.org/abs/2303.16634 arXiv:2303.16634 [cs].\\n[25] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson,\\nand Mark Steedman. 2023. Sources of Hallucination by Large Language Models\\non Inference Tasks. http://arxiv.org/abs/2305.14552 arXiv:2305.14552 [cs].\\n[26] OpenAI. 2023. GPT-4 Technical Report. http://arxiv.org/abs/2303.08774\\narXiv:2303.08774 [cs].\\n[27] doc openai. 2023. OpenAI Platform. https://platform.openai.com\\n[28] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\\nMethod for Automatic Evaluation of Machine Translation. In Proceedings of the\\n40th Annual Meeting of the Association for Computational Linguistics . Association\\nfor Computational Linguistics, Philadelphia, Pennsylvania, USA, 311–318. https:\\n//doi.org/10.3115/1073083.1073135\\n[29] Shrimai Prabhumoye, Rafal Kocielnik, Mohammad Shoeybi, Anima Anandkumar,\\nand Bryan Catanzaro. 2022. Few-shot Instruction Prompts for Pretrained Language\\nModels to Detect Social Biases. http://arxiv.org/abs/2112.07868 arXiv:2112.07868\\n[cs].\\n[30] promptflow. 2023. Harness the power of Large Language Models with Azure\\nMachine Learning prompt flow. https://techcommunity.microsoft.com/t5/ai-\\nmachine-learning-blog/harness-the-power-of-large-language-models-with-\\nazure-machine/ba-p/3828459 Section: AI - Machine Learning Blog.\\n[31] Enrico Rukzio, Chie Noda, Alexander De Luca, John Hamard, and Fatih Coskun.\\n2008. Automatic form filling on mobile devices. Pervasive and Mobile Computing\\n4, 2 (April 2008), 161–181. https://doi.org/10.1016/j.pmcj.2007.09.001\\n[32] Tim Schopf, Daniel Braun, and Florian Matthes. 2021. Lbl2Vec: An Embedding-\\nBased Approach for Unsupervised Document Retrieval on Predefined Topics.\\nInProceedings of the 17th International Conference on Web Information Sys-\\ntems and Technologies . 124–132. https://doi.org/10.5220/0010710300003058\\narXiv:2210.06023 [cs].\\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All\\nYou Need. http://arxiv.org/abs/1706.03762 arXiv:1706.03762 [cs].\\n[34] Maxim Vidgof, Stefan Bachhofner, and Jan Mendling. 2023. Large Language\\nModels for Business Process Management: Opportunities and Challenges. (2023).\\nhttps://doi.org/10.48550/ARXIV.2304.04309 Publisher: arXiv Version Number: 1.\\n[35] Paul Viola and Mukund Narasimhan. 2005. Learning to extract information from\\nsemi-structured text using a discriminative context free grammar. In Proceedings\\nof the 28th annual international ACM SIGIR conference on Research and development\\nin information retrieval (SIGIR ’05) . Association for Computing Machinery, New\\nYork, NY, USA, 330–337. https://doi.org/10.1145/1076034.1076091\\n[36] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tat-\\nsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022.\\nEmergent Abilities of Large Language Models. http://arxiv.org/abs/2206.07682\\narXiv:2206.07682 [cs].\\n[37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,\\nEd Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits\\nReasoning in Large Language Models. https://doi.org/10.48550/arXiv.2201.11903\\narXiv:2201.11903 [cs].\\n[38] Ryen W White. 2023. Tasks, Copilots, and the Future of Search. (2023).\\n[39] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming\\nJiang, Bing Yin, and Xia Hu. 2023. Harnessing the Power of LLMs in Practice: A Sur-\\nvey on ChatGPT and Beyond. http://arxiv.org/abs/2304.13712 arXiv:2304.13712\\n[cs].\\n[40] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.\\n2020. BERTScore: Evaluating Text Generation with BERT. https://doi.org/10.\\n48550/arXiv.1904.09675 arXiv:1904.09675 [cs].\\n6', 'TScIT 37, July 7, 2023, Enschede, The Netherlands Author\\nFig. 1. Architecture of the system: given the user data X (form guidelines), the extension sends it to the backend for the creation of the FAISS index storage;\\nthe search index is later used in the generation of the form completion, together with user data Y(documents relevant for the form) and the scraped labels of\\nthe web form\\norder to find the top 3 most relevant chunks to the label. The name\\nof the document for each chunk is appended, and all the formatted\\nchunks are added as context to the prompt. In addition, the text from\\nthe user documents is extracted and added to the prompt. Finally,\\nthe prompt adds the description of the general form-filling task and\\nthe generation instruction that asks the gpt3.5 model to produce\\nthe result in the form of a JSON object with three fields: reasoning,\\nsources, and value. After the response is generated, the object is\\nparsed, and the value of the field is returned.\\n4 EXPERIMENTAL SETUP\\nThe proprietary dataset consists of 50 web page files of request forms\\nfor various IT projects, submitted in the last 2 years by different em-\\nployees. Each form is accompanied by a short document describing\\nthe implementation and impact, as well as a collection of documents\\nwith general guidelines and rules related to the process. Each form\\nhas approximately 30 fields of various complexity, spanning from\\nfilling out the name of the requester to justifications and inquiries\\nabout the project plan. The fields are mainly text and number inputs\\nbut also multiple choice, with slight variations in the field labels de-\\npending on the version of the form. Each form and document were\\nprocessed as a list of JSON objects, with each object representing\\none form field. The objects contain key-value pairs that represent\\nthe id of the form, the field label, the ground-truth value, and thetext of the document. Due to the high costs of generating and eval-\\nuating the outputs of the generative model, I follow previous works\\napproaches [ 15] to subsample the dataset, in this case to 287 fields\\nfor the evaluation. To execute the evaluation, I ran the experiments\\non a cloud environment using a Standard-DS11-v2 Virtual Machine\\n(2 cores, 14 GB RAM, 28 GB disc) with a memory-optimized CPU.\\n5 EVALUATION\\nEvaluation is a great challenge in this research, and it is in general\\nfor LLMs because they can generate diverse and fluent responses\\nthat may not be easily comparable or verifiable by existing methods.\\nIt is acknowledged that there is a need for metrics that reflect the\\nsystem’s human-like cognition rather than its constrained AI coun-\\nterparts to assess their performance and intellect [ 6]. Traditional\\nmetrics, such as ROUGE and BLEU[ 22,28], focus on lexical overlap\\nwith reference texts, which may miss the semantics and complexity\\nof created texts. An alternative is G-EVAL, a methodology for as-\\nsessing the similarity of NLG outputs that combines large language\\nmodels (LLMs) with chain-of-thought (CoT)[ ?]. G-EVAL requests\\nthe LLM create comprehensive evaluation stages based on the task\\nand criteria and then score the results in a form-filling paradigm.\\nOn two NLG tasks, text summarization and dialogue production,\\nG-EVAL has been shown to outperform the state-of-the-art eval-\\nuators and achieve higher human correspondence [ 24]. However,\\n4'], [\"13 \\n In this section, we delve into the details of the RAG model, which is employed as an implementation method in this \\npaper. The RAG model stores inf ormation, leverages databases, and enhances the generation process through the \\ninclusion of relevant reference materials, ultimately leading to improved answer quality and reliability.  \\n \\n2.3.1  Architecture of the RAG Model   \\nThe RAG (Retrieval -Augmented Generation ) model is designed for text generation tasks, performing a process \\nthat involves retrieving information from given source data and utilizing that information to generate desired text. \\nFig 6 illustrates the data processing pipeline for RAG usage, involving  breaking down the original data into \\nsmaller chunks and converting text data into numerical vectors through embedding, which are then stored in a \\nvector repository  [13]. \\n \\n• Source Data Collection and Preparation : Relevant source data is required for the model's training and \\nutilization. This data can include documents, web pages, news articles, etc. It forms the foundation for the \\nmodel to search for and generate content.  \\n• Chunking of Searchable Units: Source da ta is divided into smaller units known as chunks. Chunks are typically \\nsmall text fragments, such as sentences or paragraphs, making it easier to search for and utilize information at \\nthis granular level.  \\n• Embedding: Generated chunks undergo embedding, a process of converting text into meaningful vector \\nrepresentations. Pre -trained language models are often used to transform text into dense vectors, capturing the \\nmeaning and related information in vector form.  \\n• Construction of Vector Database: A vector da tabase is built based on the embedded chunks. This database \\nrepresents the positions of each chunk within the vector space, enabling efficient retrieval and similarity \\ncalculations.  \\n• Search and Information Integration: To retrieve information relevant to the context of the text to be generated, \\nappropriate chunks are searched within the vector database. The retrieved chunks are decoded back into \\noriginal text data to extract information, which is then utilized during the generation process.  \\n• Text Generati on: Using the retrieved information as a basis, text is generated. Users can specify the type, \\nlength, and linguistic style of the text to be generated. The RAG model is designed to seamlessly integrate \\ninformation retrieval and generation processes, aimin g to produce more accurate and meaningful text outputs.  \\n \\n \\n \\nFig 6 RAG Model  Diagram (Microsoft, 2023)\", 'certain. Initial studies [Wang et al. , 2023b ]have begun to ad-\\ndress this, yet the parameter count in RAG models still lags\\nbehind that of LLMs. The possibility of an Inverse Scaling\\nLaw9, where smaller models outperform larger ones, is par-\\nticularly intriguing and merits further investigation.\\nProduction-Ready RAG . RAG’s practicality and alignment\\nwith engineering requirements have facilitated its adoption.\\nHowever, enhancing retrieval efficiency, improving document\\nrecall in large knowledge bases, and ensuring data secu-\\nrity—such as preventing inadvertent disclosure of document\\nsources or metadata by LLMs—are critical engineering chal-\\nlenges that remain to be addressed [Alon et al. , 2022 ].\\nModality Extension of RAG\\nRAG has transcended its initial text-based question-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage . RA-CM3 [Yasunaga et al. , 2022 ]stands as a pio-\\nneering multimodal model of both retrieving and generating\\ntext and images. BLIP-2 [Liet al. , 2023a ]leverages frozen\\nimage encoders alongside LLMs for efficient visual language\\npre-training, enabling zero-shot image-to-text conversions.\\nThe “Visualize Before You Write” method [Zhuet al. , 2022 ]\\nemploys image generation to steer the LM’s text generation,\\nshowing promise in open-ended text generation tasks.\\nAudio and Video . The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [Zhao et al. , 2022 ]. UEOP marks\\na significant advancement in end-to-end automatic speech\\nrecognition by incorporating external, offline strategies for\\nvoice-to-text conversion [Chan et al. , 2023 ]. Additionally,\\nKNN-based attention fusion leverages audio embeddings and\\nsemantically related text embeddings to refine ASR, thereby\\naccelerating domain adaptation. Vid2Seq augments language\\nmodels with specialized temporal markers, facilitating the\\nprediction of event boundaries and textual descriptions within\\na unified output sequence [Yang et al. , 2023a ].\\nCode . RBPS [Nashid et al. , 2023 ]excels in small-scale\\nlearning tasks by retrieving code examples that align with de-\\nvelopers’ objectives through encoding and frequency analy-\\nsis. This approach has demonstrated efficacy in tasks such as\\ntest assertion generation and program repair. For structured\\nknowledge, the CoK method [Liet al. , 2023c ]first extracts\\nfacts pertinent to the input query from a knowledge graph,\\nthen integrates these facts as hints within the input, enhancing\\nperformance in knowledge graph question-answering tasks.\\n8.2 Ecosystem of RAG\\nDownstream Tasks and Evaluation\\nRAG has shown considerable promise in enriching language\\nmodels with the capacity to handle intricate queries and pro-\\nduce detailed responses by leveraging extensive knowledge\\nbases. Empirical evidence suggests that RAG excels in a\\nvariety of downstream tasks, including open-ended question\\nanswering and fact verification. The integration of RAG not\\nonly bolsters the precision and relevance of responses but also\\ntheir diversity and depth.\\n9https://github.com/inverse-scaling/prizeThe scalability and versatility of RAG across multiple do-\\nmains warrant further investigation, particularly in special-\\nized fields such as medicine, law, and education. In these ar-\\neas, RAG could potentially reduce training costs and enhance\\nperformance compared to traditional fine-tuning approaches\\nin professional domain knowledge question answering.\\nConcurrently, refining the evaluation framework for RAG\\nis essential to maximize its efficacy and utility across different\\ntasks. This entails the development of nuanced metrics and\\nassessment tools that can gauge aspects such as contextual\\nrelevance, creativity of content, and non-maleficence.\\nFurthermore, improving the interpretability of RAG-driven\\nmodels continues to be a key goal. Doing so would allow\\nusers to understand the reasoning behind the responses gener-\\nated by the model, thereby promoting trust and transparency\\nin the use of RAG applications.\\nTechnical Stack\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.\\nEmerging technical stacks, while not as feature-rich as\\nLangChain and LLamaIndex, distinguish themselves with\\nspecialized offerings. For instance, Flowise AI10prioritizes a\\nlow-code approach, enabling users to deploy AI applications,\\nincluding RAG, through a user-friendly drag-and-drop inter-\\nface. Other technologies like HayStack, Meltano11, and Co-\\nhere Coral12are also gaining attention for their unique con-\\ntributions to the field.\\nIn addition to AI-focused providers, traditional software\\nand cloud service providers are expanding their offerings to\\ninclude RAG-centric services. Verba13from Weaviate is de-\\nsigned for personal assistant applications, while Amazon’s\\nKendra14provides an intelligent enterprise search service, al-\\nlowing users to navigate through various content repositories\\nusing built-in connectors. During the evolution of the RAG\\ntechnology landscape, there has been a clear divergence to-\\nwards different specializations, such as: 1) Customization.\\nTailoring RAG to meet a specific requirements. 2) Simpli-\\nfication. Making RAG easier to use, thereby reducing the ini-\\ntial learning curve. 3) Specialization. Refining RAG to serve\\nproduction environments more effectively.\\nThe mutual growth of RAG models and their technical\\nstack is evident; technological advancements consistently es-\\ntablish new standards for the existing infrastructure. In turn,\\nenhancements to the technical stack drive the evolution of\\nRAG capabilities. The RAG toolkit is converging into a foun-\\ndational technical stack, laying the groundwork for advanced\\nenterprise applications. However, the concept of a fully in-\\ntegrated, comprehensive platform remains on the horizon,\\npending further innovation and development.\\n10https://flowiseai.com\\n11https://meltano.com\\n12https://cohere.com/coral\\n13https://github.com/weaviate/Verba\\n14https://aws.amazon.com/cn/kendra/', \"20 \\n 3.2.1  RAG based implementation procedure  \\nThe RAG model, as discussed in the architecture of the RAG model in Chapter 2, is a search -augmented \\ngenerative model used to retrieve and generate responses based on information relevant to given questions or topics. \\nEach step follows the procedure outlined in Fig 9.  \\n1) Source Data Collection and Extraction: During the data collection phase, both structured and unstructured \\ndata are gathered. Structured data is stored in standardized formats such as CSV, JSON, or XML, while \\nunstructured data is stored in formats like PDF, TXT, HTML, images, and videos. Preparatory materials \\nrelated to the task, such as regulations, user manuals, and terms and c onditions, are loaded into \\nLangChain using the LangChain module.  \\n2) Chunk Generation: Source data is processed to split it into smaller units known as chunks. These chunks \\ntypically consist of sentences or paragraphs, serving as smaller text fragments that ca n be used to search \\nand retrieve information from LLM. LangChain's module is utilized to split data into chunks that are suitable \\nfor retrieval.  \\n3) Embedding: The generated chunk -level text data is transformed into numerical vector representations. This \\nstep involves mapping words or sentences to vectors, and libraries provided by OpenAI or GPT4All can be \\nemployed for this purpose.  \\n4) Building the Vector Database: Based on the embedded chunks, the vector database is constructed. This \\ndatabase represents the posit ions of each chunk in the vector space, facilitating efficient search and \\nsimilarity calculations. Typically, content from each document is included, embeddings and documents are \\nstored in the vector repository, and documents are indexed using embeddings. Tools like Chroma or FAISS \\nfor vector indexing can be used.  \\n5) Integration of Prompt and Search Results: This step involves searching for information based on the \\nprompted question and integrating relevant information. To search for contextually relevant info rmation \\nbased on the prompt, appropriate chunks are retrieved from the vector database. These retrieved chunks \\nare then sent to LLM to aid in the response generation process. Various search engines available within \\nLangChain for vector store similarity sea rch are utilized.  \\n6) Answer Generation: Using the retrieved information as a basis, the response text is generated. At this stage, \\nthe type, length, and linguistic style of the generated text can be specified. LLM, such as OpenAI's \\nGPT-3.5-turbo model or GPT4 All, uses the similarity search module in LangChain to retrieve relevant \\ndocuments and generate responses.  \\nThe RAG -based implementation procedure outlined above illustrates how the RAG model, in combination with \\nLangChain, can be effectively integrated int o the generative AI service framework. This comprehensive \\napproach covers data collection, processing, embedding, search, and response generation, ensuring that the \\nservice provides accurate and contextually relevant answers to user queries.  \\nIV. Experiment  \\nIn this chapter, the generative AI service implementation framework introduced in Chapter 3 is utilized to implement \\nvarious scenarios based on enterprise internal data using the integrated RAG model and LangChain according to the \\nimplementation procedure . This provides a series of practical examples for each implementation step. Through these \\nexamples, we explore the methods of implementation and consider the factors to be taken into account during the \\nconstruction process.\", 'perspectives. Additionally, we anticipate future direc-\\ntions for RAG, emphasizing potential enhancements to\\ntackle current challenges, expansions into multi-modal\\nsettings, and the development of its ecosystem.\\nThe paper unfolds as follows: Section 2 and 3 define RAG\\nand detail its developmental process. Section 4 through 6 ex-\\nplore core components—Retrieval, “Generation” and “Aug-\\nmentation”—highlighting diverse embedded technologies.\\nSection 7 focuses on RAG’s evaluation system. Section 8\\ncompare RAG with other LLM optimization methods and\\nsuggest potential directions for its evolution. The paper con-\\ncludes in Section 9.\\n2 Definition\\nThe definition of RAG can be summarized from its workflow.\\nFigure 2 depicts a typical RAG application workflow. In this\\nscenario, a user inquires ChatGPT about a recent high-profile\\nevent (i.e., the abrupt dismissal and reinstatement of Ope-\\nnAI’s CEO) which generated considerable public discourse.\\nChatGPT as the most renowned and widely utilized LLM,\\nconstrained by its pretraining data, lacks knowledge of re-\\ncent events. RAG addresses this gap by retrieving up-to-date\\ndocument excerpts from external knowledge bases. In this in-\\nstance, it procures a selection of news articles pertinent to the\\ninquiry. These articles, alongside the initial question, are then\\namalgamated into an enriched prompt that enables ChatGPT\\nto synthesize an informed response. This example illustrates\\nthe RAG process, demonstrating its capability to enhance the\\nmodel’s responses with real-time information retrieval.\\nTechnologically, RAG has been enriched through various\\ninnovative approaches addressing pivotal questions such as\\n“what to retrieve” “when to retrieve” and “how to use the\\nretrieved information”. For “what to retrieve” research has\\nprogressed from simple token [Khandelwal et al. , 2019 ]and\\nentity retrieval [Nishikawa et al. , 2022 ]to more complex\\nstructures like chunks [Ram et al. , 2023 ]and knowledge\\ngraph [Kang et al. , 2023 ], with studies focusing on the\\ngranularity of retrieval and the level of data structur-\\ning. Coarse granularity brings more information but\\nwith lower precision. Retrieving structured text provides\\nmore information while sacrificing efficiency. The ques-\\ntion of “when to retrieve” has led to strategies ranging\\nfrom single [Wang et al. , 2023e, Shi et al. , 2023 ]to adap-\\ntive [Jiang et al. , 2023b, Huang et al. , 2023 ]and multiple\\nretrieval [Izacard et al. , 2022 ]methods. High frequency of\\nretrieval brings more information and lower efficiency. As\\nfor ”how to use” the retrieved data, integration techniques\\nhave been developed across various levels of the model\\narchitecture, including the input [Khattab et al. , 2022 ],\\nintermediate [Borgeaud et al. , 2022 ], and output lay-\\ners[Liang et al. , 2023 ]. Although the “intermediate” and\\n“output layers” are more effective, there are problems with\\nthe need for training and low efficiency.\\nRAG is a paradigm that enhances LLMs by integrating ex-\\nternal knowledge bases. It employs a synergistic approach,\\ncombining information retrieval mechanisms and In-Context\\nLearning (ICL) to bolster the LLM’s performance. In this\\nframework, a query initiated by a user prompts the retrieval ofpertinent information via search algorithms. This information\\nis then woven into the LLM’s prompts, providing additional\\ncontext for the generation process. RAG’s key advantage lies\\nin its obviation of the need for retraining of LLMs for task-\\nspecific applications. Developers can instead append an ex-\\nternal knowledge repository, enriching the input and thereby\\nrefining the model’s output precision. RAG has become one\\nof the most popular architectures in LLMs’ systems, due to\\nits high practicality and low barrier to entry, with many con-\\nversational products being built almost entirely on RAG.\\nThe RAG workflow comprises three key steps. First, the\\ncorpus is partitioned into discrete chunks, upon which vec-\\ntor indices are constructed utilizing an encoder model. Sec-\\nond, RAG identifies and retrieves chunks based on their vec-\\ntor similarity to the query and indexed chunks. Finally, the\\nmodel synthesizes a response conditioned on the contextual\\ninformation gleaned from the retrieved chunks. These steps\\nform the fundamental framework of the RAG process, under-\\npinning its information retrieval and context-aware genera-\\ntion capabilities. Next, we will provide an introduction to the\\nRAG research framework.\\n3 RAG Framework\\nThe RAG research paradigm is continuously evolving, and\\nthis section primarily delineates its progression. We cate-\\ngorize it into three types: Naive RAG, Advanced RAG, and\\nModular RAG. While RAG were cost-effective and surpassed\\nthe performance of the native LLM, they also exhibited sev-\\neral limitations. The development of Advanced RAG and\\nModular RAG was a response to these specific shortcomings\\nin Naive RAG.\\n3.1 Naive RAG\\nThe Naive RAG research paradigm represents the earliest\\nmethodology, which gained prominence shortly after the\\nwidespread adoption of ChatGPT. The Naive RAG follows a\\ntraditional process that includes indexing, retrieval, and gen-\\neration. It is also characterized as a “Retrieve-Read” frame-\\nwork [Maet al. , 2023a ].\\nIndexing\\nThe indexing process is a crucial initial step in data prepara-\\ntion that occurs offline and involves several stages. It begins\\nwith data indexing, where original data is cleansed and ex-\\ntracted, and various file formats such as PDF, HTML, Word,\\nand Markdown are converted into standardized plain text. In\\norder to fit within the context limitations of language models,\\nthis text is then segmented into smaller, more manageable\\nchunks in a process known as chunking. These chunks are\\nsubsequently transformed into vector representations through\\nan embedding model, chosen for its balance between infer-\\nence efficiency and model size. This facilitates similarity\\ncomparisons during the retrieval phase. Finally, an index is\\ncreated to store these text chunks and their vector embed-\\ndings as key-value pairs, which allows for efficient and scal-\\nable search capabilities.\\nRetrieval\\nUpon receipt of a user query, the system employs the same en-\\ncoding model utilized during the indexing phase to transcode'], ['GROUP TREATMENT VALID RESPONSE NUM HALLUCINATION RATE\\n1Sample\\nFromLlama-2-7B-chat 450 51.8\\nMistral-7B-Instruct 450 57.6\\nSampling\\nStrategyRandom selection 450 55.1(-)\\nLeast detected spans 450 43.1( ↓21.8% )\\nNo detected spans 326†23.9(↓56.6% )\\n2Sample\\nFromGPT-3.5-Turbo-0613 450 10.9\\nGPT-4-0613 450 9.3\\nSampling\\nStrategyRandom selection 450 10.4(-)\\nLeast detected spans 450 5.3( ↓41.0% )\\nNo detected spans 446†4.9(↓52.9% )\\nTable 7: Utilizing the finetuned hallucination detector to sample from two responses can significantly reduce the\\nrate of hallucinations. †: Some instances did not have responses that met the required criteria.\\nPrompt (GPT -3.5-turbo ) Prompt (GPT -4-turbo ) Finetuned  Llama-2-13B010203040506070Recall(%)\\n026\\n6.4\\n055.6\\n40\\n041.741.4\\n1.552.4\\n49.4Subtle Conflict\\nEvident Conflict\\nSubtle Baseless Info\\nEvident Baseless Info\\nFigure 4: The span-level recalls of different models on\\nfour types of hallucinations.\\n6.3 Hallucination Suppression\\nWe tested the effectiveness of hallucination sup-\\npression using our finetuned hallucination detec-\\ntion model. For the 450 instances in the test set,\\nwe employed two strategies to select a final output\\nfrom two responses generated by two different mod-\\nels with similar hallucination density: one strategy\\nis based on choosing the response with fewer pre-\\ndicted hallucination spans, and the other is based on\\nthe absence of detected hallucination span. When\\nthe number of hallucination spans detected in both\\ncandidate responses is the same, one will be chosen\\nat random. Due to limited response candidates, not\\nall instances have a response that conforms to the\\nsecond strategy. In practical scenarios, this issue\\ncan be addressed by increasing the number of can-\\ndidate responses. We employed random selection\\nas a simple baseline for comparison.\\nThe results shown in Table 7 indicate that with\\nthe help of the hallucination detector, both strate-\\ngies can significantly reduce hallucination rate. For\\nthe relatively small Llama-2-7B-chat and Mistral-\\n7B-Instruct models, compared to random selection,\\nthe first strategy reduced the hallucination rate by\\n21.8%, while the second strategy achieved a re-duction of 56.6%. Even for models with a low\\nhallucination rate, specifically GPT-3.5-Turbo and\\nGPT-4, employing the finetuned hallucination de-\\ntector for sampling can still further reduce the rate\\nof hallucinations. The two strategies yielded a re-\\nduction in hallucination rates of 41% and 52.9%,\\nrespectively. These results demonstrate the poten-\\ntial of an efficient hallucination detection model in\\ndeveloping trustworthy RAG LLMs.\\n7 Conclusion\\nIn this paper, we introduce RAGTruth, a large-scale\\ncorpus of naturally generated hallucinations, fea-\\nturing detailed word-level annotations tailored for\\nRetrieval-augmented Generation (RAG) scenarios.\\nOur work includes an in-depth analysis of the in-\\nterplay between hallucinations and various factors,\\nsuch as task types, models being used, and contex-\\ntual settings.\\nAdditionally, we conduct empirical benchmarks\\nof several hallucination detection approaches using\\nour corpus. We show that fine-tuning Llama with\\nRAGTruth leads to competitive performance in hal-\\nlucination detection. This implies that by using a\\nhigh-quality dataset such as RAGTruth, it is possi-\\nble to develop specialized hallucination detection\\nmodels that are highly effective when compared to\\nprompt-based methods using general models such\\nas GPT-4. The dataset paves the way for future\\nresearch in this direction.\\nSimultaneously, our findings reveal that identi-\\nfying hallucinations in RAG contexts, particularly\\nat the span level, remains a formidable challenge,\\nwith current methods still falling short of reliable\\ndetection. We hope that RAGTruth, with its com-\\nprehensive span annotations, can assist the devel-\\nopment of hallucination detection techniques for\\nretrieval augmented generation.', 'MethodsQUESTION ANSWERING DATA-TO-TEXT SUMMARIZATION OVERALL\\nPrecision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1\\nPrompt gpt-3.5-turbo 40.0 1.1 2.2 100.0 0.2 0.3 100.0 1.0 1.9 62.5 0.5 1.0\\nPrompt gpt-4-turbo 31.0 92.0 46.4 76.0 93.0 83.6 46.9 82.2 59.7 54.8 90 .5 68.3\\nSelfCheckGPT gpt-3.5-turbo 24.3 20.1 22.0 71.2 33.2 45.2 43.2 19.7 27.1 52.8 28.0 36.6\\nSelfCheckGPT gpt-4-turbo 26.3 66.1 37.6 70.7 89.1 78.9 33.3 55.3 41.6 49.5 77.7 60.5\\nLMvLM gpt-4-turbo 19.9 75.3 31.5 68.8 76.4 72.4 23.8 82.2 36.9 37.0 77.4 50.1\\nFinetuned Llama-2-13B 82.9 58.6 68.7 91.8 88.1 89.9 79.4 48.1 59.9 88.6 74.1 80.7\\nTable 5: The response-level hallucination detection performance for each baseline method across different tasks and\\ndifferent models.\\nMethodsQUESTION ANSWERING DATA-TO-TEXT SUMMARIZATION OVERALL\\nPrecision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1\\nPrompt Baseline gpt-3.5-turbo 59.5 1.7 3.4 0.0 0.0 0.0 100.0 1.1 2.2 64.5 0.9 1.8\\nPrompt Baseline gpt-4-turbo 24.4 43.6 31.3 20.9 60.3 31.0 36.3 49.5 41.9 24.0 51.7 32.7\\nFinetuned Llama-2-13B 72.3 46.6 56.6 65.3 51.4 57.6 73.5 31.1 43.7 69.1 45.4 54.8\\nTable 6: The span-level detection performance for each baseline method across different tasks and different models.\\ninitial learning rate of 3e-4, and limiting the train-\\ning to 3 epochs, all conducted on 4 A100 GPUs.\\n5.2 Data Split\\nAll detection algorithms are tested on the same\\nRAGTruth test set, which consists of 450 instances\\nin total, derived by randomly selecting 150 in-\\nstances from each task type. The rest of the data\\nis used to fine-tune the LLama-2-13B model, as\\npreviously mentioned.\\n5.3 Evaluation Metrics\\nResponse-level Detection We report precision,\\nrecall, and F1 score for each detection algorithm\\nand its variants across different tasks.\\nSpan-level Detection We calculate the overlap\\nbetween the generated span and human-labeled\\nspan and report the precision, recall, and f1 score\\nat the char-level.\\n6 Experimental Results\\n6.1 Response-level Detection\\nThe results in Table 5 reveal that hallucination\\ndetection remains a significant challenge in the\\ncontext of RAG for all existing detection methods.\\nEven when reference information is available, the\\nresponses generated may still include hallucina-\\ntions, which current LLMs cannot reliably identify.\\nThe most advanced LLM, GPT-4-turbo, achieves\\nonly an average F1 score of 68.3%. In compari-\\nson, the widely-used GPT-3.5-turbo often fails to\\ndiscern hallucinated responses, often identifying\\nmost examples as non-hallucinated, highlightingthe difficulty in detecting naturally occurring hallu-\\ncinations. For another notable baseline, SelfCheck-\\nGPT also shows unsatisfactory performance in this\\nregard, achieving an average F1 score of 60.5%\\nwith GPT-4-turbo and 36.6% with GPT-3.5-turbo.\\nBy utilizing our human-labeled high-quality\\ntraining set, a fine-tuned Llama-2-13B can achieve\\nthe best performance with an average 80.7% f1\\nscore. This shows the effectiveness of our data in\\nimproving the model’s detection ability on halluci-\\nnation.\\n6.2 Span-level Detection\\nRAGTruth, as a hallucination corpus with fine-\\ngrained span labels, enables us to present exper-\\nimental results for span-level detection, serving\\nas a baseline for future research. As shown in\\nTable 6, the overall performance of the current\\ndetection method is sub-optimal, highlighting the\\nchallenges in span-level detection. Even the ad-\\nvanced GPT-4-turbo tends to incorrectly classify\\nmany non-hallucinated contents with a low preci-\\nsion of 24%. While our fine-tuned model shows im-\\nproved capability in identifying hallucinated spans\\nby achieving an averaged f1 score of 48.4%, it still\\nfalls short of perfect detection, emphasizing the\\ninherent difficulties of this task.\\nWe also report the detection performance across\\nfour different types of hallucination spans. In the\\ncurrent stage, as we have not differentiated the\\ntypes of detected hallucinations, we only report the\\nchar-level recall for different types of hallucina-\\ntions. As indicated in Figure 4, the detection of\\nevident hallucinations proves more effective com-\\npared to that of subtle hallucinations.', 'Task # Instance # Resp.CONTEXT LENGTH RESP. LENGTH HALLUCINATION\\nMean Max Mean Max # Resp. % Resp. # Span\\nSummarization(CNN/DM) 628 3768 648 1749 124 632 1230 32.6% 1570\\nSummarization(Recent News) 316 1896 369 481 89 240 540 28.5% 613\\nQuestion Answering 992 5952 243 509 119 381 1844 31.0% 3155\\nData-to-text 1037 6222 354 1253 159 369 4304†69.2%†9443†\\nOverall 2973 17838 380 1749 131 632 7918 44.4% 14781\\nTable 2: The basic statistics of RAGTruth. †: The data presented in the table above include hallucination spans\\ncaused by null values in JSON. If these spans are removed, the hallucination response number, hallucination\\nresponse rate, and hallucination span number of date-to-text tasks are 3898, 62.6%, and 7797, respectively.\\n0% 25% 50% 75% 100%QA\\n(Macro)\\nData2text\\n(Y elp)\\nSum.\\n(CNN/DM)\\nSum.\\n(Recent  news)Subtle baseless info\\nEvident baseless infoSubtle conflict\\nEvident conflict\\nFigure 2: Frequency of different types of hallucination\\nby task.\\nUsers can independently choose whether to con-\\nsider these spans as hallucinations. Additionally,\\nwe have included an option in the evaluation script\\nthat allows users to decide whether to include these\\nspans in the evaluation.\\n4 Hallucination Benchmark Analysis\\n4.1 Basic Statistics\\nWe presented detailed statistics of RAGTruth in\\nTable 2. Compared to existing datasets for hallu-\\ncination detection (Cao et al., 2023; Kamoi et al.,\\n2023), the RAGTruth dataset is considerably large\\nin scale. The corpus contains a total of 2,973 in-\\nstances of data, which include 944 instances for\\nsummarization, 992 instances for question answer-\\ning, and 1,037 instances for data-to-text generation.\\nEach instance comprises responses from 6 differ-\\nent models. As shown in Table 2, the RAGTruth\\ndataset also features longer prompt and response\\nlengths than existing datasets for hallucination de-\\ntection (Wang et al., 2020).\\n4.2 Hallucination Statistics\\nHallucination Types As shown in Figure 2, the\\ngeneration of information baseless in the context\\nwas significantly more prevalent than the gener-\\nation of information conflicting with the context.Within the two major categories of baseless info\\nandconflict , the more severe hallucinations, namely\\nEvident baseless info andEvident conflict , respec-\\ntively, account for a significant portion. This obser-\\nvation highlights the importance and challenges of\\nhallucination mitigation, even in RAG settings.\\nHallucination vs Tasks As shown in Table 2,\\nacross the three tasks, the data-to-text task exhib-\\nited the highest frequency of hallucinations in its\\nresponses. When processing Yelp business data,\\nthe models often generated information not present\\nin the input, influenced by stereotypes inherent in\\nthe business data. Furthermore, inconsistent han-\\ndling of JSON format data also contributed to a\\nsignificant number of hallucinations. In contrast,\\nthe hallucination rate for the summarization task\\nwas relatively lower. Interestingly, the models did\\nnot show a higher rate of hallucinations for recent\\nnews compared to outdated news. This could be\\nattributed to the shorter context length in the recent\\nnews subtask compared to the CNN/DM subtask.\\nHallucination vs Models Table 3 illustrates that\\namong the data we collected, OpenAI’s two mod-\\nels demonstrated notably lower hallucination rates\\ncompared to others. Specifically, GPT-4-0613 ex-\\nhibited the lowest hallucination frequency.\\nTo more clearly compare the hallucination rate\\nof different models, we calculated the hallucination\\ndensity for each model across three tasks. Hal-\\nlucination density is defined as the average num-\\nber of hallucination spans per hundred words in a\\nresponse. In the Llama2 series, a clear negative\\ncorrelation was observed between the model scale\\nand the hallucination density. Despite its strong\\nperformance in various benchmarks and leader-\\nboards (Zheng et al., 2023), the Mistral-7B-Instruct\\nmodel recorded the highest hallucination density\\namong the evaluated models.', 'ModelSUMMARIZATION QUESTION ANSWERING DATA-TO-TEXT OVERALL\\n# Resp. # Span Density # Resp. # Span Density # Resp. # Span Density # Resp. # Span\\nGPT-3.5-turbo-0613 64 74 0.07 81 97 0.13 283 397 0.18 428 568\\nGPT-4-0613 78 85 0.08 48 57 0.06 293 359 0.27 419 501\\nLlama-2-7B-chat 451 542 0.61 542 1082 0.62 899 1812 1.30 1892 3436\\nLlama-2-13B-chat 333 395 0.47 426 706 0.51 988 2840 1.54 1747 3941\\nLlama-2-70B-chat†220 254 0.27 354 583 0.44 873 1861 1.17 1447 2698\\nMistral-7B-Instruct 624 842 0.86 393 630 0.61 968 2174 1.53 1985 3646\\nTable 3: Hallucination counts and density of models. †: We used 4-bit quantized version of Llama-2-70B-chat.\\nCLB SUMMARIZATION DATA2TEXT QA\\n1 0.31(102 ,317] 1.55(117 ,237] 0.55(79 ,163]\\n2 0.33(317 ,387] 1.49(237 ,290] 0.45(163 ,196]\\n3 0.38(387 ,481] 1.54(290 ,355] 0.53(196 ,266]\\n4 0.41(481 ,810] 1.49(355 ,448] 0.58(266 ,327]\\n5 0.50(810 ,1749] 1.53(448 ,1253] 0.54(327 ,509]\\nRLB SUMMARIZATION DATA2TEXT QA\\n1 0.36(7,74] 1.24(35 ,120] 0.14(1,70]\\n2 0.34(74 ,92] 1.21(120 ,136] 0.26(70 ,98]\\n3 0.33(92 ,110] 1.63(136 ,161] 0.38(98 ,127]\\n4 0.48(110 ,144] 1.85(161 ,204] 0.67(127 ,166]\\n5 0.42(144 ,632] 1.67(204 ,369] 1.21(166 ,381]\\nTable 4: Average number of hallucinations per response\\nin different context length buckets (CLB) and response\\nlength buckets (RLB) for the three types of tasks. The\\nsubscript denotes the minimum and maximum length of\\nthis bucket.\\nHallucination vs Length We divided the data\\nfor each type of task into five equal buckets based\\non context/response length and calculated the av-\\nerage number of hallucination spans per response\\nwithin each bucket. As shown in Table 4, with a\\nfew exceptions, there is a clear overall trend of an\\nincrease in the average number of hallucinations as\\nthe context length or response length grows.\\nLocation of Hallucinations In Figure 3, we\\npresent an analysis of the distribution of halluci-\\nnatory spans within responses. Generally, halluci-\\nnations are more likely to occur towards the end\\nof responses, a pattern that is particularly evident\\nin question-answering and summarization tasks.\\nCompared to other tasks, the data-to-text task has\\na relatively higher occurrence of hallucinations in\\nthe first half, as indicated by the two bright areas\\non the heatmap.\\n5 Experimental Setup\\n5.1 Hallucination Detection Algorithms\\nUsing RAGTruth, we conducted experiments with\\nthe following four distinct algorithms for halluci-\\nnation detection:\\nHallucination Detection Prompt : Hallucination\\nQA\\n(Macro)\\nData2text\\n(Y elp)\\nSum.\\n(CNN/DM)\\n0% 25% 50% 75% 100%\\nNormalized  positionSum.\\n(Recent news)0.010.02\\n0.0050.0100.015\\n0.010.02\\n0.0050.0100.015Figure 3: Heatmaps of normalized hallucination occur-\\nrence positions. The numerical values on the colorbar\\nrepresent the proportion of hallucinations occurring at a\\nspecific location relative to the total number of halluci-\\nnations in that task.\\ndetection prompts are manually crafted to instruct\\nLLMs (GPT-4-turbo and GPT-3.5-turbo) in assess-\\ning whether a given reference-response pair con-\\ntains hallucinated content and to identify the corre-\\nsponding hallucinated spans in the response. For\\ndetailed information about these prompts, please\\nrefer to Appendix C.\\nSelfCheckGPT (Manakul et al., 2023): Self-\\nCheckGPT employs a zero-resource, sampling-\\nbased method to fact-check the responses of black-\\nbox models. When processing each response in\\nRAGTruth, 5 responses from other models serve\\nas references, and GPT-4-turbo/GPT-3.5-turbo are\\nqueried to verify consistency. By scanning each\\nsentence in a response, the hallucination position\\ncan be determined as well.\\nLMvLM (Cohen et al., 2023): LMvLM is an ap-\\nproach that employs a multi-turn interaction be-\\ntween two Language Models that aim to discover\\ninconsistencies through cross-examination.\\nLLM Finetuning : Llama-2-13B has been fine-\\ntuned using the training set from RAGTruth. The\\nmodel takes the context-response pair with proper\\ninstructions as the input and treats the hallucinate\\nspan as the targeted generation output. We em-\\nployed LoRA (Hu et al., 2021) fine-tuning with an'], ['The Chronicles of RAG: The Retriever, the Chunk and the Generator PREPRINT\\n5 Advanced Experiments\\nThe studies and experiments outlined in section 4 have shown unsatisfactory performance, marked by a degradation\\nof at least 20% compared to the peak relative performance. Therefore, in this section, we explore various retrieval\\napproaches for the RAG, recognizing that the quality of the retriever is a crucial factor in enhancing performance for\\nthis type of problem. We conducted an evaluation covering both sparse and dense search, a hybrid method, and even\\na multi-stage architecture using a reranker.\\nIn pursuit of code debugging flexibility and easier customization at each stage, we chose not to utilize an RAG frame-\\nwork (like LangChain or Llama-Index). For a comprehensive guide on debugging RAG and more details about retrieval\\nsystems, refer to [20] and [21].\\n5.1 Retrievers\\nWhen deploying retrieval systems, it is essential to achieve a balance between “effectiveness” ( How good are the\\nresults returned? ) and “efficiency” ( How much time it takes to return the results? orHow much resources are used in\\nterms of disk/RAM/GPU? ). This balance ensures that latency, result quality, and computational budget remain within\\nour application’s required limits. This work will exclusively focus on effectiveness measures to quantify the retrievers\\nmethods quality.\\nIn our retriever experiments, the evaluation strategy centers around assessing how well the retriever performs in re-\\ntrieving relevant information based on each given query qi. To achieve this, we employ the concept of recall. It is\\ndefined as the fraction of the relevant documents for a given query qithat are successfully retrieved in a ranked list\\nR[21]. This metric is based on binary relevance judgments, assuming that documents are either relevant or not [21].\\nIn this paper, each chunk is considered a document and only the respective chunk diis considered relevant to the query\\nqi. While recall is easy to interpret, it does not consider the specific rank positions in which the relevant chunk appears\\ninR.\\nTo overcome this limitation, we introduce Reciprocal Rank (RR) into our analysis. In this metric, the rank of the\\nfirst relevant document to the query in Ris used to compute the RR score [21]. Therefore, Reciprocal Rank offers a\\nmore nuanced evaluation by assigning a higher value when the relevant chunk is returned in the early positions of our\\nretrievers given the respective query.\\nRecall and Reciprocal Rank were evaluated at a specific cutoff so the measures are presented as R@k and MRR@k.\\nFor each query, its results are evaluated and their mean serves as an aggregated measure of effectiveness of a given\\nretriever method. The retrievers are introduced below.\\nIn the category of sparse retrievers, we emphasize the BM25, a technique grounded in statistical weighting to assess\\nrelevance between search terms and documents. BM25 employs a scoring function that takes into account term\\nfrequency and document length, offering an efficient approach for retrieving pertinent information and is typically\\nused as a strong baseline. However, it is exact-match based and can be powerless when query and document are\\nrelevant to each other but has no common words.\\nOn the other hand, when exploring dense retrievers, we often encounter approaches based on the called bi-encoder\\ndesign [22]. The bi-encoder independently encodes queries and documents, creating separate vector representations\\nbefore calculating similarity. An advantage of this approach is that it can be initialized ‘offline’: document embeddings\\ncan be precomputed, leaving only the query embedding being calculated at search time, reducing latency.\\nThe hybrid search technique aims to leverage the best of both sparse and dense search approaches. Given a question,\\nboth searches are conducted in parallel, generating two lists of candidate documents to answer it. The challenge then\\nlies in combining the two results in the best possible way, ensuring that the final hybrid list surpasses the individual\\nsearches. Essentially, we can conceptualize it as a voting system, where each searcher casts a vote on the relevance of\\na document to a given query, and in the end, the opinions are combined to produce a better result.\\nThe multi-stage search architecture is based on the retrieve-and-rerank pipeline. In the first stage, a retriever with good\\nrecall is typically used to perform an initial filtering of the documents to be returned. From this narrowed-down list,\\nthese candidate documents are then sent to a second stage, which involves higher computational complexity, to rerank\\nthem and enhance the final effectiveness of the system.\\nNext, we provide more details about each retriever used.', 'Figure 5: Technology tree of representative RAG research with different augmentation aspects\\nit typically relies on a sequence of n tokens to demarcate the\\nboundaries between generated text and retrieved documents.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized. Recursive re-\\ntrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summa-\\nrizing sections of a document or lengthy PDF before per-\\nforming a retrieval based on this summary. Subsequently, a\\nsecondary retrieval within the document refines the search,\\nembodying the recursive nature of the process. In contrast,\\nmulti-hop retrieval is designed to delve deeper into graph-\\nstructured data sources, extracting interconnected informa-\\ntion[Liet al. , 2023c ].\\nAdditionally, some methodologies integrate the steps of re-\\ntrieval and generation. ITER-RETGEN [Shao et al. , 2023 ]\\nemploys a synergistic approach that leverages “retrieval-\\nenhanced generation” alongside “generation-enhanced re-\\ntrieval” for tasks that necessitate the reproduction of specific\\ninformation. The model harnesses the content required to ad-\\ndress the input task as a contextual basis for retrieving per-\\ntinent knowledge, which in turn facilitates the generation of\\nimproved responses in subsequent iterations.\\nRecursive Retrieval\\nRecursive Retrieval is often used in information retrieval and\\nNLP to improve the depth and relevance of search results.The process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-\\nally converging on the most pertinent information through a\\nfeedback loop. IRCoT [Trivedi et al. , 2022 ]uses chain-of-\\nthought to guide the retrieval process and refines the CoT\\nwith the obtained retrieval results. ToC [Kim et al. , 2023 ]\\ncreates a clarification tree that systematically optimizes the\\nambiguous parts in the Query. It can be particularly useful in\\ncomplex search scenarios where the user’s needs are not en-\\ntirely clear from the outset or where the information sought\\nis highly specialized or nuanced. The recursive nature of the\\nprocess allows for continuous learning and adaptation to the\\nuser’s requirements, often resulting in improved satisfaction\\nwith the search outcomes.\\nAdaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare and Self-\\nRAG [Jiang et al. , 2023b, Asai et al. , 2023 ], refine the RAG\\nframework by enabling LLMs to actively determine the op-\\ntimal moments and content for retrieval, thus enhancing the\\nefficiency and relevance of the information sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operations, as\\nseen in model agents like AutoGPT, Toolformer, and\\nGraph-Toolformer [Yang et al. , 2023c, Schick et al. , 2023,', 'perspectives. Additionally, we anticipate future direc-\\ntions for RAG, emphasizing potential enhancements to\\ntackle current challenges, expansions into multi-modal\\nsettings, and the development of its ecosystem.\\nThe paper unfolds as follows: Section 2 and 3 define RAG\\nand detail its developmental process. Section 4 through 6 ex-\\nplore core components—Retrieval, “Generation” and “Aug-\\nmentation”—highlighting diverse embedded technologies.\\nSection 7 focuses on RAG’s evaluation system. Section 8\\ncompare RAG with other LLM optimization methods and\\nsuggest potential directions for its evolution. The paper con-\\ncludes in Section 9.\\n2 Definition\\nThe definition of RAG can be summarized from its workflow.\\nFigure 2 depicts a typical RAG application workflow. In this\\nscenario, a user inquires ChatGPT about a recent high-profile\\nevent (i.e., the abrupt dismissal and reinstatement of Ope-\\nnAI’s CEO) which generated considerable public discourse.\\nChatGPT as the most renowned and widely utilized LLM,\\nconstrained by its pretraining data, lacks knowledge of re-\\ncent events. RAG addresses this gap by retrieving up-to-date\\ndocument excerpts from external knowledge bases. In this in-\\nstance, it procures a selection of news articles pertinent to the\\ninquiry. These articles, alongside the initial question, are then\\namalgamated into an enriched prompt that enables ChatGPT\\nto synthesize an informed response. This example illustrates\\nthe RAG process, demonstrating its capability to enhance the\\nmodel’s responses with real-time information retrieval.\\nTechnologically, RAG has been enriched through various\\ninnovative approaches addressing pivotal questions such as\\n“what to retrieve” “when to retrieve” and “how to use the\\nretrieved information”. For “what to retrieve” research has\\nprogressed from simple token [Khandelwal et al. , 2019 ]and\\nentity retrieval [Nishikawa et al. , 2022 ]to more complex\\nstructures like chunks [Ram et al. , 2023 ]and knowledge\\ngraph [Kang et al. , 2023 ], with studies focusing on the\\ngranularity of retrieval and the level of data structur-\\ning. Coarse granularity brings more information but\\nwith lower precision. Retrieving structured text provides\\nmore information while sacrificing efficiency. The ques-\\ntion of “when to retrieve” has led to strategies ranging\\nfrom single [Wang et al. , 2023e, Shi et al. , 2023 ]to adap-\\ntive [Jiang et al. , 2023b, Huang et al. , 2023 ]and multiple\\nretrieval [Izacard et al. , 2022 ]methods. High frequency of\\nretrieval brings more information and lower efficiency. As\\nfor ”how to use” the retrieved data, integration techniques\\nhave been developed across various levels of the model\\narchitecture, including the input [Khattab et al. , 2022 ],\\nintermediate [Borgeaud et al. , 2022 ], and output lay-\\ners[Liang et al. , 2023 ]. Although the “intermediate” and\\n“output layers” are more effective, there are problems with\\nthe need for training and low efficiency.\\nRAG is a paradigm that enhances LLMs by integrating ex-\\nternal knowledge bases. It employs a synergistic approach,\\ncombining information retrieval mechanisms and In-Context\\nLearning (ICL) to bolster the LLM’s performance. In this\\nframework, a query initiated by a user prompts the retrieval ofpertinent information via search algorithms. This information\\nis then woven into the LLM’s prompts, providing additional\\ncontext for the generation process. RAG’s key advantage lies\\nin its obviation of the need for retraining of LLMs for task-\\nspecific applications. Developers can instead append an ex-\\nternal knowledge repository, enriching the input and thereby\\nrefining the model’s output precision. RAG has become one\\nof the most popular architectures in LLMs’ systems, due to\\nits high practicality and low barrier to entry, with many con-\\nversational products being built almost entirely on RAG.\\nThe RAG workflow comprises three key steps. First, the\\ncorpus is partitioned into discrete chunks, upon which vec-\\ntor indices are constructed utilizing an encoder model. Sec-\\nond, RAG identifies and retrieves chunks based on their vec-\\ntor similarity to the query and indexed chunks. Finally, the\\nmodel synthesizes a response conditioned on the contextual\\ninformation gleaned from the retrieved chunks. These steps\\nform the fundamental framework of the RAG process, under-\\npinning its information retrieval and context-aware genera-\\ntion capabilities. Next, we will provide an introduction to the\\nRAG research framework.\\n3 RAG Framework\\nThe RAG research paradigm is continuously evolving, and\\nthis section primarily delineates its progression. We cate-\\ngorize it into three types: Naive RAG, Advanced RAG, and\\nModular RAG. While RAG were cost-effective and surpassed\\nthe performance of the native LLM, they also exhibited sev-\\neral limitations. The development of Advanced RAG and\\nModular RAG was a response to these specific shortcomings\\nin Naive RAG.\\n3.1 Naive RAG\\nThe Naive RAG research paradigm represents the earliest\\nmethodology, which gained prominence shortly after the\\nwidespread adoption of ChatGPT. The Naive RAG follows a\\ntraditional process that includes indexing, retrieval, and gen-\\neration. It is also characterized as a “Retrieve-Read” frame-\\nwork [Maet al. , 2023a ].\\nIndexing\\nThe indexing process is a crucial initial step in data prepara-\\ntion that occurs offline and involves several stages. It begins\\nwith data indexing, where original data is cleansed and ex-\\ntracted, and various file formats such as PDF, HTML, Word,\\nand Markdown are converted into standardized plain text. In\\norder to fit within the context limitations of language models,\\nthis text is then segmented into smaller, more manageable\\nchunks in a process known as chunking. These chunks are\\nsubsequently transformed into vector representations through\\nan embedding model, chosen for its balance between infer-\\nence efficiency and model size. This facilitates similarity\\ncomparisons during the retrieval phase. Finally, an index is\\ncreated to store these text chunks and their vector embed-\\ndings as key-value pairs, which allows for efficient and scal-\\nable search capabilities.\\nRetrieval\\nUpon receipt of a user query, the system employs the same en-\\ncoding model utilized during the indexing phase to transcode', '111:4 Lyu, et al.\\nTo evaluate the RAG system in these four scenarios, we introduce CRUD-RAG, a comprehensive,\\nlarge-scale Chinese RAG benchmark. CRUD-RAG consists of four evaluation tasks: text continu-\\nation, question answering (with single-document and multi-document questions), hallucination\\nmodification, and open-domain multi-document summarization, which respectively correspond to\\nthe CRUD-RAG classification of RAG application scenarios. We construct CRUD-RAG by crawling\\nthe latest high-quality news data from major news websites in China, which were not exposed\\nto the LLMs during training. We then automatically create datasets using GPT-4 based on these\\nnews data. For the multi-document summarization task, we apply a reverse construction strategy.\\nWe first generate news events and their summaries using GPT-4. Then, we use these events as\\nkeywords to search for 10 related and non-duplicate reports from the web, which we add to our\\nretrieval database. During evaluation, the RAG system will use the retrieval database to generate\\nsummaries for the events. For the text continuation task, we split the news text into a beginning\\nand a continuation paragraph. We then use each sentence in the continuation paragraph as a\\nkeyword to search for 10 related reports on the Web. We remove any duplicate content and add the\\nreports to the retrieval database. For the single-document QA task, we use the RGB [ 7] construction\\nmethod. For the multi-document QA task, we use the Chain-of-Thought technology to help the\\nmodel identify common and different aspects among documents, and then generate questions based\\non these aspects with increasing difficulty. For the hallucination modification task, we use the\\nannotations in the UHGEval dataset and correct hallucinations with GPT-4. We also include the\\nreal news in UHGEval in the retrieval database.\\nIn the experiments, we systematically evaluate the RAG system’s performance on our CRUD-RAG\\nbenchmark. We also investigate various factors that affect the RAG system, such as the context\\nlength, the chunk size, the embedding model, the retrieval algorithms, and the LLM. Based on our\\nexperimental results, we provide some valuable suggestions for building effective RAG systems.\\nThe contributions of this paper are:\\n•A comprehensive evaluation benchmark : Our benchmark covers not only question\\nanswering, but also create, read, update, and delete (CRUD) of RAG applications.\\n•High-quality evaluation datasets : We constructed diverse datasets for different evaluation\\ntasks, based on the application scenarios of RAG. These tasks include text continuation,\\nmulti-document summarization, question answering and hallucination modification.\\n•Extensive experiments : we performed extensive experiments on our own benchmark, using\\nvarious metrics to measure the performance of RAG systems. Based on our experiments, we\\noffered useful guidance for future researchers and RAG system developers.\\n2 RELATED WORK\\n2.1 RAG: A Neural Text Generation Framework that Combines Retrieval and\\nGeneration\\nLLMs excel in text generation but also confront challenges such as outdated knowledge and the\\ngeneration of hallucinatory content [ 6,17,37]. In response to these challenges, RAG, also referred\\nto as RALM (Retrieval-Augmented Language Models), incorporate external knowledge to generate\\nresponses characterized by enhanced accuracy and realism [ 41]. This is particularly critical in\\ndomains that heavily depend on precision and reliability, including but not limited to the legal,\\nmedical, and financial sectors. Retrieval models have been promoting the development of language\\nmodels [14, 29, 52].\\nConventional RAG systems adhere to a standardized workflow encompassing indexing, retrieval,\\nand generation phases [ 25,32]. The indexing phase encompasses data cleansing, extraction, trans-\\nformation into plain text, segmentation, and indexing, utilizing embedding models to transform text\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.']]\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  25%|██▌       | 15/60 [00:04<00:10,  4.32it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:06<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8899139161099786\n",
      "This is the new best value!\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0  Naive      0.629361          0.966667           0.932998        0.963889   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.963462            0.883107  0.889914  \n"
     ]
    }
   ],
   "source": [
    "result_naive_rag = evaluate_system(\"Naive\", questions, answers_naive, contexts_naive, ground_truths)\n",
    "results_df = pd.concat([results_df, result_naive_rag], ignore_index=True)\n",
    "print(result_naive_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try recursive text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder()\n",
    "chunks_r = text_splitter.split_documents(documents)\n",
    "db_basic = Chroma.from_documents(chunks_r, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_basic\")\n",
    "retriever_basic = db_basic.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_basic.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_basic = Chroma(persist_directory = \"../papers/vectordb-edit/recursive_basic\", embedding_function=embeddings_client)\n",
    "retriever_basic = db_basic.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_recursive = []\n",
    "contexts_recursive = []\n",
    "for query in questions:\n",
    "    try:  \n",
    "        response = retrieval_chain(prompt, retriever_basic, llm).invoke({\"question\": query})\n",
    "        # Access the response content\n",
    "        answers_recursive.append(response[\"response\"].content)\n",
    "        # Access the context content\n",
    "        context_content = [context.page_content for context in response[\"context\"]]\n",
    "        contexts_recursive.append(context_content)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "        answers_recursive.append(\"No answer\")\n",
    "        context_full = retriever_basic.get_relevant_documents(query)\n",
    "        context_content = [context.page_content for context in context_full]\n",
    "        contexts_recursive.append(context_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  28%|██▊       | 17/60 [00:04<00:09,  4.70it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8923639412147183\n",
      "This is the new best value!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.626769</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.939179</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.883107</td>\n",
       "      <td>0.892364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "0  Recursive      0.626769          0.977778           0.939179   \n",
       "\n",
       "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "0        0.963889           0.963462            0.883107  0.892364  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_recursive = evaluate_system(\"Recursive\", questions, answers_naive, contexts_naive, ground_truths)\n",
    "results_df = pd.concat([results_df, result_recursive], ignore_index=True)\n",
    "result_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.581812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.863942</td>\n",
       "      <td>0.483329</td>\n",
       "      <td>0.638180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.567018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.864540</td>\n",
       "      <td>0.449709</td>\n",
       "      <td>0.630211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.629361</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.932998</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.883107</td>\n",
       "      <td>0.889914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.626769</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.939179</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.883107</td>\n",
       "      <td>0.892364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "0    GPT-3.5      1.000000          0.581812                NaN   \n",
       "1      GPT-4      1.000000          0.567018                NaN   \n",
       "2      Naive      0.629361          0.966667           0.932998   \n",
       "3  Recursive      0.626769          0.977778           0.939179   \n",
       "\n",
       "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "0             NaN           0.863942            0.483329  0.638180  \n",
       "1             NaN           0.864540            0.449709  0.630211  \n",
       "2        0.963889           0.963462            0.883107  0.889914  \n",
       "3        0.963889           0.963462            0.883107  0.892364  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunk size change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_evaluate(name, retriever, prompt, llm, results_df):\n",
    "    answers = []\n",
    "    contexts_extra = []\n",
    "\n",
    "    for query in questions:\n",
    "        try:  \n",
    "            response = retrieval_chain(prompt, retriever, llm).invoke({\"question\": query})\n",
    "            # Access the response content\n",
    "            answers.append(response[\"response\"].content)\n",
    "            # Access the context content\n",
    "            context_content = [context.page_content for context in response[\"context\"]]\n",
    "            contexts_extra.append(context_content)  \n",
    "        except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers.append(\"No answer\")\n",
    "            context_full = retriever.get_relevant_documents(query)\n",
    "            context_content = [context.page_content for context in context_full]\n",
    "            contexts_extra.append(context_content)\n",
    "\n",
    "    result = evaluate_system(name, questions, answers, contexts_extra, ground_truths)\n",
    "    results_df = pd.concat([results_df, result], ignore_index=True)\n",
    "    return result, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# # THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 0)\n",
    "# chunks_1000 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_1000))\n",
    "# db_1000 = Chroma.from_documents(chunks_1000, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_1000\")\n",
    "# db_1000.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_1000 = Chroma(persist_directory = \"../papers/vectordb-edit/recursive_1000\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:10<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.7505457735075017\n",
      "This is the new best value!\n",
      "CHUNK SIZE 1000, 0% overlap\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.599047</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.847668</td>\n",
       "      <td>0.459893</td>\n",
       "      <td>0.750546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
       "0  Chunk 1000, overlap 0%      0.716667          0.599047                0.9   \n",
       "\n",
       "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
       "0            0.98           0.847668            0.459893  0.750546  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever_1000 = db_1000.as_retriever()\n",
    "# result_1000_0 = run_and_evaluate(\"Chunk 1000, overlap 0%\",retriever_1000, prompt, llm, results_df)\n",
    "# print(\"CHUNK SIZE 1000, 0% overlap\")\n",
    "# result_1000_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks for chunk size 500, overlap 5%: 660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  17%|█▋        | 10/60 [00:03<00:12,  3.87it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8995507723110836\n",
      "CHUNK SIZE 500, 5% overlap:\n",
      "                  System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 5%      0.976053           0.97992           0.940768   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             1.0           0.899326            0.601237  0.899551  \n",
      "Number of chunks for chunk size 500, overlap 10%: 679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8313719761373116\n",
      "CHUNK SIZE 500, 10% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 10%           1.0          0.861279           0.872222   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0            0.78           0.891491             0.58324  0.831372  \n",
      "Number of chunks for chunk size 500, overlap 15%: 695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8941619865386087\n",
      "CHUNK SIZE 500, 15% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 15%      0.962963          0.944893           0.941667   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.9           0.901248            0.714201  0.894162  \n",
      "Number of chunks for chunk size 500, overlap 20%: 712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-750' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-751' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-752' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-753' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8984910245400295\n",
      "CHUNK SIZE 500, 20% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 500, overlap 20%      0.972222          0.958276              0.975   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.9           0.903441            0.682007  0.898491  \n",
      "Number of chunks for chunk size 1000, overlap 5%: 380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:05<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.9004659999164497\n",
      "CHUNK SIZE 1000, 5% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 5%           1.0          0.954163           0.963889   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.9           0.903903            0.680841  0.900466  \n",
      "Number of chunks for chunk size 1000, overlap 10%: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.9178452285965846\n",
      "This is the new best value!\n",
      "CHUNK SIZE 1000, 10% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 10%           1.0          0.944514           0.936111   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             1.0           0.905397            0.721049  0.917845  \n",
      "Number of chunks for chunk size 1000, overlap 15%: 385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.9158128837545355\n",
      "CHUNK SIZE 1000, 15% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 15%           1.0          0.959255              0.975   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.954762           0.903965            0.701894  0.915813  \n",
      "Number of chunks for chunk size 1000, overlap 20%: 388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:05<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8744523443241207\n",
      "CHUNK SIZE 1000, 20% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 1000, overlap 20%           0.9          0.926928           0.938889   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0           0.925           0.903688             0.65221  0.874452  \n",
      "Number of chunks for chunk size 2000, overlap 5%: 253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  22%|██▏       | 13/60 [00:04<00:14,  3.30it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8983840200557323\n",
      "CHUNK SIZE 2000, 5% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 5%      0.619888               1.0           0.936237   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.963889           0.988462            0.881828  0.898384  \n",
      "Number of chunks for chunk size 2000, overlap 10%: 253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  17%|█▋        | 10/60 [00:04<00:20,  2.39it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:06<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.9031998186367467\n",
      "CHUNK SIZE 2000, 10% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 10%      0.660727               1.0           0.923042   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness  Average  \n",
      "0        0.963889           0.988462            0.883079   0.9032  \n",
      "Number of chunks for chunk size 2000, overlap 15%: 253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   8%|▊         | 5/60 [00:04<00:37,  1.49it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8959771345180569\n",
      "CHUNK SIZE 2000, 15% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 15%      0.648617               1.0           0.930926   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.923889           0.988462            0.883969  0.895977  \n",
      "Number of chunks for chunk size 2000, overlap 20%: 253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  20%|██        | 12/60 [00:04<00:13,  3.57it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating:  92%|█████████▏| 55/60 [00:05<00:00, 16.71it/s]Invalid JSON response. Expected dictionary with key 'question'\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8873481963404403\n",
      "CHUNK SIZE 2000, 20% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 2000, overlap 20%      0.615365               1.0            0.92364   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.915556           0.988462            0.881066  0.887348  \n",
      "Number of chunks for chunk size 3000, overlap 5%: 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  32%|███▏      | 19/60 [00:04<00:07,  5.72it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8906974455207269\n",
      "CHUNK SIZE 3000, 5% overlap:\n",
      "                   System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 5%      0.609161               1.0           0.926964   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.963889           0.963462            0.880709  0.890697  \n",
      "Number of chunks for chunk size 3000, overlap 10%: 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  32%|███▏      | 19/60 [00:04<00:08,  4.68it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8888140932572158\n",
      "CHUNK SIZE 3000, 10% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 10%      0.598154               1.0           0.926966   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.963889           0.963462            0.880414  0.888814  \n",
      "Number of chunks for chunk size 3000, overlap 15%: 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  23%|██▎       | 14/60 [00:04<00:15,  2.92it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:06<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8934493967791844\n",
      "CHUNK SIZE 3000, 15% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 15%       0.62451               1.0           0.927038   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.963889           0.963462            0.881798  0.893449  \n",
      "Number of chunks for chunk size 3000, overlap 20%: 252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  30%|███       | 18/60 [00:04<00:08,  5.22it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8989719714549104\n",
      "CHUNK SIZE 3000, 20% overlap:\n",
      "                    System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Chunk 3000, overlap 20%      0.629029               1.0           0.930217   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.963889           0.988462            0.882235  0.898972  \n"
     ]
    }
   ],
   "source": [
    "chunk_sizes = [500, 1000, 2000, 3000]\n",
    "overlap_percentages = [5, 10, 15, 20]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    for overlap_percentage in overlap_percentages:\n",
    "        # Calculate overlap based on percentage\n",
    "        chunk_overlap = int(chunk_size * overlap_percentage / 100)\n",
    "        \n",
    "        # Create text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        \n",
    "        # Split documents\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # Print number of chunks\n",
    "        print(f\"Number of chunks for chunk size {chunk_size}, overlap {overlap_percentage}%: {len(chunks)}\")\n",
    "        \n",
    "        # Create Chroma database\n",
    "        db = Chroma.from_documents(chunks, embeddings_client, persist_directory=f\"../papers/vectordb-edit/chunking_{chunk_size}_{overlap_percentage}\")\n",
    "        db.persist()\n",
    "        \n",
    "        # Create retriever\n",
    "        retriever = db.as_retriever()\n",
    "        \n",
    "        # Run and evaluate\n",
    "        result, results_df = run_and_evaluate(f\"Chunk {chunk_size}, overlap {overlap_percentage}%\", retriever, prompt, llm, results_df)\n",
    "        print(f\"CHUNK SIZE {chunk_size}, {overlap_percentage}% overlap:\")\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"../papers/results/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE FIRST TIME RUN THIS, AFTER RUN THE NEXT CELL\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 50)\n",
    "# chunks_1000_5 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_1000))\n",
    "# db_1000_5 = Chroma.from_documents(chunks_1000_5, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_1000_5\")\n",
    "# db_1000_5.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_1000_5 = db_1000_5.as_retriever()\n",
    "# result_1000_5 = run_and_evaluate(\"Chunk 1000, overlap 0%\",retriever_1000_5, prompt, llm, results_df)\n",
    "# print(\"CHUNK SIZE 1000, 5% overlap\")\n",
    "# result_1000_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500, chunk_overlap = 50)\n",
    "# chunks_500 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_500))\n",
    "# db_500 = Chroma.from_documents(chunks_500, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_500\")\n",
    "# retriever_500 = db_500.as_retriever()\n",
    "# result_500 = change_chunk_size(retriever_500)\n",
    "# print(\"CHUNK SIZE 500\")\n",
    "# print(result_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 2000, chunk_overlap = 200)\n",
    "# chunks_2000 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_2000))\n",
    "# db_2000 = Chroma.from_documents(chunks_2000, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_2000\")\n",
    "# retriever_2000 = db_2000.as_retriever()\n",
    "# result_2000 = change_chunk_size(retriever_2000)\n",
    "# print(\"CHUNK SIZE 2000\")\n",
    "# print(result_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 3000, chunk_overlap = 300)\n",
    "# chunks_3000 = text_splitter.split_documents(documents)\n",
    "# print(len(chunks_3000))\n",
    "# db_3000 = Chroma.from_documents(chunks_3000, embeddings_client, persist_directory = \"../papers/vectordb-edit/recursive_3000\")\n",
    "# retriever_3000 = db_3000.as_retriever()\n",
    "# result_3000 = change_chunk_size(retriever_3000)\n",
    "# print(\"CHUNK SIZE 3000\")\n",
    "# print(result_3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.581812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.863942</td>\n",
       "      <td>0.483329</td>\n",
       "      <td>0.638180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.567018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.864540</td>\n",
       "      <td>0.449709</td>\n",
       "      <td>0.630211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.629361</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.932998</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.883107</td>\n",
       "      <td>0.889914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.626769</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.939179</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.883107</td>\n",
       "      <td>0.892364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775448</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.888674</td>\n",
       "      <td>0.566091</td>\n",
       "      <td>0.812443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956411</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.905858</td>\n",
       "      <td>0.642752</td>\n",
       "      <td>0.908059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936453</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.883719</td>\n",
       "      <td>0.900804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.630187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926450</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.882235</td>\n",
       "      <td>0.894370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.976053</td>\n",
       "      <td>0.979920</td>\n",
       "      <td>0.940768</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899326</td>\n",
       "      <td>0.601237</td>\n",
       "      <td>0.899551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861279</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.891491</td>\n",
       "      <td>0.583240</td>\n",
       "      <td>0.831372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.944893</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.901248</td>\n",
       "      <td>0.714201</td>\n",
       "      <td>0.894162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.958276</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.903441</td>\n",
       "      <td>0.682007</td>\n",
       "      <td>0.898491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954163</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.903903</td>\n",
       "      <td>0.680841</td>\n",
       "      <td>0.900466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944514</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905397</td>\n",
       "      <td>0.721049</td>\n",
       "      <td>0.917845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959255</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.954762</td>\n",
       "      <td>0.903965</td>\n",
       "      <td>0.701894</td>\n",
       "      <td>0.915813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.926928</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.903688</td>\n",
       "      <td>0.652210</td>\n",
       "      <td>0.874452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>0.619888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936237</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.881828</td>\n",
       "      <td>0.898384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.660727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923042</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.883079</td>\n",
       "      <td>0.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.648617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930926</td>\n",
       "      <td>0.923889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.883969</td>\n",
       "      <td>0.895977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.615365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923640</td>\n",
       "      <td>0.915556</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.881066</td>\n",
       "      <td>0.887348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.609161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926964</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.880709</td>\n",
       "      <td>0.890697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.598154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.880414</td>\n",
       "      <td>0.888814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.624510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927038</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.881798</td>\n",
       "      <td>0.893449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.629029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930217</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.882235</td>\n",
       "      <td>0.898972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     System  Faithfulness  Answer Relevancy  \\\n",
       "0                   GPT-3.5      1.000000          0.581812   \n",
       "1                     GPT-4      1.000000          0.567018   \n",
       "2                     Naive      0.629361          0.966667   \n",
       "3                 Recursive      0.626769          0.977778   \n",
       "4     Chunk 500, overlap 0%      1.000000          0.775448   \n",
       "5    Chunk 1000, overlap 0%      1.000000          0.956411   \n",
       "6    Chunk 2000, overlap 0%      0.632300          1.000000   \n",
       "7    Chunk 3000, overlap 0%      0.630187          1.000000   \n",
       "8     Chunk 500, overlap 5%      0.976053          0.979920   \n",
       "9    Chunk 500, overlap 10%      1.000000          0.861279   \n",
       "10   Chunk 500, overlap 15%      0.962963          0.944893   \n",
       "11   Chunk 500, overlap 20%      0.972222          0.958276   \n",
       "12   Chunk 1000, overlap 5%      1.000000          0.954163   \n",
       "13  Chunk 1000, overlap 10%      1.000000          0.944514   \n",
       "14  Chunk 1000, overlap 15%      1.000000          0.959255   \n",
       "15  Chunk 1000, overlap 20%      0.900000          0.926928   \n",
       "16   Chunk 2000, overlap 5%      0.619888          1.000000   \n",
       "17  Chunk 2000, overlap 10%      0.660727          1.000000   \n",
       "18  Chunk 2000, overlap 15%      0.648617          1.000000   \n",
       "19  Chunk 2000, overlap 20%      0.615365          1.000000   \n",
       "20   Chunk 3000, overlap 5%      0.609161          1.000000   \n",
       "21  Chunk 3000, overlap 10%      0.598154          1.000000   \n",
       "22  Chunk 3000, overlap 15%      0.624510          1.000000   \n",
       "23  Chunk 3000, overlap 20%      0.629029          1.000000   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.863942            0.483329   \n",
       "1                 NaN             NaN           0.864540            0.449709   \n",
       "2            0.932998        0.963889           0.963462            0.883107   \n",
       "3            0.939179        0.963889           0.963462            0.883107   \n",
       "4            0.894444        0.750000           0.888674            0.566091   \n",
       "5            0.983333        0.960000           0.905858            0.642752   \n",
       "6            0.936453        0.963889           0.988462            0.883719   \n",
       "7            0.926450        0.963889           0.963462            0.882235   \n",
       "8            0.940768        1.000000           0.899326            0.601237   \n",
       "9            0.872222        0.780000           0.891491            0.583240   \n",
       "10           0.941667        0.900000           0.901248            0.714201   \n",
       "11           0.975000        0.900000           0.903441            0.682007   \n",
       "12           0.963889        0.900000           0.903903            0.680841   \n",
       "13           0.936111        1.000000           0.905397            0.721049   \n",
       "14           0.975000        0.954762           0.903965            0.701894   \n",
       "15           0.938889        0.925000           0.903688            0.652210   \n",
       "16           0.936237        0.963889           0.988462            0.881828   \n",
       "17           0.923042        0.963889           0.988462            0.883079   \n",
       "18           0.930926        0.923889           0.988462            0.883969   \n",
       "19           0.923640        0.915556           0.988462            0.881066   \n",
       "20           0.926964        0.963889           0.963462            0.880709   \n",
       "21           0.926966        0.963889           0.963462            0.880414   \n",
       "22           0.927038        0.963889           0.963462            0.881798   \n",
       "23           0.930217        0.963889           0.988462            0.882235   \n",
       "\n",
       "     Average  \n",
       "0   0.638180  \n",
       "1   0.630211  \n",
       "2   0.889914  \n",
       "3   0.892364  \n",
       "4   0.812443  \n",
       "5   0.908059  \n",
       "6   0.900804  \n",
       "7   0.894370  \n",
       "8   0.899551  \n",
       "9   0.831372  \n",
       "10  0.894162  \n",
       "11  0.898491  \n",
       "12  0.900466  \n",
       "13  0.917845  \n",
       "14  0.915813  \n",
       "15  0.874452  \n",
       "16  0.898384  \n",
       "17  0.903200  \n",
       "18  0.895977  \n",
       "19  0.887348  \n",
       "20  0.890697  \n",
       "21  0.888814  \n",
       "22  0.893449  \n",
       "23  0.898972  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944514</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.905397</td>\n",
       "      <td>0.721049</td>\n",
       "      <td>0.917845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     System  Faithfulness  Answer Relevancy  \\\n",
       "13  Chunk 1000, overlap 10%           1.0          0.944514   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "13           0.936111             1.0           0.905397            0.721049   \n",
       "\n",
       "     Average  \n",
       "13  0.917845  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest = results_df.nlargest(1, \"Average\")\n",
    "highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"../papers/results/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now time to look for different top-k\n",
    "\n",
    "Note: We continue with the size chunk of 1000 as it had the highest average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(f\"../papers/results/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_k = Chroma(persist_directory=f\"../papers/vectordb-edit/chunking_1000_10\", embedding_function=embeddings_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8595588343265853\n",
      "Results for K=2:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 1000, overlap 10%, K=2      0.868333          0.957011   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0                1.0        0.757143           0.900639            0.674227   \n",
      "\n",
      "    Average  \n",
      "0  0.859559  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8663941376727795\n",
      "Results for K=3:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 1000, overlap 10%, K=3      0.946667          0.932556   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.966667        0.807143           0.900012            0.645321   \n",
      "\n",
      "    Average  \n",
      "0  0.866394  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]Task exception was never retrieved\n",
      "future: <Task finished name='Task-1691' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1692' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1693' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1694' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1695' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1696' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1697' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1698' coro=<AsyncClient.aclose() done, defined at c:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\httpx\\_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.892436032893865\n",
      "Results for K=5:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 1000, overlap 10%, K=5      0.935185          0.921362   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.903611             1.0           0.907359            0.687099   \n",
      "\n",
      "    Average  \n",
      "0  0.892436  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  22%|██▏       | 13/60 [00:04<00:16,  2.81it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8723694090203784\n",
      "Results for K=6:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 1000, overlap 10%, K=6      0.738083          0.951749   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.888203        0.953667           0.866972            0.835543   \n",
      "\n",
      "    Average  \n",
      "0  0.872369  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8658537789971031\n",
      "Results for K=7:\n",
      "                              System  Faithfulness  Answer Relevancy  \\\n",
      "0  Chunk size 1000, overlap 10%, K=7      0.925926          0.892252   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.901504             0.9           0.902719            0.672722   \n",
      "\n",
      "    Average  \n",
      "0  0.865854  \n"
     ]
    }
   ],
   "source": [
    "k_values = [2, 3, 5, 6, 7]\n",
    "\n",
    "# Iterate over different k values\n",
    "for k in k_values:\n",
    "    # Create retriever with k value\n",
    "    retriever = db_k.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    # Run and evaluate\n",
    "    result, results_df = run_and_evaluate(f\"Chunk size 1000, overlap 10%, K={k}\", retriever, prompt, llm, results_df)\n",
    "    print(f\"Results for K={k}:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"../papers/results/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000, K=2\n",
      "{'faithfulness': 1.0000, 'answer_relevancy': 0.7317, 'context_precision': 0.9000, 'context_recall': 1.0000, 'answer_similarity': 0.9071, 'answer_correctness': 0.5908}\n"
     ]
    }
   ],
   "source": [
    "# retriever_2 = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "# result_2 = run_and_evaluate(\"K=2\",retriever_2, prompt, llm, results_df)\n",
    "# print(\"CHUNK SIZE 1000, K=2\")\n",
    "# print(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000, K=3\n",
      "{'faithfulness': 1.0000, 'answer_relevancy': 0.7832, 'context_precision': 0.9333, 'context_recall': 0.9250, 'answer_similarity': 0.9326, 'answer_correctness': 0.6117}\n"
     ]
    }
   ],
   "source": [
    "# retriever_3 = db_1000.as_retriever(search_kwargs={\"k\": 3})\n",
    "# result_3 = change_chunk_size(retriever_3)\n",
    "# print(\"CHUNK SIZE 1000, K=3\")\n",
    "# print(result_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:14<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK SIZE 1000, K=5\n",
      "{'faithfulness': 0.9028, 'answer_relevancy': 0.6788, 'context_precision': 0.9333, 'context_recall': 1.0000, 'answer_similarity': 0.9250, 'answer_correctness': 0.6024}\n"
     ]
    }
   ],
   "source": [
    "# retriever_5 = db_1000.as_retriever(search_kwargs={\"k\": 5})\n",
    "# result_5 = change_chunk_size(retriever_5)\n",
    "# print(\"CHUNK SIZE 1000, K=5\")\n",
    "# print(result_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look for different retrievers\n",
    "\n",
    "4 chunks was the best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parent document retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8579152257448598\n",
      "                      System  Faithfulness  Answer Relevancy  \\\n",
      "0  Parent Retriever 1000-200      0.949074           0.83992   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.972222            0.85           0.900484            0.635791   \n",
      "\n",
      "    Average  \n",
      "0  0.857915  \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap = 200)\n",
    "child_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap = 0)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents\",persist_directory = \"../papers/vectordb-edit/parent-qa\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "parent_document_retriever.add_documents(documents)\n",
    "result_parent, results_df = run_and_evaluate(f\"Parent Retriever 1000-200\", parent_document_retriever, prompt, llm, results_df)\n",
    "print(result_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8874619132516387\n",
      "                     System  Faithfulness  Answer Relevancy  \\\n",
      "0  Parent Retriever 500-100           1.0          0.943953   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.913889           0.925           0.899494            0.642435   \n",
      "\n",
      "    Average  \n",
      "0  0.887462  \n"
     ]
    }
   ],
   "source": [
    "parent_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap = 50)\n",
    "child_splitter_small = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap = 0)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents_small\",persist_directory = \"../papers/vectordb-edit/parent_small-qa\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_small = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_small,\n",
    "    parent_splitter=parent_splitter_small,\n",
    ")\n",
    "parent_document_retriever_small.add_documents(documents)\n",
    "result_parent_small, results_df = run_and_evaluate(f\"Parent Retriever 500-100\", parent_document_retriever_small, prompt, llm, results_df)\n",
    "print(result_parent_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:  33%|███▎      | 20/60 [00:04<00:08,  4.87it/s]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8426689968348454\n",
      "                      System  Faithfulness  Answer Relevancy  \\\n",
      "0  Parent Retriever 1500-200      0.653428           0.94709   \n",
      "\n",
      "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
      "0           0.829265         0.90119           0.885671            0.839369   \n",
      "\n",
      "    Average  \n",
      "0  0.842669  \n"
     ]
    }
   ],
   "source": [
    "parent_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=1500, chunk_overlap = 150)\n",
    "child_splitter_large = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap = 0)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents_large\",persist_directory = \"../papers/vectordb-edit/parent_large-qa\", embedding_function=embeddings_client)\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever_large = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter_large,\n",
    "    parent_splitter=parent_splitter_large,\n",
    ")\n",
    "parent_document_retriever_large.add_documents(documents)\n",
    "result_parent_large , results_df = run_and_evaluate(f\"Parent Retriever 1500-200\", parent_document_retriever_large, prompt, llm, results_df)\n",
    "print(result_parent_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944514</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.905397</td>\n",
       "      <td>0.721049</td>\n",
       "      <td>0.917845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     System  Faithfulness  Answer Relevancy  \\\n",
       "13  Chunk 1000, overlap 10%           1.0          0.944514   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "13           0.936111             1.0           0.905397            0.721049   \n",
       "\n",
       "     Average  \n",
       "13  0.917845  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest = results_df.nlargest(1, \"Average\")\n",
    "highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum marginal relevance retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   2%|▏         | 1/60 [00:01<01:43,  1.75s/it]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.838339932479549\n",
      "Marginal relevance\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0    MMR      0.905192          0.869071           0.889952            0.85   \n",
      "\n",
      "   Answer Similarity  Answer Correctness  Average  \n",
      "0             0.9013            0.614524  0.83834  \n"
     ]
    }
   ],
   "source": [
    "retriever_mmr = db_k.as_retriever(search_type=\"mmr\")\n",
    "result_mmr, results_df = run_and_evaluate(f\"MMR\", retriever_mmr, prompt, llm, results_df)\n",
    "print(\"Marginal relevance\")\n",
    "print(result_mmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating:   2%|▏         | 1/60 [00:01<01:43,  1.75s/it]Runner in Executor raised an exception\n",
      "TypeError: expected string or buffer\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8170132577714905\n",
      "BM25\n",
      "  System  Faithfulness  Answer Relevancy  Context Precision  Context Recall  \\\n",
      "0   BM25      0.948826          0.829658           0.775303        0.854762   \n",
      "\n",
      "   Answer Similarity  Answer Correctness   Average  \n",
      "0           0.896502            0.597029  0.817013  \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 100)\n",
    "chunks_1000_100 = text_splitter.split_documents(documents)\n",
    "retriever_bm25 = BM25Retriever.from_documents(chunks_1000_100)\n",
    "result_bm25, results_df = run_and_evaluate(f\"BM25\", retriever_bm25, prompt, llm, results_df)\n",
    "print(\"BM25\")\n",
    "print(result_bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensambler - Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:08<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8564256858539333\n",
      "Ensambler\n",
      "        System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Ensambler 1           1.0          0.887284           0.774095   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             1.0           0.894385            0.582789  0.856426  \n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "ret = db_k.as_retriever()\n",
    "ensemble_retriever_1 = EnsembleRetriever(retrievers=[retriever_bm25, ret], weights=[0.75, 0.25])\n",
    "result_ensemble1, results_df = run_and_evaluate(f\"Ensambler 1\", ensemble_retriever_1, prompt, llm, results_df)\n",
    "print(\"Ensambler\")\n",
    "print(result_ensemble1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:12<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8558923090987111\n",
      "Ensambler\n",
      "        System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Ensambler 2         0.975          0.823256           0.828897   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0        0.966667            0.89855            0.642985  0.855892  \n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_2 = EnsembleRetriever(retrievers=[retriever_bm25, ret], weights=[0.5, 0.5])\n",
    "result_ensemble2, results_df = run_and_evaluate(f\"Ensambler 2\", ensemble_retriever_2, prompt, llm, results_df)\n",
    "print(\"Ensambler\")\n",
    "print(result_ensemble2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8530856747444694\n",
      "Ensambler\n",
      "        System  Faithfulness  Answer Relevancy  Context Precision  \\\n",
      "0  Ensambler 3      0.885714          0.815345           0.915738   \n",
      "\n",
      "   Context Recall  Answer Similarity  Answer Correctness   Average  \n",
      "0             0.9           0.899274            0.702443  0.853086  \n"
     ]
    }
   ],
   "source": [
    "ensemble_retriever_3 = EnsembleRetriever(retrievers=[retriever_bm25, ret], weights=[0.25,0.75])\n",
    "result_ensemble3, results_df = run_and_evaluate(f\"Ensambler 3\", ensemble_retriever_3, prompt, llm, results_df)\n",
    "print(\"Ensambler\")\n",
    "print(result_ensemble3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.581812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.863942</td>\n",
       "      <td>0.483329</td>\n",
       "      <td>0.638180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.567018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.864540</td>\n",
       "      <td>0.449709</td>\n",
       "      <td>0.630211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.629361</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.932998</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.883107</td>\n",
       "      <td>0.889914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.626769</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.939179</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.883107</td>\n",
       "      <td>0.892364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775448</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.888674</td>\n",
       "      <td>0.566091</td>\n",
       "      <td>0.812443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956411</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.905858</td>\n",
       "      <td>0.642752</td>\n",
       "      <td>0.908059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936453</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.883719</td>\n",
       "      <td>0.900804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.630187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926450</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.882235</td>\n",
       "      <td>0.894370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.976053</td>\n",
       "      <td>0.979920</td>\n",
       "      <td>0.940768</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899326</td>\n",
       "      <td>0.601237</td>\n",
       "      <td>0.899551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861279</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.891491</td>\n",
       "      <td>0.583240</td>\n",
       "      <td>0.831372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.944893</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.901248</td>\n",
       "      <td>0.714201</td>\n",
       "      <td>0.894162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.958276</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.903441</td>\n",
       "      <td>0.682007</td>\n",
       "      <td>0.898491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954163</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.903903</td>\n",
       "      <td>0.680841</td>\n",
       "      <td>0.900466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944514</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905397</td>\n",
       "      <td>0.721049</td>\n",
       "      <td>0.917845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959255</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.954762</td>\n",
       "      <td>0.903965</td>\n",
       "      <td>0.701894</td>\n",
       "      <td>0.915813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.926928</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.903688</td>\n",
       "      <td>0.652210</td>\n",
       "      <td>0.874452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>0.619888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936237</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.881828</td>\n",
       "      <td>0.898384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.660727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923042</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.883079</td>\n",
       "      <td>0.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.648617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930926</td>\n",
       "      <td>0.923889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.883969</td>\n",
       "      <td>0.895977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.615365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923640</td>\n",
       "      <td>0.915556</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.881066</td>\n",
       "      <td>0.887348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.609161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926964</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.880709</td>\n",
       "      <td>0.890697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.598154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.880414</td>\n",
       "      <td>0.888814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.624510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927038</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.881798</td>\n",
       "      <td>0.893449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.629029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930217</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.882235</td>\n",
       "      <td>0.898972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=2</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.957011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.900639</td>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.859559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=3</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.932556</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.900012</td>\n",
       "      <td>0.645321</td>\n",
       "      <td>0.866394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=5</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>0.921362</td>\n",
       "      <td>0.903611</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907359</td>\n",
       "      <td>0.687099</td>\n",
       "      <td>0.892436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=6</td>\n",
       "      <td>0.738083</td>\n",
       "      <td>0.951749</td>\n",
       "      <td>0.888203</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.866972</td>\n",
       "      <td>0.835543</td>\n",
       "      <td>0.872369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=7</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.892252</td>\n",
       "      <td>0.901504</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.902719</td>\n",
       "      <td>0.672722</td>\n",
       "      <td>0.865854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.949074</td>\n",
       "      <td>0.839920</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.900484</td>\n",
       "      <td>0.635791</td>\n",
       "      <td>0.857915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943953</td>\n",
       "      <td>0.913889</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.899494</td>\n",
       "      <td>0.642435</td>\n",
       "      <td>0.887462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Parent Retriever 1500-200</td>\n",
       "      <td>0.653428</td>\n",
       "      <td>0.947090</td>\n",
       "      <td>0.829265</td>\n",
       "      <td>0.901190</td>\n",
       "      <td>0.885671</td>\n",
       "      <td>0.839369</td>\n",
       "      <td>0.842669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MMR</td>\n",
       "      <td>0.905192</td>\n",
       "      <td>0.869071</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.901300</td>\n",
       "      <td>0.614524</td>\n",
       "      <td>0.838340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.948826</td>\n",
       "      <td>0.829658</td>\n",
       "      <td>0.775303</td>\n",
       "      <td>0.854762</td>\n",
       "      <td>0.896502</td>\n",
       "      <td>0.597029</td>\n",
       "      <td>0.817013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ensambler 1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.887284</td>\n",
       "      <td>0.774095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894385</td>\n",
       "      <td>0.582789</td>\n",
       "      <td>0.856426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ensambler 2</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.823256</td>\n",
       "      <td>0.828897</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.898550</td>\n",
       "      <td>0.642985</td>\n",
       "      <td>0.855892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ensambler 3</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.815345</td>\n",
       "      <td>0.915738</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.899274</td>\n",
       "      <td>0.702443</td>\n",
       "      <td>0.853086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               System  Faithfulness  Answer Relevancy  \\\n",
       "0                             GPT-3.5      1.000000          0.581812   \n",
       "1                               GPT-4      1.000000          0.567018   \n",
       "2                               Naive      0.629361          0.966667   \n",
       "3                           Recursive      0.626769          0.977778   \n",
       "4               Chunk 500, overlap 0%      1.000000          0.775448   \n",
       "5              Chunk 1000, overlap 0%      1.000000          0.956411   \n",
       "6              Chunk 2000, overlap 0%      0.632300          1.000000   \n",
       "7              Chunk 3000, overlap 0%      0.630187          1.000000   \n",
       "8               Chunk 500, overlap 5%      0.976053          0.979920   \n",
       "9              Chunk 500, overlap 10%      1.000000          0.861279   \n",
       "10             Chunk 500, overlap 15%      0.962963          0.944893   \n",
       "11             Chunk 500, overlap 20%      0.972222          0.958276   \n",
       "12             Chunk 1000, overlap 5%      1.000000          0.954163   \n",
       "13            Chunk 1000, overlap 10%      1.000000          0.944514   \n",
       "14            Chunk 1000, overlap 15%      1.000000          0.959255   \n",
       "15            Chunk 1000, overlap 20%      0.900000          0.926928   \n",
       "16             Chunk 2000, overlap 5%      0.619888          1.000000   \n",
       "17            Chunk 2000, overlap 10%      0.660727          1.000000   \n",
       "18            Chunk 2000, overlap 15%      0.648617          1.000000   \n",
       "19            Chunk 2000, overlap 20%      0.615365          1.000000   \n",
       "20             Chunk 3000, overlap 5%      0.609161          1.000000   \n",
       "21            Chunk 3000, overlap 10%      0.598154          1.000000   \n",
       "22            Chunk 3000, overlap 15%      0.624510          1.000000   \n",
       "23            Chunk 3000, overlap 20%      0.629029          1.000000   \n",
       "24  Chunk size 1000, overlap 10%, K=2      0.868333          0.957011   \n",
       "25  Chunk size 1000, overlap 10%, K=3      0.946667          0.932556   \n",
       "26  Chunk size 1000, overlap 10%, K=5      0.935185          0.921362   \n",
       "27  Chunk size 1000, overlap 10%, K=6      0.738083          0.951749   \n",
       "28  Chunk size 1000, overlap 10%, K=7      0.925926          0.892252   \n",
       "29          Parent Retriever 1000-200      0.949074          0.839920   \n",
       "30           Parent Retriever 500-100      1.000000          0.943953   \n",
       "31          Parent Retriever 1500-200      0.653428          0.947090   \n",
       "32                                MMR      0.905192          0.869071   \n",
       "33                               BM25      0.948826          0.829658   \n",
       "34                        Ensambler 1      1.000000          0.887284   \n",
       "35                        Ensambler 2      0.975000          0.823256   \n",
       "36                        Ensambler 3      0.885714          0.815345   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.863942            0.483329   \n",
       "1                 NaN             NaN           0.864540            0.449709   \n",
       "2            0.932998        0.963889           0.963462            0.883107   \n",
       "3            0.939179        0.963889           0.963462            0.883107   \n",
       "4            0.894444        0.750000           0.888674            0.566091   \n",
       "5            0.983333        0.960000           0.905858            0.642752   \n",
       "6            0.936453        0.963889           0.988462            0.883719   \n",
       "7            0.926450        0.963889           0.963462            0.882235   \n",
       "8            0.940768        1.000000           0.899326            0.601237   \n",
       "9            0.872222        0.780000           0.891491            0.583240   \n",
       "10           0.941667        0.900000           0.901248            0.714201   \n",
       "11           0.975000        0.900000           0.903441            0.682007   \n",
       "12           0.963889        0.900000           0.903903            0.680841   \n",
       "13           0.936111        1.000000           0.905397            0.721049   \n",
       "14           0.975000        0.954762           0.903965            0.701894   \n",
       "15           0.938889        0.925000           0.903688            0.652210   \n",
       "16           0.936237        0.963889           0.988462            0.881828   \n",
       "17           0.923042        0.963889           0.988462            0.883079   \n",
       "18           0.930926        0.923889           0.988462            0.883969   \n",
       "19           0.923640        0.915556           0.988462            0.881066   \n",
       "20           0.926964        0.963889           0.963462            0.880709   \n",
       "21           0.926966        0.963889           0.963462            0.880414   \n",
       "22           0.927038        0.963889           0.963462            0.881798   \n",
       "23           0.930217        0.963889           0.988462            0.882235   \n",
       "24           1.000000        0.757143           0.900639            0.674227   \n",
       "25           0.966667        0.807143           0.900012            0.645321   \n",
       "26           0.903611        1.000000           0.907359            0.687099   \n",
       "27           0.888203        0.953667           0.866972            0.835543   \n",
       "28           0.901504        0.900000           0.902719            0.672722   \n",
       "29           0.972222        0.850000           0.900484            0.635791   \n",
       "30           0.913889        0.925000           0.899494            0.642435   \n",
       "31           0.829265        0.901190           0.885671            0.839369   \n",
       "32           0.889952        0.850000           0.901300            0.614524   \n",
       "33           0.775303        0.854762           0.896502            0.597029   \n",
       "34           0.774095        1.000000           0.894385            0.582789   \n",
       "35           0.828897        0.966667           0.898550            0.642985   \n",
       "36           0.915738        0.900000           0.899274            0.702443   \n",
       "\n",
       "     Average  \n",
       "0   0.638180  \n",
       "1   0.630211  \n",
       "2   0.889914  \n",
       "3   0.892364  \n",
       "4   0.812443  \n",
       "5   0.908059  \n",
       "6   0.900804  \n",
       "7   0.894370  \n",
       "8   0.899551  \n",
       "9   0.831372  \n",
       "10  0.894162  \n",
       "11  0.898491  \n",
       "12  0.900466  \n",
       "13  0.917845  \n",
       "14  0.915813  \n",
       "15  0.874452  \n",
       "16  0.898384  \n",
       "17  0.903200  \n",
       "18  0.895977  \n",
       "19  0.887348  \n",
       "20  0.890697  \n",
       "21  0.888814  \n",
       "22  0.893449  \n",
       "23  0.898972  \n",
       "24  0.859559  \n",
       "25  0.866394  \n",
       "26  0.892436  \n",
       "27  0.872369  \n",
       "28  0.865854  \n",
       "29  0.857915  \n",
       "30  0.887462  \n",
       "31  0.842669  \n",
       "32  0.838340  \n",
       "33  0.817013  \n",
       "34  0.856426  \n",
       "35  0.855892  \n",
       "36  0.853086  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-stage - reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: BaseCohere.rerank() takes 1 positional argument but 4 positional arguments (and 2 keyword-only arguments) were givenon the following question: Why did the door blow out of Boeing 737 Max shortly after take-off?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseCohere.rerank() takes 1 positional argument but 4 positional arguments (and 2 keyword-only arguments) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m, in \u001b[0;36mrun_and_evaluate\u001b[1;34m(name, retriever, prompt, llm, results_df)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \n\u001b[1;32m----> 7\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mretrieval_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Access the response content\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2056\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2056\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2059\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2060\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2061\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2062\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2693\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2681\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2682\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m   2683\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2691\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   2692\u001b[0m         ]\n\u001b[1;32m-> 2693\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2693\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2681\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2682\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m   2683\u001b[0m                 step\u001b[38;5;241m.\u001b[39minvoke,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2691\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   2692\u001b[0m         ]\n\u001b[1;32m-> 2693\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2056\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2056\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2059\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2060\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2061\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2062\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py:141\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py:244\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py:237\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 237\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain\\retrievers\\contextual_compression.py:48\u001b[0m, in \u001b[0;36mContextualCompressionRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[1;32m---> 48\u001b[0m     compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain\\retrievers\\document_compressors\\cohere_rerank.py:107\u001b[0m, in \u001b[0;36mCohereRerank.compress_documents\u001b[1;34m(self, documents, query, callbacks)\u001b[0m\n\u001b[0;32m    106\u001b[0m compressed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m documents[res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain\\retrievers\\document_compressors\\cohere_rerank.py:79\u001b[0m, in \u001b[0;36mCohereRerank.rerank\u001b[1;34m(self, documents, query, model, top_n, max_chunks_per_doc)\u001b[0m\n\u001b[0;32m     78\u001b[0m top_n \u001b[38;5;241m=\u001b[39m top_n \u001b[38;5;28;01mif\u001b[39;00m (top_n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m top_n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_n\n\u001b[1;32m---> 79\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chunks_per_doc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_chunks_per_doc\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m result_dicts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseCohere.rerank() takes 1 positional argument but 4 positional arguments (and 2 keyword-only arguments) were given",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m compressor \u001b[38;5;241m=\u001b[39m CohereRerank()\n\u001b[0;32m      6\u001b[0m compression_retriever \u001b[38;5;241m=\u001b[39m ContextualCompressionRetriever(\n\u001b[0;32m      7\u001b[0m     base_compressor\u001b[38;5;241m=\u001b[39mcompressor, base_retriever\u001b[38;5;241m=\u001b[39mretriever_context\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m result_compression, results_df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCohere reranker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReranker\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_compression)\n",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m, in \u001b[0;36mrun_and_evaluate\u001b[1;34m(name, retriever, prompt, llm, results_df)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon the following question: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m query)  \n\u001b[0;32m     15\u001b[0m answers\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo answer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m context_full \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m context_content \u001b[38;5;241m=\u001b[39m [context\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m context \u001b[38;5;129;01min\u001b[39;00m context_full]\n\u001b[0;32m     18\u001b[0m contexts_extra\u001b[38;5;241m.\u001b[39mappend(context_content)\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py:244\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    243\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    247\u001b[0m         result,\n\u001b[0;32m    248\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain_core\\retrievers.py:237\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[1;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 237\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain\\retrievers\\contextual_compression.py:48\u001b[0m, in \u001b[0;36mContextualCompressionRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(\n\u001b[0;32m     45\u001b[0m     query, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[1;32m---> 48\u001b[0m     compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain\\retrievers\\document_compressors\\cohere_rerank.py:107\u001b[0m, in \u001b[0;36mCohereRerank.compress_documents\u001b[1;34m(self, documents, query, callbacks)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03mCompress documents using Cohere's rerank API.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    A sequence of compressed documents.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m compressed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m documents[res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    109\u001b[0m     doc_copy \u001b[38;5;241m=\u001b[39m Document(doc\u001b[38;5;241m.\u001b[39mpage_content, metadata\u001b[38;5;241m=\u001b[39mdeepcopy(doc\u001b[38;5;241m.\u001b[39mmetadata))\n",
      "File \u001b[1;32mc:\\Users\\sigitalapina\\OneDrive - KPMG\\Desktop\\thesis-rag\\.venv\\Lib\\site-packages\\langchain\\retrievers\\document_compressors\\cohere_rerank.py:79\u001b[0m, in \u001b[0;36mCohereRerank.rerank\u001b[1;34m(self, documents, query, model, top_n, max_chunks_per_doc)\u001b[0m\n\u001b[0;32m     77\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m     78\u001b[0m top_n \u001b[38;5;241m=\u001b[39m top_n \u001b[38;5;28;01mif\u001b[39;00m (top_n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m top_n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_n\n\u001b[1;32m---> 79\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrerank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_chunks_per_doc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_chunks_per_doc\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m result_dicts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseCohere.rerank() takes 1 positional argument but 4 positional arguments (and 2 keyword-only arguments) were given"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "retriever_context = ret\n",
    "compressor = CohereRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=retriever_context\n",
    ")\n",
    "\n",
    "result_compression, results_df = run_and_evaluate(f\"Cohere reranker\", compression_retriever, prompt, llm, results_df)\n",
    "print(\"Reranker\")\n",
    "print(result_compression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating context by remaking the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944514</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.905397</td>\n",
       "      <td>0.721049</td>\n",
       "      <td>0.917845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     System  Faithfulness  Answer Relevancy  \\\n",
       "13  Chunk 1000, overlap 10%           1.0          0.944514   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "13           0.936111             1.0           0.905397            0.721049   \n",
       "\n",
       "     Average  \n",
       "13  0.917845  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest = results_df.nlargest(1, \"Average\")\n",
    "highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_context = \"Generate a search query to fetch the relevant documents using the user's {question}. Craft a query that specifically targets the keywords in the question. In the answer provide only the query.\"\n",
    "prompt_context = ChatPromptTemplate.from_template(template_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  8.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 1.0000, 'answer_relevancy': 0.9439, 'context_precision': 0.8306, 'context_recall': 0.8000, 'answer_similarity': 0.9013, 'answer_correctness': 0.6398}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_final = []\n",
    "contexts_final = []\n",
    "# retriever_context_q = EnsembleRetriever(retrievers=[retriever_bm25, retriever_3], weights=[0.5, 0.5])\n",
    "llm_for_context =(\n",
    "    { \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough()\n",
    "    | {\"response\": prompt_context | llm}\n",
    ")\n",
    "for query in questions:\n",
    "    response_check = llm_for_context.invoke({\"question\": query})\n",
    "    search_query = response_check[\"response\"].content\n",
    "    retrieval_augmented_qa_chain = (\n",
    "        {\"context\": itemgetter(\"context\"), \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")\n",
    "    docs = ret.get_relevant_documents(search_query)\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        resulting_doc = doc.page_content\n",
    "        formatted_docs.append(resulting_doc)\n",
    "    try:  \n",
    "            response = retrieval_augmented_qa_chain.invoke({\"context\": formatted_docs, \"question\": query})\n",
    "            # Access the response content\n",
    "            answers_final.append(response[\"response\"].content)\n",
    "            contexts_final.append(formatted_docs)  \n",
    "    except Exception as e:  \n",
    "            print(f\"Warning: {e}\" + \"on the following question: \" + query)  \n",
    "            answers_final.append(\"No answer\")\n",
    "            contexts_final.append(formatted_docs)\n",
    "\n",
    "\n",
    "result_search_query = evaluation_rag(questions, answers_final, contexts_final, ground_truths)\n",
    "result_search_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8525949606908769\n"
     ]
    }
   ],
   "source": [
    "average = dictionary(result_search_query)\n",
    "    # Create a dictionary to store the results\n",
    "system_results = {\n",
    "        \"System\": \"Search query\",\n",
    "        \"Faithfulness\": result_search_query[\"faithfulness\"],\n",
    "        \"Answer Relevancy\": result_search_query[\"answer_relevancy\"],\n",
    "        \"Context Precision\": result_search_query[\"context_precision\"],\n",
    "        \"Context Recall\": result_search_query[\"context_recall\"],\n",
    "        \"Answer Similarity\": result_search_query[\"answer_similarity\"],\n",
    "        \"Answer Correctness\": result_search_query[\"answer_correctness\"],\n",
    "        \"Average\": average\n",
    "    }\n",
    "df_result_search_query = pd.DataFrame([system_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.581812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.863942</td>\n",
       "      <td>0.483329</td>\n",
       "      <td>0.638180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.567018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.864540</td>\n",
       "      <td>0.449709</td>\n",
       "      <td>0.630211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive</td>\n",
       "      <td>0.629361</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.932998</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.883107</td>\n",
       "      <td>0.889914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recursive</td>\n",
       "      <td>0.626769</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.939179</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.883107</td>\n",
       "      <td>0.892364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chunk 500, overlap 0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775448</td>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.888674</td>\n",
       "      <td>0.566091</td>\n",
       "      <td>0.812443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956411</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.905858</td>\n",
       "      <td>0.642752</td>\n",
       "      <td>0.908059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chunk 2000, overlap 0%</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936453</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.883719</td>\n",
       "      <td>0.900804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chunk 3000, overlap 0%</td>\n",
       "      <td>0.630187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926450</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.882235</td>\n",
       "      <td>0.894370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chunk 500, overlap 5%</td>\n",
       "      <td>0.976053</td>\n",
       "      <td>0.979920</td>\n",
       "      <td>0.940768</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.899326</td>\n",
       "      <td>0.601237</td>\n",
       "      <td>0.899551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chunk 500, overlap 10%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861279</td>\n",
       "      <td>0.872222</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.891491</td>\n",
       "      <td>0.583240</td>\n",
       "      <td>0.831372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chunk 500, overlap 15%</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.944893</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.901248</td>\n",
       "      <td>0.714201</td>\n",
       "      <td>0.894162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Chunk 500, overlap 20%</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.958276</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.903441</td>\n",
       "      <td>0.682007</td>\n",
       "      <td>0.898491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chunk 1000, overlap 5%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954163</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.903903</td>\n",
       "      <td>0.680841</td>\n",
       "      <td>0.900466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944514</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905397</td>\n",
       "      <td>0.721049</td>\n",
       "      <td>0.917845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959255</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.954762</td>\n",
       "      <td>0.903965</td>\n",
       "      <td>0.701894</td>\n",
       "      <td>0.915813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chunk 1000, overlap 20%</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.926928</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.903688</td>\n",
       "      <td>0.652210</td>\n",
       "      <td>0.874452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chunk 2000, overlap 5%</td>\n",
       "      <td>0.619888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936237</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.881828</td>\n",
       "      <td>0.898384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Chunk 2000, overlap 10%</td>\n",
       "      <td>0.660727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923042</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.883079</td>\n",
       "      <td>0.903200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chunk 2000, overlap 15%</td>\n",
       "      <td>0.648617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930926</td>\n",
       "      <td>0.923889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.883969</td>\n",
       "      <td>0.895977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chunk 2000, overlap 20%</td>\n",
       "      <td>0.615365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923640</td>\n",
       "      <td>0.915556</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.881066</td>\n",
       "      <td>0.887348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chunk 3000, overlap 5%</td>\n",
       "      <td>0.609161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926964</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.880709</td>\n",
       "      <td>0.890697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chunk 3000, overlap 10%</td>\n",
       "      <td>0.598154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.880414</td>\n",
       "      <td>0.888814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chunk 3000, overlap 15%</td>\n",
       "      <td>0.624510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927038</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.963462</td>\n",
       "      <td>0.881798</td>\n",
       "      <td>0.893449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Chunk 3000, overlap 20%</td>\n",
       "      <td>0.629029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930217</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.882235</td>\n",
       "      <td>0.898972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=2</td>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.957011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.900639</td>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.859559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=3</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.932556</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.900012</td>\n",
       "      <td>0.645321</td>\n",
       "      <td>0.866394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=5</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>0.921362</td>\n",
       "      <td>0.903611</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907359</td>\n",
       "      <td>0.687099</td>\n",
       "      <td>0.892436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=6</td>\n",
       "      <td>0.738083</td>\n",
       "      <td>0.951749</td>\n",
       "      <td>0.888203</td>\n",
       "      <td>0.953667</td>\n",
       "      <td>0.866972</td>\n",
       "      <td>0.835543</td>\n",
       "      <td>0.872369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chunk size 1000, overlap 10%, K=7</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.892252</td>\n",
       "      <td>0.901504</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.902719</td>\n",
       "      <td>0.672722</td>\n",
       "      <td>0.865854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Parent Retriever 1000-200</td>\n",
       "      <td>0.949074</td>\n",
       "      <td>0.839920</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.900484</td>\n",
       "      <td>0.635791</td>\n",
       "      <td>0.857915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Parent Retriever 500-100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943953</td>\n",
       "      <td>0.913889</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.899494</td>\n",
       "      <td>0.642435</td>\n",
       "      <td>0.887462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Parent Retriever 1500-200</td>\n",
       "      <td>0.653428</td>\n",
       "      <td>0.947090</td>\n",
       "      <td>0.829265</td>\n",
       "      <td>0.901190</td>\n",
       "      <td>0.885671</td>\n",
       "      <td>0.839369</td>\n",
       "      <td>0.842669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MMR</td>\n",
       "      <td>0.905192</td>\n",
       "      <td>0.869071</td>\n",
       "      <td>0.889952</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.901300</td>\n",
       "      <td>0.614524</td>\n",
       "      <td>0.838340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.948826</td>\n",
       "      <td>0.829658</td>\n",
       "      <td>0.775303</td>\n",
       "      <td>0.854762</td>\n",
       "      <td>0.896502</td>\n",
       "      <td>0.597029</td>\n",
       "      <td>0.817013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Ensambler 1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.887284</td>\n",
       "      <td>0.774095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894385</td>\n",
       "      <td>0.582789</td>\n",
       "      <td>0.856426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ensambler 2</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.823256</td>\n",
       "      <td>0.828897</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.898550</td>\n",
       "      <td>0.642985</td>\n",
       "      <td>0.855892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ensambler 3</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.815345</td>\n",
       "      <td>0.915738</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.899274</td>\n",
       "      <td>0.702443</td>\n",
       "      <td>0.853086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Search query</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943884</td>\n",
       "      <td>0.830556</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.901307</td>\n",
       "      <td>0.639823</td>\n",
       "      <td>0.852595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               System  Faithfulness  Answer Relevancy  \\\n",
       "0                             GPT-3.5      1.000000          0.581812   \n",
       "1                               GPT-4      1.000000          0.567018   \n",
       "2                               Naive      0.629361          0.966667   \n",
       "3                           Recursive      0.626769          0.977778   \n",
       "4               Chunk 500, overlap 0%      1.000000          0.775448   \n",
       "5              Chunk 1000, overlap 0%      1.000000          0.956411   \n",
       "6              Chunk 2000, overlap 0%      0.632300          1.000000   \n",
       "7              Chunk 3000, overlap 0%      0.630187          1.000000   \n",
       "8               Chunk 500, overlap 5%      0.976053          0.979920   \n",
       "9              Chunk 500, overlap 10%      1.000000          0.861279   \n",
       "10             Chunk 500, overlap 15%      0.962963          0.944893   \n",
       "11             Chunk 500, overlap 20%      0.972222          0.958276   \n",
       "12             Chunk 1000, overlap 5%      1.000000          0.954163   \n",
       "13            Chunk 1000, overlap 10%      1.000000          0.944514   \n",
       "14            Chunk 1000, overlap 15%      1.000000          0.959255   \n",
       "15            Chunk 1000, overlap 20%      0.900000          0.926928   \n",
       "16             Chunk 2000, overlap 5%      0.619888          1.000000   \n",
       "17            Chunk 2000, overlap 10%      0.660727          1.000000   \n",
       "18            Chunk 2000, overlap 15%      0.648617          1.000000   \n",
       "19            Chunk 2000, overlap 20%      0.615365          1.000000   \n",
       "20             Chunk 3000, overlap 5%      0.609161          1.000000   \n",
       "21            Chunk 3000, overlap 10%      0.598154          1.000000   \n",
       "22            Chunk 3000, overlap 15%      0.624510          1.000000   \n",
       "23            Chunk 3000, overlap 20%      0.629029          1.000000   \n",
       "24  Chunk size 1000, overlap 10%, K=2      0.868333          0.957011   \n",
       "25  Chunk size 1000, overlap 10%, K=3      0.946667          0.932556   \n",
       "26  Chunk size 1000, overlap 10%, K=5      0.935185          0.921362   \n",
       "27  Chunk size 1000, overlap 10%, K=6      0.738083          0.951749   \n",
       "28  Chunk size 1000, overlap 10%, K=7      0.925926          0.892252   \n",
       "29          Parent Retriever 1000-200      0.949074          0.839920   \n",
       "30           Parent Retriever 500-100      1.000000          0.943953   \n",
       "31          Parent Retriever 1500-200      0.653428          0.947090   \n",
       "32                                MMR      0.905192          0.869071   \n",
       "33                               BM25      0.948826          0.829658   \n",
       "34                        Ensambler 1      1.000000          0.887284   \n",
       "35                        Ensambler 2      0.975000          0.823256   \n",
       "36                        Ensambler 3      0.885714          0.815345   \n",
       "37                       Search query      1.000000          0.943884   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0                 NaN             NaN           0.863942            0.483329   \n",
       "1                 NaN             NaN           0.864540            0.449709   \n",
       "2            0.932998        0.963889           0.963462            0.883107   \n",
       "3            0.939179        0.963889           0.963462            0.883107   \n",
       "4            0.894444        0.750000           0.888674            0.566091   \n",
       "5            0.983333        0.960000           0.905858            0.642752   \n",
       "6            0.936453        0.963889           0.988462            0.883719   \n",
       "7            0.926450        0.963889           0.963462            0.882235   \n",
       "8            0.940768        1.000000           0.899326            0.601237   \n",
       "9            0.872222        0.780000           0.891491            0.583240   \n",
       "10           0.941667        0.900000           0.901248            0.714201   \n",
       "11           0.975000        0.900000           0.903441            0.682007   \n",
       "12           0.963889        0.900000           0.903903            0.680841   \n",
       "13           0.936111        1.000000           0.905397            0.721049   \n",
       "14           0.975000        0.954762           0.903965            0.701894   \n",
       "15           0.938889        0.925000           0.903688            0.652210   \n",
       "16           0.936237        0.963889           0.988462            0.881828   \n",
       "17           0.923042        0.963889           0.988462            0.883079   \n",
       "18           0.930926        0.923889           0.988462            0.883969   \n",
       "19           0.923640        0.915556           0.988462            0.881066   \n",
       "20           0.926964        0.963889           0.963462            0.880709   \n",
       "21           0.926966        0.963889           0.963462            0.880414   \n",
       "22           0.927038        0.963889           0.963462            0.881798   \n",
       "23           0.930217        0.963889           0.988462            0.882235   \n",
       "24           1.000000        0.757143           0.900639            0.674227   \n",
       "25           0.966667        0.807143           0.900012            0.645321   \n",
       "26           0.903611        1.000000           0.907359            0.687099   \n",
       "27           0.888203        0.953667           0.866972            0.835543   \n",
       "28           0.901504        0.900000           0.902719            0.672722   \n",
       "29           0.972222        0.850000           0.900484            0.635791   \n",
       "30           0.913889        0.925000           0.899494            0.642435   \n",
       "31           0.829265        0.901190           0.885671            0.839369   \n",
       "32           0.889952        0.850000           0.901300            0.614524   \n",
       "33           0.775303        0.854762           0.896502            0.597029   \n",
       "34           0.774095        1.000000           0.894385            0.582789   \n",
       "35           0.828897        0.966667           0.898550            0.642985   \n",
       "36           0.915738        0.900000           0.899274            0.702443   \n",
       "37           0.830556        0.800000           0.901307            0.639823   \n",
       "\n",
       "     Average  \n",
       "0   0.638180  \n",
       "1   0.630211  \n",
       "2   0.889914  \n",
       "3   0.892364  \n",
       "4   0.812443  \n",
       "5   0.908059  \n",
       "6   0.900804  \n",
       "7   0.894370  \n",
       "8   0.899551  \n",
       "9   0.831372  \n",
       "10  0.894162  \n",
       "11  0.898491  \n",
       "12  0.900466  \n",
       "13  0.917845  \n",
       "14  0.915813  \n",
       "15  0.874452  \n",
       "16  0.898384  \n",
       "17  0.903200  \n",
       "18  0.895977  \n",
       "19  0.887348  \n",
       "20  0.890697  \n",
       "21  0.888814  \n",
       "22  0.893449  \n",
       "23  0.898972  \n",
       "24  0.859559  \n",
       "25  0.866394  \n",
       "26  0.892436  \n",
       "27  0.872369  \n",
       "28  0.865854  \n",
       "29  0.857915  \n",
       "30  0.887462  \n",
       "31  0.842669  \n",
       "32  0.838340  \n",
       "33  0.817013  \n",
       "34  0.856426  \n",
       "35  0.855892  \n",
       "36  0.853086  \n",
       "37  0.852595  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.concat([results_df, df_result_search_query], ignore_index=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change model to GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chunk 1000, overlap 10%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944514</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905397</td>\n",
       "      <td>0.721049</td>\n",
       "      <td>0.917845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Chunk 1000, overlap 15%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959255</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.954762</td>\n",
       "      <td>0.903965</td>\n",
       "      <td>0.701894</td>\n",
       "      <td>0.915813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chunk 1000, overlap 0%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956411</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.905858</td>\n",
       "      <td>0.642752</td>\n",
       "      <td>0.908059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     System  Faithfulness  Answer Relevancy  \\\n",
       "13  Chunk 1000, overlap 10%           1.0          0.944514   \n",
       "14  Chunk 1000, overlap 15%           1.0          0.959255   \n",
       "5    Chunk 1000, overlap 0%           1.0          0.956411   \n",
       "\n",
       "    Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "13           0.936111        1.000000           0.905397            0.721049   \n",
       "14           0.975000        0.954762           0.903965            0.701894   \n",
       "5            0.983333        0.960000           0.905858            0.642752   \n",
       "\n",
       "     Average  \n",
       "13  0.917845  \n",
       "14  0.915813  \n",
       "5   0.908059  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest = results_df.nlargest(3, \"Average\")\n",
    "highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8902044941248054\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chunk 1000, overlap 10%, GPT-4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.944886</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.904391</td>\n",
       "      <td>0.647505</td>\n",
       "      <td>0.890204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           System  Faithfulness  Answer Relevancy  \\\n",
       "0  Chunk 1000, overlap 10%, GPT-4           0.9          0.944886   \n",
       "\n",
       "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0           0.944444             1.0           0.904391            0.647505   \n",
       "\n",
       "    Average  \n",
       "0  0.890204  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_gpt4_1000_10, results_df = run_and_evaluate(f\"Chunk 1000, overlap 10%, GPT-4\", ret, prompt, llm_gpt4, results_df)\n",
    "result_gpt4_1000_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:09<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.8921944588786568\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chunk 1000, overlap 15%, GPT-4</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.949334</td>\n",
       "      <td>0.963889</td>\n",
       "      <td>0.969048</td>\n",
       "      <td>0.903324</td>\n",
       "      <td>0.678684</td>\n",
       "      <td>0.892194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           System  Faithfulness  Answer Relevancy  \\\n",
       "0  Chunk 1000, overlap 15%, GPT-4      0.888889          0.949334   \n",
       "\n",
       "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0           0.963889        0.969048           0.903324            0.678684   \n",
       "\n",
       "    Average  \n",
       "0  0.892194  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_15 = Chroma(persist_directory = \"../papers/vectordb-edit/chunking_1000_15\", embedding_function=embeddings_client)\n",
    "ret_15 = db_15.as_retriever()\n",
    "result_search_query_gpt4, results_df = run_and_evaluate(f\"Chunk 1000, overlap 15%, GPT-4\", ret_15, prompt, llm_gpt4, results_df)\n",
    "result_search_query_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "passing column names as 'ground_truths' is deprecated and will be removed in the next version, please use 'ground_truth' instead. Note that `ground_truth` should be of type string and not Sequence[string] like `ground_truths`\n",
      "Evaluating: 100%|██████████| 60/60 [00:07<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score is: 0.9124498020107327\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System</th>\n",
       "      <th>Faithfulness</th>\n",
       "      <th>Answer Relevancy</th>\n",
       "      <th>Context Precision</th>\n",
       "      <th>Context Recall</th>\n",
       "      <th>Answer Similarity</th>\n",
       "      <th>Answer Correctness</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chunk 1000, overlap 0%, GPT-4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949857</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.902345</td>\n",
       "      <td>0.68083</td>\n",
       "      <td>0.91245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          System  Faithfulness  Answer Relevancy  \\\n",
       "0  Chunk 1000, overlap 0%, GPT-4           1.0          0.949857   \n",
       "\n",
       "   Context Precision  Context Recall  Answer Similarity  Answer Correctness  \\\n",
       "0           0.991667            0.95           0.902345             0.68083   \n",
       "\n",
       "   Average  \n",
       "0  0.91245  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_0 = Chroma(persist_directory = \"../papers/vectordb-edit/chunking_1000_0\", embedding_function=embeddings_client)\n",
    "ret_0 = db_0.as_retriever()\n",
    "result_search_query_gpt4, results_df = run_and_evaluate(f\"Chunk 1000, overlap 0%, GPT-4\", ret_0, prompt, llm_gpt4, results_df)\n",
    "result_search_query_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f\"../papers/results/results_qa.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
